{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries and import modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the vanila libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.random as nrd\n",
    "import os\n",
    "import pathlib \n",
    "import sys\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "#\n",
    "import umap\n",
    "\n",
    "# Pytorch modules \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# this for the custom Dataset \n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "\n",
    "\n",
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# for timing functions\n",
    "from timeit import default_timer as timer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Project Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\gpano\\\\Desktop\\\\github_py\\\\proteomics_latent_space'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check your current directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** Run the configuration file first `configs.py`. Importing this script and setting the seed and device parameters before importing any of the other modules ensures that evereything is sync.\n",
    "\n",
    "**Important** If you want *change the configuration parameters*, change them before importing and running the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing models_util.configs module\n",
      "First set device and seed for reproducibility.\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from models_util import configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seed: None, Device: None'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs.get_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n"
     ]
    }
   ],
   "source": [
    "# print the global variables\n",
    "print(configs.project_seed, configs.project_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During configuration random seed 789 has been set.\n",
      "789 cpu\n"
     ]
    }
   ],
   "source": [
    "configs.set_seed(789)\n",
    "device = configs.set_device(force_cpu=True)\n",
    "\n",
    "# global variables have changed too\n",
    "print(configs.project_seed, configs.project_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seed: 789, Device: cpu'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see if the get function also agrees:\n",
    "configs.get_configs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the configurations values are assigned globally, we can import the modules. If this is working, we expect each module to access the **same** **seed** and **device** we set. We are also expecting generated numbers **inside the modules** to be reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During configuration random seed 789 has been set.\n",
      "Importing models_util.utility_functions, running in cpu with seed: 789\n"
     ]
    }
   ],
   "source": [
    "# Load home modules and check the device where they are running \n",
    "from models_util import utility_functions as uf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During configuration random seed 789 has been set.\n",
      "Importing models_util.custom_dataset, running in cpu with seed: 789\n"
     ]
    }
   ],
   "source": [
    "from models_util import custom_dataset as cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During configuration random seed 789 has been set.\n",
      "Importing models_util.cost_functions, running in cpu with seed: 789\n"
     ]
    }
   ],
   "source": [
    "from models_util import cost_functions as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During configuration random seed 789 has been set.\n",
      "Importing models_util.VAE1, running in cpu with seed: 789\n"
     ]
    }
   ],
   "source": [
    "from models_util import VAE1 as v1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCBC Data scale and split for VAE\n",
    "- We will perform min-max scaling to the TMT-Ratios of the proteomic SCBC data. <br>\n",
    "- We will scale the array version of our scbc data, the `npscbc` matrix.\n",
    "- Then we will copy this scaled matrix and reshuffle the copy. The `npscbc_scaled_shuffled` will be used for the model training and performance evaluattion. <br>\n",
    "- The `npscbc_scaled` matrix with the original order of rows will be used later for the validation of the latent variables. <br> \n",
    "- It is important to use the non-missing min and max values of dataset row-by-row <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create path and read the scbc data\n",
    "data_path = os.getcwd() + \"\\\\data\\\\processed\\\\\" \n",
    "scbc = pd.read_csv(data_path+\"prot_abms_norm.txt\",delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(15306)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to numpy \n",
    "npscbc = scbc.to_numpy()\n",
    "np.isnan(npscbc).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11209, 1) (11209, 1) 0 0\n"
     ]
    }
   ],
   "source": [
    "# Get extreme values (non-missing) frome ach row. \n",
    "scbc_min = np.nanmin(npscbc, axis=1, keepdims=True)  # minimum among non-NaN\n",
    "scbc_max = np.nanmax(npscbc, axis=1,keepdims=True)  # maximum among non-NaN\n",
    "\n",
    "# check that that shapes and values are as expected \n",
    "print(scbc_max.shape,scbc_min.shape,np.isnan(scbc_max).sum(), np.isnan(scbc_min).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11209, 54)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scale data \n",
    "npscbc_scaled = (npscbc - scbc_min) /(scbc_max - scbc_min + 1e-8)\n",
    "npscbc_scaled.shape\n",
    "\n",
    "# npscbc_scaled[0:2,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the rows but keep scaled original\n",
    "npscbc_scaled_shuffled = npscbc_scaled.copy()\n",
    "np.random.shuffle(npscbc_scaled_shuffled)\n",
    "# npscbc_scaled[1,],scbc.iloc[1,:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Split Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8406, 54), (1121, 54), (1682, 54))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, val_data, test_data = uf.create_data_partition(\n",
    "    npscbc_scaled_shuffled, test_perc=0.15, val=True, val_perc=0.1\n",
    ")\n",
    "train_data.shape, val_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test reproducibility by re-runing the function and checking the data in the first index of the matrix. We expect it to be the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass data to Custom Dataset and DataLoaders \n",
    "- check that your data is numpy matrix.\n",
    "- check if data is scaled to (0,1).\n",
    "- create three custom dataset instances.\n",
    "- the custom dataset will save all the data to memory and create a mask where NaNs are located.\n",
    "- the numpy arrays will be converted to tensors of appropriate dimensions and NaNs to zeroes.\n",
    "- then we pass the custom dataset to the dataloader object.\n",
    "- The DataLoader object contains for each row (training example) i) a tensor of 1 x 130 columns with 0-1 scaled values, ii) a 1x130 mask indicating NA positions and iii) index of the examples per batch (could be 64, 128,..., batch_size). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protein Dataset is passed to memory\n",
      "No Protein Symbols were identified\n",
      "Protein Dataset is passed to memory\n",
      "No Protein Symbols were identified\n",
      "Protein Dataset is passed to memory\n",
      "No Protein Symbols were identified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = cd.ProteinDataset(train_data)\n",
    "val_dataset = cd.ProteinDataset(val_data)\n",
    "test_dataset = cd.ProteinDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass data to the dataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.2391, 0.1722, 0.1519,  ..., 0.4952, 0.5124, 0.4771],\n",
       "         [0.0353, 0.0033, 0.0000,  ..., 0.5410, 0.5976, 0.5736],\n",
       "         [0.0764, 0.0489, 0.0000,  ..., 0.1683, 0.0580, 0.1417],\n",
       "         ...,\n",
       "         [0.9355, 0.9443, 0.8803,  ..., 0.4635, 0.4635, 0.4635],\n",
       "         [0.8342, 1.0000, 0.5116,  ..., 0.1154, 0.1154, 0.0902],\n",
       "         [0.8522, 0.9352, 0.8787,  ..., 0.9263, 1.0000, 0.8641]]),\n",
       " tensor([[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]]),\n",
       " tensor([6772,  307, 1713, 5072, 4315,  982, 3389, 1842, 3162, 3762, 2830, 3983,\n",
       "         2531, 2847, 7875, 3727, 2129, 5405, 1716,  652, 4243, 3225, 3548, 4350,\n",
       "         4845, 5468, 5199, 4107, 1662, 8146,  296, 2400, 6955, 2199, 3056, 3070,\n",
       "         6780, 1876, 4467, 8392, 1942, 8106, 5466, 2205, 1693, 2933, 7612, 5303,\n",
       "         8161, 5483, 8040, 1894, 3258,  813,  428, 2260, 5621, 6163, 3707, 1349,\n",
       "         6602, 5899, 8253, 5146, 5368, 6340,  567,  928, 1766, 1502, 5573, 1818,\n",
       "         6046, 7671,  678, 5006, 7236, 6147, 2194, 4072, 7397,  514, 3483, 4730,\n",
       "         7494, 6137, 8401, 7757, 5063, 5546, 1918, 3516, 4460, 4663, 3577, 3689,\n",
       "         8032, 5506, 8072, 2704, 1742, 3796, 3662, 5219, 1775, 2082, 4431, 7399,\n",
       "         1577,  390, 7407,  673, 1530, 3957, 1858, 4596, 7530, 1015, 1718, 3325,\n",
       "         1623, 7953, 7819, 2498, 4860, 1124, 4036, 3511], dtype=torch.int32)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the train loader is not reproducible bcs it shuffles but it is not seeded yet. \n",
    "# here is one batch of training examples \n",
    "# torch.manual_seed(888)\n",
    "\n",
    "\n",
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bit optimization Loop \n",
    "It comprises the run of the training and validation set. VAE inherently have a tendency to overfit, so it is important to keep the test set after training loop. In this tutorial we run one model. The name is based on a simple numbering system and its layers to track it down. Furthermore the train_val_loop creates a hyperparameter string to track other parameters. The whole loop is parametrized in a function: <br>\n",
    "- The function starts with a pre-training evaluation to initialize metrics at epoch = 0 <br>\n",
    "- Then training of the model begins and after each epoch, the validation set is passed through the model to get the validation - epoch metrics.<br>\n",
    "\n",
    "\n",
    "During training, these are computed:\n",
    "- KL, Gaussian Logliklihood error, and Total Error are monitored per training batch, and also averaged every n batches.\n",
    "- KL, Gaussian Logliklihood error, and Total Error are monitored per validation round (per epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.4, 0.8, 1.2, 1.6, 2.0, 2.4, 2.8, 3.2, 3.6, 4.0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\n",
      "model path c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\modelscbc_0_54_30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e27a843d67456db1dbf31c60ef35a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing pre-training evaluation on the model in epoch 0\n",
      "\n",
      "Val loss: 1.321| Val KL: 0.36493626568052506 | Val Rec: 0.956\n",
      "\n",
      "Epoch 1\n",
      "--------------------\n",
      "Train loss: 0.686|Train Rec: 0.281 | Val loss: 0.130, Val Rec: 0.118\n",
      "\n",
      "Epoch 2\n",
      "--------------------\n",
      "Train loss: 0.126|Train Rec: 0.113 | Val loss: 0.101, Val Rec: 0.098\n",
      "\n",
      "Epoch 3\n",
      "--------------------\n",
      "Train loss: 0.102|Train Rec: 0.099 | Val loss: 0.093, Val Rec: 0.092\n",
      "\n",
      "Epoch 4\n",
      "--------------------\n",
      "Train loss: 0.092|Train Rec: 0.091 | Val loss: 0.086, Val Rec: 0.086\n",
      "\n",
      "Epoch 5\n",
      "--------------------\n",
      "Train loss: 0.085|Train Rec: 0.084 | Val loss: 0.081, Val Rec: 0.081\n",
      "\n",
      "Epoch 6\n",
      "--------------------\n",
      "Train loss: 0.082|Train Rec: 0.082 | Val loss: 0.077, Val Rec: 0.076\n",
      "\n",
      "Epoch 7\n",
      "--------------------\n",
      "Train loss: 0.079|Train Rec: 0.079 | Val loss: 0.074, Val Rec: 0.074\n",
      "\n",
      "Epoch 8\n",
      "--------------------\n",
      "Train loss: 0.076|Train Rec: 0.076 | Val loss: 0.070, Val Rec: 0.069\n",
      "\n",
      "Epoch 9\n",
      "--------------------\n",
      "Train loss: 0.075|Train Rec: 0.075 | Val loss: 0.068, Val Rec: 0.068\n",
      "\n",
      "Epoch 10\n",
      "--------------------\n",
      "Train loss: 0.072|Train Rec: 0.071 | Val loss: 0.064, Val Rec: 0.063\n",
      "\n",
      "Epoch 11\n",
      "--------------------\n",
      "Train loss: 0.070|Train Rec: 0.070 | Val loss: 0.065, Val Rec: 0.065\n",
      "\n",
      "Epoch 12\n",
      "--------------------\n",
      "Train loss: 0.070|Train Rec: 0.070 | Val loss: 0.065, Val Rec: 0.065\n",
      "\n",
      "Epoch 13\n",
      "--------------------\n",
      "Train loss: 0.069|Train Rec: 0.069 | Val loss: 0.062, Val Rec: 0.062\n",
      "\n",
      "Epoch 14\n",
      "--------------------\n",
      "Train loss: 0.067|Train Rec: 0.067 | Val loss: 0.062, Val Rec: 0.062\n",
      "\n",
      "Epoch 15\n",
      "--------------------\n",
      "Train loss: 0.067|Train Rec: 0.067 | Val loss: 0.062, Val Rec: 0.062\n",
      "\n",
      "Epoch 16\n",
      "--------------------\n",
      "Train loss: 0.067|Train Rec: 0.067 | Val loss: 0.062, Val Rec: 0.062\n",
      "\n",
      "Epoch 17\n",
      "--------------------\n",
      "Train loss: 0.067|Train Rec: 0.067 | Val loss: 0.061, Val Rec: 0.061\n",
      "\n",
      "Epoch 18\n",
      "--------------------\n",
      "Train loss: 0.066|Train Rec: 0.066 | Val loss: 0.061, Val Rec: 0.061\n",
      "\n",
      "Epoch 19\n",
      "--------------------\n",
      "Train loss: 0.066|Train Rec: 0.066 | Val loss: 0.061, Val Rec: 0.061\n",
      "\n",
      "Epoch 20\n",
      "--------------------\n",
      "Train loss: 0.066|Train Rec: 0.066 | Val loss: 0.063, Val Rec: 0.063\n",
      "\n",
      "Epoch 21\n",
      "--------------------\n",
      "Train loss: 0.066|Train Rec: 0.066 | Val loss: 0.063, Val Rec: 0.063\n",
      "\n",
      "Epoch 22\n",
      "--------------------\n",
      "Train loss: 0.066|Train Rec: 0.066 | Val loss: 0.064, Val Rec: 0.064\n",
      "\n",
      "Epoch 23\n",
      "--------------------\n",
      "Train loss: 0.066|Train Rec: 0.066 | Val loss: 0.061, Val Rec: 0.061\n",
      "\n",
      "Epoch 24\n",
      "--------------------\n",
      "Train loss: 0.065|Train Rec: 0.065 | Val loss: 0.061, Val Rec: 0.061\n",
      "\n",
      "Epoch 25\n",
      "--------------------\n",
      "Train loss: 0.066|Train Rec: 0.066 | Val loss: 0.061, Val Rec: 0.061\n",
      "\n",
      "Epoch 26\n",
      "--------------------\n",
      "Train loss: 0.066|Train Rec: 0.066 | Val loss: 0.061, Val Rec: 0.061\n",
      "\n",
      "Epoch 27\n",
      "--------------------\n",
      "Train loss: 0.065|Train Rec: 0.065 | Val loss: 0.061, Val Rec: 0.061\n",
      "\n",
      "Epoch 28\n",
      "--------------------\n",
      "Train loss: 0.066|Train Rec: 0.066 | Val loss: 0.061, Val Rec: 0.061\n",
      "\n",
      "Epoch 29\n",
      "--------------------\n",
      "Train loss: 0.065|Train Rec: 0.065 | Val loss: 0.061, Val Rec: 0.061\n",
      "\n",
      "Epoch 30\n",
      "--------------------\n",
      "Train loss: 0.065|Train Rec: 0.065 | Val loss: 0.061, Val Rec: 0.061\n",
      "\n",
      "Epoch 31\n",
      "--------------------\n",
      "Train loss: 0.066|Train Rec: 0.066 | Val loss: 0.061, Val Rec: 0.061\n",
      "\n",
      "Epoch 32\n",
      "--------------------\n",
      "Train loss: 0.065|Train Rec: 0.065 | Val loss: 0.062, Val Rec: 0.062\n",
      "\n",
      "Epoch 33\n",
      "--------------------\n",
      "Train loss: 0.065|Train Rec: 0.065 | Val loss: 0.061, Val Rec: 0.061\n",
      "\n",
      "Epoch 34\n",
      "--------------------\n",
      "Train loss: 0.065|Train Rec: 0.065 | Val loss: 0.061, Val Rec: 0.061\n",
      "\n",
      "Epoch 35\n",
      "--------------------\n",
      "Train loss: 0.065|Train Rec: 0.065 | Val loss: 0.061, Val Rec: 0.061\n",
      "\n",
      "Epoch 36\n",
      "--------------------\n",
      "Train loss: 0.065|Train Rec: 0.065 | Val loss: 0.061, Val Rec: 0.061\n",
      "\n",
      "Epoch 37\n",
      "--------------------\n",
      "Train loss: 0.065|Train Rec: 0.065 | Val loss: 0.062, Val Rec: 0.062\n",
      "\n",
      "Epoch 38\n",
      "--------------------\n",
      "Train loss: 0.065|Train Rec: 0.065 | Val loss: 0.062, Val Rec: 0.062\n",
      "\n",
      "Epoch 39\n",
      "--------------------\n",
      "Train loss: 0.066|Train Rec: 0.066 | Val loss: 0.061, Val Rec: 0.061\n",
      "\n",
      "Epoch 40\n",
      "--------------------\n",
      "Train loss: 0.065|Train Rec: 0.065 | Val loss: 0.061, Val Rec: 0.061\n",
      "\n",
      "Epoch 41\n",
      "--------------------\n",
      "Train loss: 0.065|Train Rec: 0.065 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 42\n",
      "--------------------\n",
      "Train loss: 0.066|Train Rec: 0.066 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 43\n",
      "--------------------\n",
      "Train loss: 0.065|Train Rec: 0.065 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 44\n",
      "--------------------\n",
      "Train loss: 0.065|Train Rec: 0.065 | Val loss: 0.061, Val Rec: 0.061\n",
      "\n",
      "Epoch 45\n",
      "--------------------\n",
      "Train loss: 0.065|Train Rec: 0.065 | Val loss: 0.061, Val Rec: 0.061\n",
      "\n",
      "Epoch 46\n",
      "--------------------\n",
      "Train loss: 0.065|Train Rec: 0.065 | Val loss: 0.061, Val Rec: 0.061\n",
      "\n",
      "Epoch 47\n",
      "--------------------\n",
      "Train loss: 0.065|Train Rec: 0.065 | Val loss: 0.062, Val Rec: 0.062\n",
      "\n",
      "Epoch 48\n",
      "--------------------\n",
      "Train loss: 0.065|Train Rec: 0.065 | Val loss: 0.061, Val Rec: 0.061\n",
      "\n",
      "Epoch 49\n",
      "--------------------\n",
      "Train loss: 0.065|Train Rec: 0.065 | Val loss: 0.061, Val Rec: 0.061\n",
      "\n",
      "Epoch 50\n",
      "--------------------\n",
      "Train loss: 0.065|Train Rec: 0.065 | Val loss: 0.061, Val Rec: 0.061\n",
      "\n",
      "Patience exceeded at 50 with last checkpoint saved at 42\n",
      "changed learning rate to 0.001\n",
      "Epoch 51\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 52\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 53\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 54\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 55\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 56\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 57\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 58\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 59\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 60\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 61\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 62\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 63\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 64\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 65\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 66\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 67\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 68\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 69\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 70\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 71\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 72\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 73\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 74\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 75\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Epoch 76\n",
      "--------------------\n",
      "Train loss: 0.064|Train Rec: 0.064 | Val loss: 0.060, Val Rec: 0.060\n",
      "\n",
      "Early stopping at epoch 76 with last checkpoint saved at 65\n",
      "Model saved at: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\modelscbc_0_54_30\n",
      "Model: modelscbc_0_54_30_ep65_norm0_bits0.0_bs128_lr0.001 has been trained\n",
      "Using this model modelscbc_0_54_30_ep65_norm0_bits0.0_bs128_lr0.001\n",
      "Directory already exists: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\n",
      "model path c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\modelscbc_1_54_30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94cabf6be3d4733881bac7ff8bf7874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing pre-training evaluation on the model in epoch 0\n",
      "\n",
      "Val loss: 9.280| Val KL: 8.317765341864693 | Val Rec: 0.963\n",
      "\n",
      "Epoch 1\n",
      "--------------------\n",
      "Train loss: 8.605|Train Rec: 0.254 | Val loss: 8.366, Val Rec: 0.047\n",
      "\n",
      "Epoch 2\n",
      "--------------------\n",
      "Train loss: 8.362|Train Rec: 0.031 | Val loss: 8.325, Val Rec: 0.003\n",
      "\n",
      "Epoch 3\n",
      "--------------------\n",
      "Train loss: 8.316|Train Rec: -0.015 | Val loss: 8.296, Val Rec: -0.022\n",
      "\n",
      "Epoch 4\n",
      "--------------------\n",
      "Train loss: 8.297|Train Rec: -0.033 | Val loss: 8.281, Val Rec: -0.037\n",
      "\n",
      "Epoch 5\n",
      "--------------------\n",
      "Train loss: 8.281|Train Rec: -0.049 | Val loss: 8.267, Val Rec: -0.052\n",
      "\n",
      "Epoch 6\n",
      "--------------------\n",
      "Train loss: 8.271|Train Rec: -0.060 | Val loss: 8.262, Val Rec: -0.057\n",
      "\n",
      "Epoch 7\n",
      "--------------------\n",
      "Train loss: 8.257|Train Rec: -0.072 | Val loss: 8.246, Val Rec: -0.073\n",
      "\n",
      "Epoch 8\n",
      "--------------------\n",
      "Train loss: 8.253|Train Rec: -0.076 | Val loss: 8.230, Val Rec: -0.089\n",
      "\n",
      "Epoch 9\n",
      "--------------------\n",
      "Train loss: 8.248|Train Rec: -0.082 | Val loss: 8.242, Val Rec: -0.076\n",
      "\n",
      "Epoch 10\n",
      "--------------------\n",
      "Train loss: 8.239|Train Rec: -0.091 | Val loss: 8.229, Val Rec: -0.089\n",
      "\n",
      "Epoch 11\n",
      "--------------------\n",
      "Train loss: 8.235|Train Rec: -0.093 | Val loss: 8.223, Val Rec: -0.097\n",
      "\n",
      "Epoch 12\n",
      "--------------------\n",
      "Train loss: 8.229|Train Rec: -0.099 | Val loss: 8.223, Val Rec: -0.095\n",
      "\n",
      "Epoch 13\n",
      "--------------------\n",
      "Train loss: 8.225|Train Rec: -0.103 | Val loss: 8.212, Val Rec: -0.107\n",
      "\n",
      "Epoch 14\n",
      "--------------------\n",
      "Train loss: 8.222|Train Rec: -0.107 | Val loss: 8.201, Val Rec: -0.118\n",
      "\n",
      "Epoch 15\n",
      "--------------------\n",
      "Train loss: 8.217|Train Rec: -0.111 | Val loss: 8.208, Val Rec: -0.110\n",
      "\n",
      "Epoch 16\n",
      "--------------------\n",
      "Train loss: 8.214|Train Rec: -0.113 | Val loss: 8.218, Val Rec: -0.101\n",
      "\n",
      "Epoch 17\n",
      "--------------------\n",
      "Train loss: 8.208|Train Rec: -0.119 | Val loss: 8.203, Val Rec: -0.116\n",
      "\n",
      "Epoch 18\n",
      "--------------------\n",
      "Train loss: 8.209|Train Rec: -0.118 | Val loss: 8.193, Val Rec: -0.127\n",
      "\n",
      "Epoch 19\n",
      "--------------------\n",
      "Train loss: 8.208|Train Rec: -0.121 | Val loss: 8.189, Val Rec: -0.131\n",
      "\n",
      "Epoch 20\n",
      "--------------------\n",
      "Train loss: 8.206|Train Rec: -0.121 | Val loss: 8.190, Val Rec: -0.129\n",
      "\n",
      "Epoch 21\n",
      "--------------------\n",
      "Train loss: 8.205|Train Rec: -0.123 | Val loss: 8.187, Val Rec: -0.132\n",
      "\n",
      "Epoch 22\n",
      "--------------------\n",
      "Train loss: 8.203|Train Rec: -0.124 | Val loss: 8.189, Val Rec: -0.130\n",
      "\n",
      "Epoch 23\n",
      "--------------------\n",
      "Train loss: 8.198|Train Rec: -0.129 | Val loss: 8.185, Val Rec: -0.135\n",
      "\n",
      "Epoch 24\n",
      "--------------------\n",
      "Train loss: 8.200|Train Rec: -0.128 | Val loss: 8.184, Val Rec: -0.136\n",
      "\n",
      "Epoch 25\n",
      "--------------------\n",
      "Train loss: 8.201|Train Rec: -0.127 | Val loss: 8.185, Val Rec: -0.134\n",
      "\n",
      "Epoch 26\n",
      "--------------------\n",
      "Train loss: 8.200|Train Rec: -0.129 | Val loss: 8.190, Val Rec: -0.130\n",
      "\n",
      "Epoch 27\n",
      "--------------------\n",
      "Train loss: 8.197|Train Rec: -0.131 | Val loss: 8.169, Val Rec: -0.150\n",
      "\n",
      "Epoch 28\n",
      "--------------------\n",
      "Train loss: 8.197|Train Rec: -0.131 | Val loss: 8.171, Val Rec: -0.148\n",
      "\n",
      "Epoch 29\n",
      "--------------------\n",
      "Train loss: 8.196|Train Rec: -0.132 | Val loss: 8.176, Val Rec: -0.143\n",
      "\n",
      "Epoch 30\n",
      "--------------------\n",
      "Train loss: 8.196|Train Rec: -0.132 | Val loss: 8.180, Val Rec: -0.142\n",
      "\n",
      "Epoch 31\n",
      "--------------------\n",
      "Train loss: 8.199|Train Rec: -0.129 | Val loss: 8.177, Val Rec: -0.143\n",
      "\n",
      "Epoch 32\n",
      "--------------------\n",
      "Train loss: 8.194|Train Rec: -0.133 | Val loss: 8.163, Val Rec: -0.156\n",
      "\n",
      "Epoch 33\n",
      "--------------------\n",
      "Train loss: 8.192|Train Rec: -0.135 | Val loss: 8.163, Val Rec: -0.157\n",
      "\n",
      "Epoch 34\n",
      "--------------------\n",
      "Train loss: 8.190|Train Rec: -0.137 | Val loss: 8.175, Val Rec: -0.144\n",
      "\n",
      "Epoch 35\n",
      "--------------------\n",
      "Train loss: 8.192|Train Rec: -0.136 | Val loss: 8.168, Val Rec: -0.151\n",
      "\n",
      "Epoch 36\n",
      "--------------------\n",
      "Train loss: 8.191|Train Rec: -0.137 | Val loss: 8.171, Val Rec: -0.148\n",
      "\n",
      "Epoch 37\n",
      "--------------------\n",
      "Train loss: 8.190|Train Rec: -0.138 | Val loss: 8.163, Val Rec: -0.157\n",
      "\n",
      "Epoch 38\n",
      "--------------------\n",
      "Train loss: 8.187|Train Rec: -0.141 | Val loss: 8.166, Val Rec: -0.153\n",
      "\n",
      "Epoch 39\n",
      "--------------------\n",
      "Train loss: 8.187|Train Rec: -0.140 | Val loss: 8.171, Val Rec: -0.150\n",
      "\n",
      "Epoch 40\n",
      "--------------------\n",
      "Train loss: 8.192|Train Rec: -0.136 | Val loss: 8.170, Val Rec: -0.148\n",
      "\n",
      "Epoch 41\n",
      "--------------------\n",
      "Train loss: 8.189|Train Rec: -0.138 | Val loss: 8.166, Val Rec: -0.153\n",
      "\n",
      "Epoch 42\n",
      "--------------------\n",
      "Train loss: 8.188|Train Rec: -0.140 | Val loss: 8.169, Val Rec: -0.152\n",
      "\n",
      "Epoch 43\n",
      "--------------------\n",
      "Train loss: 8.189|Train Rec: -0.138 | Val loss: 8.158, Val Rec: -0.161\n",
      "\n",
      "Epoch 44\n",
      "--------------------\n",
      "Train loss: 8.190|Train Rec: -0.138 | Val loss: 8.166, Val Rec: -0.154\n",
      "\n",
      "Epoch 45\n",
      "--------------------\n",
      "Train loss: 8.187|Train Rec: -0.140 | Val loss: 8.161, Val Rec: -0.157\n",
      "\n",
      "Epoch 46\n",
      "--------------------\n",
      "Train loss: 8.189|Train Rec: -0.139 | Val loss: 8.162, Val Rec: -0.157\n",
      "\n",
      "Epoch 47\n",
      "--------------------\n",
      "Train loss: 8.185|Train Rec: -0.143 | Val loss: 8.169, Val Rec: -0.151\n",
      "\n",
      "Epoch 48\n",
      "--------------------\n",
      "Train loss: 8.187|Train Rec: -0.140 | Val loss: 8.162, Val Rec: -0.159\n",
      "\n",
      "Epoch 49\n",
      "--------------------\n",
      "Train loss: 8.188|Train Rec: -0.140 | Val loss: 8.159, Val Rec: -0.160\n",
      "\n",
      "Epoch 50\n",
      "--------------------\n",
      "Train loss: 8.187|Train Rec: -0.141 | Val loss: 8.162, Val Rec: -0.157\n",
      "\n",
      "Epoch 51\n",
      "--------------------\n",
      "Train loss: 8.185|Train Rec: -0.143 | Val loss: 8.159, Val Rec: -0.159\n",
      "\n",
      "Patience exceeded at 51 with last checkpoint saved at 43\n",
      "changed learning rate to 0.001\n",
      "Epoch 52\n",
      "--------------------\n",
      "Train loss: 8.179|Train Rec: -0.148 | Val loss: 8.145, Val Rec: -0.174\n",
      "\n",
      "Epoch 53\n",
      "--------------------\n",
      "Train loss: 8.177|Train Rec: -0.150 | Val loss: 8.148, Val Rec: -0.171\n",
      "\n",
      "Epoch 54\n",
      "--------------------\n",
      "Train loss: 8.176|Train Rec: -0.152 | Val loss: 8.150, Val Rec: -0.168\n",
      "\n",
      "Epoch 55\n",
      "--------------------\n",
      "Train loss: 8.175|Train Rec: -0.152 | Val loss: 8.153, Val Rec: -0.165\n",
      "\n",
      "Epoch 56\n",
      "--------------------\n",
      "Train loss: 8.174|Train Rec: -0.153 | Val loss: 8.145, Val Rec: -0.173\n",
      "\n",
      "Epoch 57\n",
      "--------------------\n",
      "Train loss: 8.175|Train Rec: -0.152 | Val loss: 8.144, Val Rec: -0.175\n",
      "\n",
      "Epoch 58\n",
      "--------------------\n",
      "Train loss: 8.175|Train Rec: -0.153 | Val loss: 8.144, Val Rec: -0.176\n",
      "\n",
      "Epoch 59\n",
      "--------------------\n",
      "Train loss: 8.175|Train Rec: -0.152 | Val loss: 8.148, Val Rec: -0.171\n",
      "\n",
      "Epoch 60\n",
      "--------------------\n",
      "Train loss: 8.173|Train Rec: -0.154 | Val loss: 8.142, Val Rec: -0.178\n",
      "\n",
      "Epoch 61\n",
      "--------------------\n",
      "Train loss: 8.174|Train Rec: -0.154 | Val loss: 8.139, Val Rec: -0.179\n",
      "\n",
      "Epoch 62\n",
      "--------------------\n",
      "Train loss: 8.173|Train Rec: -0.155 | Val loss: 8.144, Val Rec: -0.175\n",
      "\n",
      "Epoch 63\n",
      "--------------------\n",
      "Train loss: 8.174|Train Rec: -0.153 | Val loss: 8.145, Val Rec: -0.173\n",
      "\n",
      "Epoch 64\n",
      "--------------------\n",
      "Train loss: 8.174|Train Rec: -0.153 | Val loss: 8.153, Val Rec: -0.166\n",
      "\n",
      "Epoch 65\n",
      "--------------------\n",
      "Train loss: 8.172|Train Rec: -0.155 | Val loss: 8.143, Val Rec: -0.176\n",
      "\n",
      "Epoch 66\n",
      "--------------------\n",
      "Train loss: 8.174|Train Rec: -0.154 | Val loss: 8.140, Val Rec: -0.178\n",
      "\n",
      "Epoch 67\n",
      "--------------------\n",
      "Train loss: 8.174|Train Rec: -0.153 | Val loss: 8.140, Val Rec: -0.179\n",
      "\n",
      "Epoch 68\n",
      "--------------------\n",
      "Train loss: 8.172|Train Rec: -0.156 | Val loss: 8.145, Val Rec: -0.174\n",
      "\n",
      "Epoch 69\n",
      "--------------------\n",
      "Train loss: 8.172|Train Rec: -0.156 | Val loss: 8.145, Val Rec: -0.174\n",
      "\n",
      "Epoch 70\n",
      "--------------------\n",
      "Train loss: 8.171|Train Rec: -0.156 | Val loss: 8.140, Val Rec: -0.179\n",
      "\n",
      "Epoch 71\n",
      "--------------------\n",
      "Train loss: 8.172|Train Rec: -0.155 | Val loss: 8.142, Val Rec: -0.177\n",
      "\n",
      "Epoch 72\n",
      "--------------------\n",
      "Train loss: 8.173|Train Rec: -0.154 | Val loss: 8.141, Val Rec: -0.178\n",
      "\n",
      "Early stopping at epoch 72 with last checkpoint saved at 61\n",
      "Model saved at: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\modelscbc_1_54_30\n",
      "Model: modelscbc_1_54_30_ep61_norm0_bits0.4_bs128_lr0.001 has been trained\n",
      "Using this model modelscbc_1_54_30_ep61_norm0_bits0.4_bs128_lr0.001\n",
      "Directory already exists: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\n",
      "model path c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\modelscbc_2_54_30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145b0382e42c4ebf9b956118e12c3cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing pre-training evaluation on the model in epoch 0\n",
      "\n",
      "Val loss: 17.610| Val KL: 16.635530683729385 | Val Rec: 0.975\n",
      "\n",
      "Epoch 1\n",
      "--------------------\n",
      "Train loss: 16.878|Train Rec: 0.230 | Val loss: 16.635, Val Rec: -0.003\n",
      "\n",
      "Epoch 2\n",
      "--------------------\n",
      "Train loss: 16.617|Train Rec: -0.032 | Val loss: 16.577, Val Rec: -0.062\n",
      "\n",
      "Epoch 3\n",
      "--------------------\n",
      "Train loss: 16.570|Train Rec: -0.082 | Val loss: 16.540, Val Rec: -0.099\n",
      "\n",
      "Epoch 4\n",
      "--------------------\n",
      "Train loss: 16.546|Train Rec: -0.105 | Val loss: 16.526, Val Rec: -0.112\n",
      "\n",
      "Epoch 5\n",
      "--------------------\n",
      "Train loss: 16.527|Train Rec: -0.126 | Val loss: 16.499, Val Rec: -0.138\n",
      "\n",
      "Epoch 6\n",
      "--------------------\n",
      "Train loss: 16.508|Train Rec: -0.143 | Val loss: 16.473, Val Rec: -0.164\n",
      "\n",
      "Epoch 7\n",
      "--------------------\n",
      "Train loss: 16.492|Train Rec: -0.159 | Val loss: 16.484, Val Rec: -0.153\n",
      "\n",
      "Epoch 8\n",
      "--------------------\n",
      "Train loss: 16.481|Train Rec: -0.170 | Val loss: 16.452, Val Rec: -0.187\n",
      "\n",
      "Epoch 9\n",
      "--------------------\n",
      "Train loss: 16.467|Train Rec: -0.183 | Val loss: 16.436, Val Rec: -0.201\n",
      "\n",
      "Epoch 10\n",
      "--------------------\n",
      "Train loss: 16.467|Train Rec: -0.183 | Val loss: 16.452, Val Rec: -0.185\n",
      "\n",
      "Epoch 11\n",
      "--------------------\n",
      "Train loss: 16.450|Train Rec: -0.200 | Val loss: 16.440, Val Rec: -0.196\n",
      "\n",
      "Epoch 12\n",
      "--------------------\n",
      "Train loss: 16.443|Train Rec: -0.206 | Val loss: 16.423, Val Rec: -0.214\n",
      "\n",
      "Epoch 13\n",
      "--------------------\n",
      "Train loss: 16.438|Train Rec: -0.211 | Val loss: 16.404, Val Rec: -0.233\n",
      "\n",
      "Epoch 14\n",
      "--------------------\n",
      "Train loss: 16.432|Train Rec: -0.216 | Val loss: 16.399, Val Rec: -0.238\n",
      "\n",
      "Epoch 15\n",
      "--------------------\n",
      "Train loss: 16.427|Train Rec: -0.222 | Val loss: 16.385, Val Rec: -0.251\n",
      "\n",
      "Epoch 16\n",
      "--------------------\n",
      "Train loss: 16.421|Train Rec: -0.228 | Val loss: 16.378, Val Rec: -0.260\n",
      "\n",
      "Epoch 17\n",
      "--------------------\n",
      "Train loss: 16.417|Train Rec: -0.232 | Val loss: 16.378, Val Rec: -0.259\n",
      "\n",
      "Epoch 18\n",
      "--------------------\n",
      "Train loss: 16.413|Train Rec: -0.235 | Val loss: 16.369, Val Rec: -0.269\n",
      "\n",
      "Epoch 19\n",
      "--------------------\n",
      "Train loss: 16.408|Train Rec: -0.241 | Val loss: 16.372, Val Rec: -0.266\n",
      "\n",
      "Epoch 20\n",
      "--------------------\n",
      "Train loss: 16.406|Train Rec: -0.243 | Val loss: 16.365, Val Rec: -0.272\n",
      "\n",
      "Epoch 21\n",
      "--------------------\n",
      "Train loss: 16.403|Train Rec: -0.246 | Val loss: 16.345, Val Rec: -0.294\n",
      "\n",
      "Epoch 22\n",
      "--------------------\n",
      "Train loss: 16.398|Train Rec: -0.252 | Val loss: 16.351, Val Rec: -0.289\n",
      "\n",
      "Epoch 23\n",
      "--------------------\n",
      "Train loss: 16.397|Train Rec: -0.253 | Val loss: 16.348, Val Rec: -0.291\n",
      "\n",
      "Epoch 24\n",
      "--------------------\n",
      "Train loss: 16.395|Train Rec: -0.254 | Val loss: 16.344, Val Rec: -0.293\n",
      "\n",
      "Epoch 25\n",
      "--------------------\n",
      "Train loss: 16.391|Train Rec: -0.259 | Val loss: 16.343, Val Rec: -0.294\n",
      "\n",
      "Epoch 26\n",
      "--------------------\n",
      "Train loss: 16.388|Train Rec: -0.261 | Val loss: 16.334, Val Rec: -0.304\n",
      "\n",
      "Epoch 27\n",
      "--------------------\n",
      "Train loss: 16.387|Train Rec: -0.261 | Val loss: 16.328, Val Rec: -0.310\n",
      "\n",
      "Epoch 28\n",
      "--------------------\n",
      "Train loss: 16.389|Train Rec: -0.260 | Val loss: 16.331, Val Rec: -0.308\n",
      "\n",
      "Epoch 29\n",
      "--------------------\n",
      "Train loss: 16.387|Train Rec: -0.263 | Val loss: 16.324, Val Rec: -0.315\n",
      "\n",
      "Epoch 30\n",
      "--------------------\n",
      "Train loss: 16.387|Train Rec: -0.263 | Val loss: 16.328, Val Rec: -0.310\n",
      "\n",
      "Epoch 31\n",
      "--------------------\n",
      "Train loss: 16.384|Train Rec: -0.265 | Val loss: 16.332, Val Rec: -0.305\n",
      "\n",
      "Epoch 32\n",
      "--------------------\n",
      "Train loss: 16.382|Train Rec: -0.267 | Val loss: 16.324, Val Rec: -0.314\n",
      "\n",
      "Epoch 33\n",
      "--------------------\n",
      "Train loss: 16.383|Train Rec: -0.266 | Val loss: 16.326, Val Rec: -0.312\n",
      "\n",
      "Epoch 34\n",
      "--------------------\n",
      "Train loss: 16.380|Train Rec: -0.270 | Val loss: 16.318, Val Rec: -0.320\n",
      "\n",
      "Epoch 35\n",
      "--------------------\n",
      "Train loss: 16.378|Train Rec: -0.271 | Val loss: 16.310, Val Rec: -0.328\n",
      "\n",
      "Epoch 36\n",
      "--------------------\n",
      "Train loss: 16.378|Train Rec: -0.272 | Val loss: 16.319, Val Rec: -0.319\n",
      "\n",
      "Epoch 37\n",
      "--------------------\n",
      "Train loss: 16.379|Train Rec: -0.270 | Val loss: 16.314, Val Rec: -0.325\n",
      "\n",
      "Epoch 38\n",
      "--------------------\n",
      "Train loss: 16.376|Train Rec: -0.273 | Val loss: 16.324, Val Rec: -0.315\n",
      "\n",
      "Epoch 39\n",
      "--------------------\n",
      "Train loss: 16.376|Train Rec: -0.273 | Val loss: 16.313, Val Rec: -0.324\n",
      "\n",
      "Epoch 40\n",
      "--------------------\n",
      "Train loss: 16.373|Train Rec: -0.276 | Val loss: 16.312, Val Rec: -0.326\n",
      "\n",
      "Epoch 41\n",
      "--------------------\n",
      "Train loss: 16.373|Train Rec: -0.276 | Val loss: 16.315, Val Rec: -0.323\n",
      "\n",
      "Epoch 42\n",
      "--------------------\n",
      "Train loss: 16.370|Train Rec: -0.279 | Val loss: 16.309, Val Rec: -0.327\n",
      "\n",
      "Epoch 43\n",
      "--------------------\n",
      "Train loss: 16.372|Train Rec: -0.277 | Val loss: 16.315, Val Rec: -0.322\n",
      "\n",
      "Epoch 44\n",
      "--------------------\n",
      "Train loss: 16.367|Train Rec: -0.282 | Val loss: 16.311, Val Rec: -0.327\n",
      "\n",
      "Epoch 45\n",
      "--------------------\n",
      "Train loss: 16.371|Train Rec: -0.278 | Val loss: 16.305, Val Rec: -0.332\n",
      "\n",
      "Epoch 46\n",
      "--------------------\n",
      "Train loss: 16.370|Train Rec: -0.279 | Val loss: 16.310, Val Rec: -0.326\n",
      "\n",
      "Epoch 47\n",
      "--------------------\n",
      "Train loss: 16.368|Train Rec: -0.281 | Val loss: 16.306, Val Rec: -0.331\n",
      "\n",
      "Epoch 48\n",
      "--------------------\n",
      "Train loss: 16.365|Train Rec: -0.284 | Val loss: 16.307, Val Rec: -0.329\n",
      "\n",
      "Epoch 49\n",
      "--------------------\n",
      "Train loss: 16.364|Train Rec: -0.285 | Val loss: 16.306, Val Rec: -0.331\n",
      "\n",
      "Epoch 50\n",
      "--------------------\n",
      "Train loss: 16.365|Train Rec: -0.283 | Val loss: 16.303, Val Rec: -0.334\n",
      "\n",
      "Epoch 51\n",
      "--------------------\n",
      "Train loss: 16.363|Train Rec: -0.285 | Val loss: 16.300, Val Rec: -0.337\n",
      "\n",
      "Epoch 52\n",
      "--------------------\n",
      "Train loss: 16.362|Train Rec: -0.287 | Val loss: 16.298, Val Rec: -0.339\n",
      "\n",
      "Epoch 53\n",
      "--------------------\n",
      "Train loss: 16.360|Train Rec: -0.289 | Val loss: 16.315, Val Rec: -0.321\n",
      "\n",
      "Epoch 54\n",
      "--------------------\n",
      "Train loss: 16.364|Train Rec: -0.284 | Val loss: 16.301, Val Rec: -0.335\n",
      "\n",
      "Epoch 55\n",
      "--------------------\n",
      "Train loss: 16.366|Train Rec: -0.283 | Val loss: 16.299, Val Rec: -0.338\n",
      "\n",
      "Epoch 56\n",
      "--------------------\n",
      "Train loss: 16.357|Train Rec: -0.292 | Val loss: 16.290, Val Rec: -0.346\n",
      "\n",
      "Epoch 57\n",
      "--------------------\n",
      "Train loss: 16.359|Train Rec: -0.290 | Val loss: 16.300, Val Rec: -0.337\n",
      "\n",
      "Epoch 58\n",
      "--------------------\n",
      "Train loss: 16.361|Train Rec: -0.287 | Val loss: 16.297, Val Rec: -0.339\n",
      "\n",
      "Epoch 59\n",
      "--------------------\n",
      "Train loss: 16.358|Train Rec: -0.290 | Val loss: 16.295, Val Rec: -0.342\n",
      "\n",
      "Epoch 60\n",
      "--------------------\n",
      "Train loss: 16.356|Train Rec: -0.292 | Val loss: 16.291, Val Rec: -0.346\n",
      "\n",
      "Epoch 61\n",
      "--------------------\n",
      "Train loss: 16.359|Train Rec: -0.290 | Val loss: 16.290, Val Rec: -0.347\n",
      "\n",
      "Epoch 62\n",
      "--------------------\n",
      "Train loss: 16.357|Train Rec: -0.292 | Val loss: 16.297, Val Rec: -0.341\n",
      "\n",
      "Epoch 63\n",
      "--------------------\n",
      "Train loss: 16.357|Train Rec: -0.291 | Val loss: 16.294, Val Rec: -0.342\n",
      "\n",
      "Epoch 64\n",
      "--------------------\n",
      "Train loss: 16.357|Train Rec: -0.292 | Val loss: 16.304, Val Rec: -0.332\n",
      "\n",
      "Epoch 65\n",
      "--------------------\n",
      "Train loss: 16.361|Train Rec: -0.287 | Val loss: 16.282, Val Rec: -0.355\n",
      "\n",
      "Epoch 66\n",
      "--------------------\n",
      "Train loss: 16.356|Train Rec: -0.293 | Val loss: 16.289, Val Rec: -0.349\n",
      "\n",
      "Epoch 67\n",
      "--------------------\n",
      "Train loss: 16.357|Train Rec: -0.292 | Val loss: 16.301, Val Rec: -0.335\n",
      "\n",
      "Epoch 68\n",
      "--------------------\n",
      "Train loss: 16.355|Train Rec: -0.292 | Val loss: 16.295, Val Rec: -0.341\n",
      "\n",
      "Epoch 69\n",
      "--------------------\n",
      "Train loss: 16.355|Train Rec: -0.294 | Val loss: 16.291, Val Rec: -0.345\n",
      "\n",
      "Epoch 70\n",
      "--------------------\n",
      "Train loss: 16.354|Train Rec: -0.294 | Val loss: 16.291, Val Rec: -0.345\n",
      "\n",
      "Epoch 71\n",
      "--------------------\n",
      "Train loss: 16.355|Train Rec: -0.294 | Val loss: 16.290, Val Rec: -0.346\n",
      "\n",
      "Epoch 72\n",
      "--------------------\n",
      "Train loss: 16.357|Train Rec: -0.292 | Val loss: 16.302, Val Rec: -0.334\n",
      "\n",
      "Epoch 73\n",
      "--------------------\n",
      "Train loss: 16.352|Train Rec: -0.296 | Val loss: 16.292, Val Rec: -0.345\n",
      "\n",
      "Patience exceeded at 73 with last checkpoint saved at 65\n",
      "changed learning rate to 0.001\n",
      "Epoch 74\n",
      "--------------------\n",
      "Train loss: 16.345|Train Rec: -0.302 | Val loss: 16.273, Val Rec: -0.362\n",
      "\n",
      "Epoch 75\n",
      "--------------------\n",
      "Train loss: 16.338|Train Rec: -0.309 | Val loss: 16.275, Val Rec: -0.361\n",
      "\n",
      "Epoch 76\n",
      "--------------------\n",
      "Train loss: 16.338|Train Rec: -0.309 | Val loss: 16.275, Val Rec: -0.361\n",
      "\n",
      "Epoch 77\n",
      "--------------------\n",
      "Train loss: 16.342|Train Rec: -0.305 | Val loss: 16.280, Val Rec: -0.356\n",
      "\n",
      "Epoch 78\n",
      "--------------------\n",
      "Train loss: 16.342|Train Rec: -0.305 | Val loss: 16.272, Val Rec: -0.364\n",
      "\n",
      "Epoch 79\n",
      "--------------------\n",
      "Train loss: 16.337|Train Rec: -0.310 | Val loss: 16.267, Val Rec: -0.369\n",
      "\n",
      "Epoch 80\n",
      "--------------------\n",
      "Train loss: 16.338|Train Rec: -0.309 | Val loss: 16.274, Val Rec: -0.362\n",
      "\n",
      "Epoch 81\n",
      "--------------------\n",
      "Train loss: 16.338|Train Rec: -0.309 | Val loss: 16.267, Val Rec: -0.369\n",
      "\n",
      "Epoch 82\n",
      "--------------------\n",
      "Train loss: 16.339|Train Rec: -0.309 | Val loss: 16.273, Val Rec: -0.362\n",
      "\n",
      "Epoch 83\n",
      "--------------------\n",
      "Train loss: 16.338|Train Rec: -0.309 | Val loss: 16.268, Val Rec: -0.368\n",
      "\n",
      "Epoch 84\n",
      "--------------------\n",
      "Train loss: 16.339|Train Rec: -0.308 | Val loss: 16.270, Val Rec: -0.366\n",
      "\n",
      "Epoch 85\n",
      "--------------------\n",
      "Train loss: 16.339|Train Rec: -0.308 | Val loss: 16.266, Val Rec: -0.370\n",
      "\n",
      "Epoch 86\n",
      "--------------------\n",
      "Train loss: 16.337|Train Rec: -0.310 | Val loss: 16.269, Val Rec: -0.367\n",
      "\n",
      "Epoch 87\n",
      "--------------------\n",
      "Train loss: 16.338|Train Rec: -0.309 | Val loss: 16.273, Val Rec: -0.363\n",
      "\n",
      "Epoch 88\n",
      "--------------------\n",
      "Train loss: 16.339|Train Rec: -0.308 | Val loss: 16.270, Val Rec: -0.366\n",
      "\n",
      "Epoch 89\n",
      "--------------------\n",
      "Train loss: 16.336|Train Rec: -0.311 | Val loss: 16.269, Val Rec: -0.367\n",
      "\n",
      "Epoch 90\n",
      "--------------------\n",
      "Train loss: 16.336|Train Rec: -0.311 | Val loss: 16.273, Val Rec: -0.363\n",
      "\n",
      "Epoch 91\n",
      "--------------------\n",
      "Train loss: 16.337|Train Rec: -0.309 | Val loss: 16.262, Val Rec: -0.374\n",
      "\n",
      "Epoch 92\n",
      "--------------------\n",
      "Train loss: 16.337|Train Rec: -0.311 | Val loss: 16.271, Val Rec: -0.364\n",
      "\n",
      "Epoch 93\n",
      "--------------------\n",
      "Train loss: 16.340|Train Rec: -0.307 | Val loss: 16.270, Val Rec: -0.366\n",
      "\n",
      "Epoch 94\n",
      "--------------------\n",
      "Train loss: 16.336|Train Rec: -0.312 | Val loss: 16.265, Val Rec: -0.371\n",
      "\n",
      "Epoch 95\n",
      "--------------------\n",
      "Train loss: 16.339|Train Rec: -0.309 | Val loss: 16.271, Val Rec: -0.365\n",
      "\n",
      "Epoch 96\n",
      "--------------------\n",
      "Train loss: 16.337|Train Rec: -0.309 | Val loss: 16.269, Val Rec: -0.367\n",
      "\n",
      "Epoch 97\n",
      "--------------------\n",
      "Train loss: 16.336|Train Rec: -0.311 | Val loss: 16.267, Val Rec: -0.369\n",
      "\n",
      "Epoch 98\n",
      "--------------------\n",
      "Train loss: 16.339|Train Rec: -0.309 | Val loss: 16.268, Val Rec: -0.368\n",
      "\n",
      "Epoch 99\n",
      "--------------------\n",
      "Train loss: 16.336|Train Rec: -0.311 | Val loss: 16.265, Val Rec: -0.371\n",
      "\n",
      "Epoch 100\n",
      "--------------------\n",
      "Train loss: 16.338|Train Rec: -0.309 | Val loss: 16.269, Val Rec: -0.367\n",
      "\n",
      "Epoch 101\n",
      "--------------------\n",
      "Train loss: 16.339|Train Rec: -0.309 | Val loss: 16.270, Val Rec: -0.366\n",
      "\n",
      "Epoch 102\n",
      "--------------------\n",
      "Train loss: 16.338|Train Rec: -0.309 | Val loss: 16.268, Val Rec: -0.368\n",
      "\n",
      "Early stopping at epoch 102 with last checkpoint saved at 91\n",
      "Model saved at: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\modelscbc_2_54_30\n",
      "Model: modelscbc_2_54_30_ep91_norm0_bits0.8_bs128_lr0.001 has been trained\n",
      "Using this model modelscbc_2_54_30_ep91_norm0_bits0.8_bs128_lr0.001\n",
      "Directory already exists: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\n",
      "model path c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\modelscbc_3_54_30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f47af6b5d8452ba784d4ba54550d6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing pre-training evaluation on the model in epoch 0\n",
      "\n",
      "Val loss: 25.911| Val KL: 24.953304502699112 | Val Rec: 0.958\n",
      "\n",
      "Epoch 1\n",
      "--------------------\n",
      "Train loss: 25.162|Train Rec: 0.199 | Val loss: 24.933, Val Rec: -0.027\n",
      "\n",
      "Epoch 2\n",
      "--------------------\n",
      "Train loss: 24.900|Train Rec: -0.069 | Val loss: 24.848, Val Rec: -0.107\n",
      "\n",
      "Epoch 3\n",
      "--------------------\n",
      "Train loss: 24.834|Train Rec: -0.138 | Val loss: 24.797, Val Rec: -0.158\n",
      "\n",
      "Epoch 4\n",
      "--------------------\n",
      "Train loss: 24.790|Train Rec: -0.184 | Val loss: 24.761, Val Rec: -0.198\n",
      "\n",
      "Epoch 5\n",
      "--------------------\n",
      "Train loss: 24.766|Train Rec: -0.208 | Val loss: 24.727, Val Rec: -0.227\n",
      "\n",
      "Epoch 6\n",
      "--------------------\n",
      "Train loss: 24.742|Train Rec: -0.231 | Val loss: 24.698, Val Rec: -0.258\n",
      "\n",
      "Epoch 7\n",
      "--------------------\n",
      "Train loss: 24.723|Train Rec: -0.250 | Val loss: 24.696, Val Rec: -0.261\n",
      "\n",
      "Epoch 8\n",
      "--------------------\n",
      "Train loss: 24.712|Train Rec: -0.261 | Val loss: 24.650, Val Rec: -0.309\n",
      "\n",
      "Epoch 9\n",
      "--------------------\n",
      "Train loss: 24.701|Train Rec: -0.271 | Val loss: 24.628, Val Rec: -0.331\n",
      "\n",
      "Epoch 10\n",
      "--------------------\n",
      "Train loss: 24.687|Train Rec: -0.285 | Val loss: 24.627, Val Rec: -0.329\n",
      "\n",
      "Epoch 11\n",
      "--------------------\n",
      "Train loss: 24.677|Train Rec: -0.295 | Val loss: 24.624, Val Rec: -0.332\n",
      "\n",
      "Epoch 12\n",
      "--------------------\n",
      "Train loss: 24.674|Train Rec: -0.297 | Val loss: 24.611, Val Rec: -0.345\n",
      "\n",
      "Epoch 13\n",
      "--------------------\n",
      "Train loss: 24.667|Train Rec: -0.304 | Val loss: 24.608, Val Rec: -0.349\n",
      "\n",
      "Epoch 14\n",
      "--------------------\n",
      "Train loss: 24.660|Train Rec: -0.311 | Val loss: 24.586, Val Rec: -0.371\n",
      "\n",
      "Epoch 15\n",
      "--------------------\n",
      "Train loss: 24.654|Train Rec: -0.316 | Val loss: 24.585, Val Rec: -0.371\n",
      "\n",
      "Epoch 16\n",
      "--------------------\n",
      "Train loss: 24.648|Train Rec: -0.322 | Val loss: 24.577, Val Rec: -0.380\n",
      "\n",
      "Epoch 17\n",
      "--------------------\n",
      "Train loss: 24.641|Train Rec: -0.329 | Val loss: 24.577, Val Rec: -0.379\n",
      "\n",
      "Epoch 18\n",
      "--------------------\n",
      "Train loss: 24.637|Train Rec: -0.333 | Val loss: 24.562, Val Rec: -0.394\n",
      "\n",
      "Epoch 19\n",
      "--------------------\n",
      "Train loss: 24.631|Train Rec: -0.338 | Val loss: 24.557, Val Rec: -0.400\n",
      "\n",
      "Epoch 20\n",
      "--------------------\n",
      "Train loss: 24.633|Train Rec: -0.337 | Val loss: 24.562, Val Rec: -0.395\n",
      "\n",
      "Epoch 21\n",
      "--------------------\n",
      "Train loss: 24.625|Train Rec: -0.344 | Val loss: 24.551, Val Rec: -0.404\n",
      "\n",
      "Epoch 22\n",
      "--------------------\n",
      "Train loss: 24.621|Train Rec: -0.348 | Val loss: 24.540, Val Rec: -0.416\n",
      "\n",
      "Epoch 23\n",
      "--------------------\n",
      "Train loss: 24.618|Train Rec: -0.351 | Val loss: 24.531, Val Rec: -0.424\n",
      "\n",
      "Epoch 24\n",
      "--------------------\n",
      "Train loss: 24.615|Train Rec: -0.353 | Val loss: 24.531, Val Rec: -0.425\n",
      "\n",
      "Epoch 25\n",
      "--------------------\n",
      "Train loss: 24.611|Train Rec: -0.358 | Val loss: 24.539, Val Rec: -0.416\n",
      "\n",
      "Epoch 26\n",
      "--------------------\n",
      "Train loss: 24.605|Train Rec: -0.363 | Val loss: 24.525, Val Rec: -0.432\n",
      "\n",
      "Epoch 27\n",
      "--------------------\n",
      "Train loss: 24.608|Train Rec: -0.359 | Val loss: 24.525, Val Rec: -0.430\n",
      "\n",
      "Epoch 28\n",
      "--------------------\n",
      "Train loss: 24.600|Train Rec: -0.367 | Val loss: 24.511, Val Rec: -0.444\n",
      "\n",
      "Epoch 29\n",
      "--------------------\n",
      "Train loss: 24.598|Train Rec: -0.369 | Val loss: 24.521, Val Rec: -0.435\n",
      "\n",
      "Epoch 30\n",
      "--------------------\n",
      "Train loss: 24.599|Train Rec: -0.368 | Val loss: 24.504, Val Rec: -0.451\n",
      "\n",
      "Epoch 31\n",
      "--------------------\n",
      "Train loss: 24.595|Train Rec: -0.372 | Val loss: 24.510, Val Rec: -0.445\n",
      "\n",
      "Epoch 32\n",
      "--------------------\n",
      "Train loss: 24.592|Train Rec: -0.375 | Val loss: 24.513, Val Rec: -0.443\n",
      "\n",
      "Epoch 33\n",
      "--------------------\n",
      "Train loss: 24.592|Train Rec: -0.375 | Val loss: 24.509, Val Rec: -0.445\n",
      "\n",
      "Epoch 34\n",
      "--------------------\n",
      "Train loss: 24.587|Train Rec: -0.379 | Val loss: 24.503, Val Rec: -0.452\n",
      "\n",
      "Epoch 35\n",
      "--------------------\n",
      "Train loss: 24.590|Train Rec: -0.376 | Val loss: 24.499, Val Rec: -0.455\n",
      "\n",
      "Epoch 36\n",
      "--------------------\n",
      "Train loss: 24.589|Train Rec: -0.379 | Val loss: 24.507, Val Rec: -0.449\n",
      "\n",
      "Epoch 37\n",
      "--------------------\n",
      "Train loss: 24.580|Train Rec: -0.386 | Val loss: 24.493, Val Rec: -0.461\n",
      "\n",
      "Epoch 38\n",
      "--------------------\n",
      "Train loss: 24.582|Train Rec: -0.384 | Val loss: 24.491, Val Rec: -0.464\n",
      "\n",
      "Epoch 39\n",
      "--------------------\n",
      "Train loss: 24.580|Train Rec: -0.386 | Val loss: 24.496, Val Rec: -0.459\n",
      "\n",
      "Epoch 40\n",
      "--------------------\n",
      "Train loss: 24.580|Train Rec: -0.386 | Val loss: 24.493, Val Rec: -0.463\n",
      "\n",
      "Epoch 41\n",
      "--------------------\n",
      "Train loss: 24.578|Train Rec: -0.389 | Val loss: 24.482, Val Rec: -0.473\n",
      "\n",
      "Epoch 42\n",
      "--------------------\n",
      "Train loss: 24.575|Train Rec: -0.392 | Val loss: 24.482, Val Rec: -0.473\n",
      "\n",
      "Epoch 43\n",
      "--------------------\n",
      "Train loss: 24.581|Train Rec: -0.386 | Val loss: 24.486, Val Rec: -0.468\n",
      "\n",
      "Epoch 44\n",
      "--------------------\n",
      "Train loss: 24.579|Train Rec: -0.388 | Val loss: 24.478, Val Rec: -0.476\n",
      "\n",
      "Epoch 45\n",
      "--------------------\n",
      "Train loss: 24.577|Train Rec: -0.389 | Val loss: 24.480, Val Rec: -0.475\n",
      "\n",
      "Epoch 46\n",
      "--------------------\n",
      "Train loss: 24.578|Train Rec: -0.389 | Val loss: 24.478, Val Rec: -0.477\n",
      "\n",
      "Epoch 47\n",
      "--------------------\n",
      "Train loss: 24.574|Train Rec: -0.392 | Val loss: 24.481, Val Rec: -0.473\n",
      "\n",
      "Epoch 48\n",
      "--------------------\n",
      "Train loss: 24.574|Train Rec: -0.392 | Val loss: 24.479, Val Rec: -0.476\n",
      "\n",
      "Epoch 49\n",
      "--------------------\n",
      "Train loss: 24.570|Train Rec: -0.396 | Val loss: 24.480, Val Rec: -0.475\n",
      "\n",
      "Epoch 50\n",
      "--------------------\n",
      "Train loss: 24.576|Train Rec: -0.390 | Val loss: 24.473, Val Rec: -0.483\n",
      "\n",
      "Epoch 51\n",
      "--------------------\n",
      "Train loss: 24.571|Train Rec: -0.395 | Val loss: 24.476, Val Rec: -0.478\n",
      "\n",
      "Epoch 52\n",
      "--------------------\n",
      "Train loss: 24.573|Train Rec: -0.393 | Val loss: 24.477, Val Rec: -0.479\n",
      "\n",
      "Epoch 53\n",
      "--------------------\n",
      "Train loss: 24.566|Train Rec: -0.401 | Val loss: 24.477, Val Rec: -0.477\n",
      "\n",
      "Epoch 54\n",
      "--------------------\n",
      "Train loss: 24.569|Train Rec: -0.398 | Val loss: 24.480, Val Rec: -0.473\n",
      "\n",
      "Epoch 55\n",
      "--------------------\n",
      "Train loss: 24.567|Train Rec: -0.399 | Val loss: 24.476, Val Rec: -0.478\n",
      "\n",
      "Epoch 56\n",
      "--------------------\n",
      "Train loss: 24.569|Train Rec: -0.397 | Val loss: 24.495, Val Rec: -0.460\n",
      "\n",
      "Epoch 57\n",
      "--------------------\n",
      "Train loss: 24.572|Train Rec: -0.393 | Val loss: 24.476, Val Rec: -0.478\n",
      "\n",
      "Epoch 58\n",
      "--------------------\n",
      "Train loss: 24.565|Train Rec: -0.401 | Val loss: 24.474, Val Rec: -0.480\n",
      "\n",
      "Patience exceeded at 58 with last checkpoint saved at 50\n",
      "changed learning rate to 0.001\n",
      "Epoch 59\n",
      "--------------------\n",
      "Train loss: 24.554|Train Rec: -0.411 | Val loss: 24.450, Val Rec: -0.504\n",
      "\n",
      "Epoch 60\n",
      "--------------------\n",
      "Train loss: 24.551|Train Rec: -0.415 | Val loss: 24.451, Val Rec: -0.503\n",
      "\n",
      "Epoch 61\n",
      "--------------------\n",
      "Train loss: 24.546|Train Rec: -0.418 | Val loss: 24.451, Val Rec: -0.503\n",
      "\n",
      "Epoch 62\n",
      "--------------------\n",
      "Train loss: 24.548|Train Rec: -0.417 | Val loss: 24.447, Val Rec: -0.507\n",
      "\n",
      "Epoch 63\n",
      "--------------------\n",
      "Train loss: 24.547|Train Rec: -0.418 | Val loss: 24.449, Val Rec: -0.504\n",
      "\n",
      "Epoch 64\n",
      "--------------------\n",
      "Train loss: 24.544|Train Rec: -0.420 | Val loss: 24.448, Val Rec: -0.506\n",
      "\n",
      "Epoch 65\n",
      "--------------------\n",
      "Train loss: 24.547|Train Rec: -0.417 | Val loss: 24.448, Val Rec: -0.506\n",
      "\n",
      "Epoch 66\n",
      "--------------------\n",
      "Train loss: 24.545|Train Rec: -0.420 | Val loss: 24.446, Val Rec: -0.507\n",
      "\n",
      "Epoch 67\n",
      "--------------------\n",
      "Train loss: 24.545|Train Rec: -0.420 | Val loss: 24.452, Val Rec: -0.502\n",
      "\n",
      "Epoch 68\n",
      "--------------------\n",
      "Train loss: 24.545|Train Rec: -0.420 | Val loss: 24.450, Val Rec: -0.503\n",
      "\n",
      "Epoch 69\n",
      "--------------------\n",
      "Train loss: 24.544|Train Rec: -0.420 | Val loss: 24.447, Val Rec: -0.507\n",
      "\n",
      "Epoch 70\n",
      "--------------------\n",
      "Train loss: 24.545|Train Rec: -0.420 | Val loss: 24.447, Val Rec: -0.507\n",
      "\n",
      "Epoch 71\n",
      "--------------------\n",
      "Train loss: 24.543|Train Rec: -0.421 | Val loss: 24.446, Val Rec: -0.508\n",
      "\n",
      "Epoch 72\n",
      "--------------------\n",
      "Train loss: 24.541|Train Rec: -0.423 | Val loss: 24.447, Val Rec: -0.507\n",
      "\n",
      "Epoch 73\n",
      "--------------------\n",
      "Train loss: 24.544|Train Rec: -0.420 | Val loss: 24.441, Val Rec: -0.513\n",
      "\n",
      "Epoch 74\n",
      "--------------------\n",
      "Train loss: 24.544|Train Rec: -0.420 | Val loss: 24.445, Val Rec: -0.509\n",
      "\n",
      "Epoch 75\n",
      "--------------------\n",
      "Train loss: 24.541|Train Rec: -0.423 | Val loss: 24.441, Val Rec: -0.512\n",
      "\n",
      "Epoch 76\n",
      "--------------------\n",
      "Train loss: 24.542|Train Rec: -0.423 | Val loss: 24.446, Val Rec: -0.508\n",
      "\n",
      "Epoch 77\n",
      "--------------------\n",
      "Train loss: 24.542|Train Rec: -0.421 | Val loss: 24.441, Val Rec: -0.512\n",
      "\n",
      "Epoch 78\n",
      "--------------------\n",
      "Train loss: 24.544|Train Rec: -0.420 | Val loss: 24.443, Val Rec: -0.510\n",
      "\n",
      "Epoch 79\n",
      "--------------------\n",
      "Train loss: 24.541|Train Rec: -0.423 | Val loss: 24.439, Val Rec: -0.514\n",
      "\n",
      "Epoch 80\n",
      "--------------------\n",
      "Train loss: 24.542|Train Rec: -0.422 | Val loss: 24.439, Val Rec: -0.514\n",
      "\n",
      "Epoch 81\n",
      "--------------------\n",
      "Train loss: 24.544|Train Rec: -0.420 | Val loss: 24.441, Val Rec: -0.513\n",
      "\n",
      "Epoch 82\n",
      "--------------------\n",
      "Train loss: 24.545|Train Rec: -0.419 | Val loss: 24.444, Val Rec: -0.510\n",
      "\n",
      "Epoch 83\n",
      "--------------------\n",
      "Train loss: 24.541|Train Rec: -0.423 | Val loss: 24.434, Val Rec: -0.519\n",
      "\n",
      "Epoch 84\n",
      "--------------------\n",
      "Train loss: 24.543|Train Rec: -0.422 | Val loss: 24.440, Val Rec: -0.514\n",
      "\n",
      "Epoch 85\n",
      "--------------------\n",
      "Train loss: 24.543|Train Rec: -0.421 | Val loss: 24.442, Val Rec: -0.511\n",
      "\n",
      "Epoch 86\n",
      "--------------------\n",
      "Train loss: 24.543|Train Rec: -0.421 | Val loss: 24.438, Val Rec: -0.516\n",
      "\n",
      "Epoch 87\n",
      "--------------------\n",
      "Train loss: 24.542|Train Rec: -0.423 | Val loss: 24.439, Val Rec: -0.514\n",
      "\n",
      "Epoch 88\n",
      "--------------------\n",
      "Train loss: 24.543|Train Rec: -0.422 | Val loss: 24.444, Val Rec: -0.510\n",
      "\n",
      "Epoch 89\n",
      "--------------------\n",
      "Train loss: 24.542|Train Rec: -0.423 | Val loss: 24.437, Val Rec: -0.517\n",
      "\n",
      "Epoch 90\n",
      "--------------------\n",
      "Train loss: 24.539|Train Rec: -0.425 | Val loss: 24.437, Val Rec: -0.517\n",
      "\n",
      "Epoch 91\n",
      "--------------------\n",
      "Train loss: 24.541|Train Rec: -0.423 | Val loss: 24.442, Val Rec: -0.511\n",
      "\n",
      "Epoch 92\n",
      "--------------------\n",
      "Train loss: 24.539|Train Rec: -0.425 | Val loss: 24.436, Val Rec: -0.518\n",
      "\n",
      "Epoch 93\n",
      "--------------------\n",
      "Train loss: 24.539|Train Rec: -0.426 | Val loss: 24.443, Val Rec: -0.510\n",
      "\n",
      "Epoch 94\n",
      "--------------------\n",
      "Train loss: 24.543|Train Rec: -0.422 | Val loss: 24.439, Val Rec: -0.515\n",
      "\n",
      "Early stopping at epoch 94 with last checkpoint saved at 83\n",
      "Model saved at: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\modelscbc_3_54_30\n",
      "Model: modelscbc_3_54_30_ep83_norm0_bits1.2_bs128_lr0.001 has been trained\n",
      "Using this model modelscbc_3_54_30_ep83_norm0_bits1.2_bs128_lr0.001\n",
      "Directory already exists: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\n",
      "model path c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\modelscbc_4_54_30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59fdc84affd542ecb019481830432244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing pre-training evaluation on the model in epoch 0\n",
      "\n",
      "Val loss: 34.237| Val KL: 33.27106136745877 | Val Rec: 0.966\n",
      "\n",
      "Epoch 1\n",
      "--------------------\n",
      "Train loss: 33.469|Train Rec: 0.189 | Val loss: 33.215, Val Rec: -0.060\n",
      "\n",
      "Epoch 2\n",
      "--------------------\n",
      "Train loss: 33.184|Train Rec: -0.105 | Val loss: 33.105, Val Rec: -0.167\n",
      "\n",
      "Epoch 3\n",
      "--------------------\n",
      "Train loss: 33.105|Train Rec: -0.185 | Val loss: 33.042, Val Rec: -0.231\n",
      "\n",
      "Epoch 4\n",
      "--------------------\n",
      "Train loss: 33.058|Train Rec: -0.233 | Val loss: 33.006, Val Rec: -0.267\n",
      "\n",
      "Epoch 5\n",
      "--------------------\n",
      "Train loss: 33.026|Train Rec: -0.266 | Val loss: 32.981, Val Rec: -0.291\n",
      "\n",
      "Epoch 6\n",
      "--------------------\n",
      "Train loss: 33.001|Train Rec: -0.290 | Val loss: 32.939, Val Rec: -0.336\n",
      "\n",
      "Epoch 7\n",
      "--------------------\n",
      "Train loss: 32.976|Train Rec: -0.314 | Val loss: 32.911, Val Rec: -0.362\n",
      "\n",
      "Epoch 8\n",
      "--------------------\n",
      "Train loss: 32.962|Train Rec: -0.328 | Val loss: 32.908, Val Rec: -0.365\n",
      "\n",
      "Epoch 9\n",
      "--------------------\n",
      "Train loss: 32.947|Train Rec: -0.343 | Val loss: 32.879, Val Rec: -0.395\n",
      "\n",
      "Epoch 10\n",
      "--------------------\n",
      "Train loss: 32.936|Train Rec: -0.354 | Val loss: 32.855, Val Rec: -0.419\n",
      "\n",
      "Epoch 11\n",
      "--------------------\n",
      "Train loss: 32.923|Train Rec: -0.366 | Val loss: 32.851, Val Rec: -0.423\n",
      "\n",
      "Epoch 12\n",
      "--------------------\n",
      "Train loss: 32.911|Train Rec: -0.378 | Val loss: 32.822, Val Rec: -0.457\n",
      "\n",
      "Epoch 13\n",
      "--------------------\n",
      "Train loss: 32.904|Train Rec: -0.385 | Val loss: 32.830, Val Rec: -0.443\n",
      "\n",
      "Epoch 14\n",
      "--------------------\n",
      "Train loss: 32.898|Train Rec: -0.390 | Val loss: 32.812, Val Rec: -0.463\n",
      "\n",
      "Epoch 15\n",
      "--------------------\n",
      "Train loss: 32.892|Train Rec: -0.395 | Val loss: 32.805, Val Rec: -0.470\n",
      "\n",
      "Epoch 16\n",
      "--------------------\n",
      "Train loss: 32.884|Train Rec: -0.403 | Val loss: 32.789, Val Rec: -0.485\n",
      "\n",
      "Epoch 17\n",
      "--------------------\n",
      "Train loss: 32.879|Train Rec: -0.407 | Val loss: 32.783, Val Rec: -0.492\n",
      "\n",
      "Epoch 18\n",
      "--------------------\n",
      "Train loss: 32.872|Train Rec: -0.415 | Val loss: 32.764, Val Rec: -0.511\n",
      "\n",
      "Epoch 19\n",
      "--------------------\n",
      "Train loss: 32.870|Train Rec: -0.417 | Val loss: 32.762, Val Rec: -0.515\n",
      "\n",
      "Epoch 20\n",
      "--------------------\n",
      "Train loss: 32.857|Train Rec: -0.429 | Val loss: 32.754, Val Rec: -0.521\n",
      "\n",
      "Epoch 21\n",
      "--------------------\n",
      "Train loss: 32.863|Train Rec: -0.423 | Val loss: 32.757, Val Rec: -0.520\n",
      "\n",
      "Epoch 22\n",
      "--------------------\n",
      "Train loss: 32.853|Train Rec: -0.433 | Val loss: 32.747, Val Rec: -0.529\n",
      "\n",
      "Epoch 23\n",
      "--------------------\n",
      "Train loss: 32.846|Train Rec: -0.440 | Val loss: 32.736, Val Rec: -0.538\n",
      "\n",
      "Epoch 24\n",
      "--------------------\n",
      "Train loss: 32.846|Train Rec: -0.441 | Val loss: 32.735, Val Rec: -0.539\n",
      "\n",
      "Epoch 25\n",
      "--------------------\n",
      "Train loss: 32.845|Train Rec: -0.441 | Val loss: 32.735, Val Rec: -0.539\n",
      "\n",
      "Epoch 26\n",
      "--------------------\n",
      "Train loss: 32.838|Train Rec: -0.447 | Val loss: 32.734, Val Rec: -0.538\n",
      "\n",
      "Epoch 27\n",
      "--------------------\n",
      "Train loss: 32.832|Train Rec: -0.454 | Val loss: 32.731, Val Rec: -0.545\n",
      "\n",
      "Epoch 28\n",
      "--------------------\n",
      "Train loss: 32.834|Train Rec: -0.452 | Val loss: 32.736, Val Rec: -0.537\n",
      "\n",
      "Epoch 29\n",
      "--------------------\n",
      "Train loss: 32.832|Train Rec: -0.454 | Val loss: 32.730, Val Rec: -0.544\n",
      "\n",
      "Epoch 30\n",
      "--------------------\n",
      "Train loss: 32.828|Train Rec: -0.458 | Val loss: 32.712, Val Rec: -0.562\n",
      "\n",
      "Epoch 31\n",
      "--------------------\n",
      "Train loss: 32.823|Train Rec: -0.462 | Val loss: 32.708, Val Rec: -0.565\n",
      "\n",
      "Epoch 32\n",
      "--------------------\n",
      "Train loss: 32.826|Train Rec: -0.459 | Val loss: 32.722, Val Rec: -0.551\n",
      "\n",
      "Epoch 33\n",
      "--------------------\n",
      "Train loss: 32.824|Train Rec: -0.461 | Val loss: 32.708, Val Rec: -0.566\n",
      "\n",
      "Epoch 34\n",
      "--------------------\n",
      "Train loss: 32.822|Train Rec: -0.463 | Val loss: 32.734, Val Rec: -0.539\n",
      "\n",
      "Epoch 35\n",
      "--------------------\n",
      "Train loss: 32.824|Train Rec: -0.460 | Val loss: 32.704, Val Rec: -0.570\n",
      "\n",
      "Epoch 36\n",
      "--------------------\n",
      "Train loss: 32.818|Train Rec: -0.467 | Val loss: 32.698, Val Rec: -0.575\n",
      "\n",
      "Epoch 37\n",
      "--------------------\n",
      "Train loss: 32.813|Train Rec: -0.471 | Val loss: 32.701, Val Rec: -0.571\n",
      "\n",
      "Epoch 38\n",
      "--------------------\n",
      "Train loss: 32.812|Train Rec: -0.472 | Val loss: 32.696, Val Rec: -0.577\n",
      "\n",
      "Epoch 39\n",
      "--------------------\n",
      "Train loss: 32.810|Train Rec: -0.474 | Val loss: 32.695, Val Rec: -0.577\n",
      "\n",
      "Epoch 40\n",
      "--------------------\n",
      "Train loss: 32.811|Train Rec: -0.472 | Val loss: 32.695, Val Rec: -0.578\n",
      "\n",
      "Epoch 41\n",
      "--------------------\n",
      "Train loss: 32.807|Train Rec: -0.477 | Val loss: 32.679, Val Rec: -0.593\n",
      "\n",
      "Epoch 42\n",
      "--------------------\n",
      "Train loss: 32.806|Train Rec: -0.477 | Val loss: 32.682, Val Rec: -0.590\n",
      "\n",
      "Epoch 43\n",
      "--------------------\n",
      "Train loss: 32.803|Train Rec: -0.480 | Val loss: 32.684, Val Rec: -0.590\n",
      "\n",
      "Epoch 44\n",
      "--------------------\n",
      "Train loss: 32.801|Train Rec: -0.482 | Val loss: 32.677, Val Rec: -0.596\n",
      "\n",
      "Epoch 45\n",
      "--------------------\n",
      "Train loss: 32.807|Train Rec: -0.477 | Val loss: 32.687, Val Rec: -0.585\n",
      "\n",
      "Epoch 46\n",
      "--------------------\n",
      "Train loss: 32.799|Train Rec: -0.484 | Val loss: 32.675, Val Rec: -0.597\n",
      "\n",
      "Epoch 47\n",
      "--------------------\n",
      "Train loss: 32.798|Train Rec: -0.486 | Val loss: 32.673, Val Rec: -0.600\n",
      "\n",
      "Epoch 48\n",
      "--------------------\n",
      "Train loss: 32.798|Train Rec: -0.487 | Val loss: 32.667, Val Rec: -0.606\n",
      "\n",
      "Epoch 49\n",
      "--------------------\n",
      "Train loss: 32.796|Train Rec: -0.487 | Val loss: 32.673, Val Rec: -0.599\n",
      "\n",
      "Epoch 50\n",
      "--------------------\n",
      "Train loss: 32.797|Train Rec: -0.486 | Val loss: 32.673, Val Rec: -0.600\n",
      "\n",
      "Epoch 51\n",
      "--------------------\n",
      "Train loss: 32.795|Train Rec: -0.488 | Val loss: 32.673, Val Rec: -0.600\n",
      "\n",
      "Epoch 52\n",
      "--------------------\n",
      "Train loss: 32.798|Train Rec: -0.486 | Val loss: 32.697, Val Rec: -0.575\n",
      "\n",
      "Epoch 53\n",
      "--------------------\n",
      "Train loss: 32.791|Train Rec: -0.492 | Val loss: 32.665, Val Rec: -0.607\n",
      "\n",
      "Epoch 54\n",
      "--------------------\n",
      "Train loss: 32.792|Train Rec: -0.491 | Val loss: 32.676, Val Rec: -0.596\n",
      "\n",
      "Epoch 55\n",
      "--------------------\n",
      "Train loss: 32.793|Train Rec: -0.490 | Val loss: 32.672, Val Rec: -0.600\n",
      "\n",
      "Epoch 56\n",
      "--------------------\n",
      "Train loss: 32.792|Train Rec: -0.490 | Val loss: 32.667, Val Rec: -0.607\n",
      "\n",
      "Epoch 57\n",
      "--------------------\n",
      "Train loss: 32.790|Train Rec: -0.493 | Val loss: 32.671, Val Rec: -0.601\n",
      "\n",
      "Epoch 58\n",
      "--------------------\n",
      "Train loss: 32.791|Train Rec: -0.493 | Val loss: 32.651, Val Rec: -0.621\n",
      "\n",
      "Epoch 59\n",
      "--------------------\n",
      "Train loss: 32.790|Train Rec: -0.493 | Val loss: 32.665, Val Rec: -0.608\n",
      "\n",
      "Epoch 60\n",
      "--------------------\n",
      "Train loss: 32.787|Train Rec: -0.496 | Val loss: 32.656, Val Rec: -0.616\n",
      "\n",
      "Epoch 61\n",
      "--------------------\n",
      "Train loss: 32.785|Train Rec: -0.497 | Val loss: 32.664, Val Rec: -0.609\n",
      "\n",
      "Epoch 62\n",
      "--------------------\n",
      "Train loss: 32.785|Train Rec: -0.498 | Val loss: 32.661, Val Rec: -0.611\n",
      "\n",
      "Epoch 63\n",
      "--------------------\n",
      "Train loss: 32.782|Train Rec: -0.500 | Val loss: 32.654, Val Rec: -0.618\n",
      "\n",
      "Epoch 64\n",
      "--------------------\n",
      "Train loss: 32.784|Train Rec: -0.499 | Val loss: 32.655, Val Rec: -0.617\n",
      "\n",
      "Epoch 65\n",
      "--------------------\n",
      "Train loss: 32.782|Train Rec: -0.500 | Val loss: 32.653, Val Rec: -0.618\n",
      "\n",
      "Epoch 66\n",
      "--------------------\n",
      "Train loss: 32.784|Train Rec: -0.499 | Val loss: 32.665, Val Rec: -0.606\n",
      "\n",
      "Patience exceeded at 66 with last checkpoint saved at 58\n",
      "changed learning rate to 0.001\n",
      "Epoch 67\n",
      "--------------------\n",
      "Train loss: 32.771|Train Rec: -0.510 | Val loss: 32.632, Val Rec: -0.640\n",
      "\n",
      "Epoch 68\n",
      "--------------------\n",
      "Train loss: 32.763|Train Rec: -0.518 | Val loss: 32.629, Val Rec: -0.643\n",
      "\n",
      "Epoch 69\n",
      "--------------------\n",
      "Train loss: 32.763|Train Rec: -0.518 | Val loss: 32.628, Val Rec: -0.643\n",
      "\n",
      "Epoch 70\n",
      "--------------------\n",
      "Train loss: 32.760|Train Rec: -0.521 | Val loss: 32.631, Val Rec: -0.640\n",
      "\n",
      "Epoch 71\n",
      "--------------------\n",
      "Train loss: 32.761|Train Rec: -0.520 | Val loss: 32.631, Val Rec: -0.641\n",
      "\n",
      "Epoch 72\n",
      "--------------------\n",
      "Train loss: 32.761|Train Rec: -0.521 | Val loss: 32.630, Val Rec: -0.642\n",
      "\n",
      "Epoch 73\n",
      "--------------------\n",
      "Train loss: 32.762|Train Rec: -0.519 | Val loss: 32.627, Val Rec: -0.644\n",
      "\n",
      "Epoch 74\n",
      "--------------------\n",
      "Train loss: 32.763|Train Rec: -0.519 | Val loss: 32.632, Val Rec: -0.639\n",
      "\n",
      "Epoch 75\n",
      "--------------------\n",
      "Train loss: 32.758|Train Rec: -0.523 | Val loss: 32.632, Val Rec: -0.639\n",
      "\n",
      "Epoch 76\n",
      "--------------------\n",
      "Train loss: 32.763|Train Rec: -0.518 | Val loss: 32.633, Val Rec: -0.638\n",
      "\n",
      "Epoch 77\n",
      "--------------------\n",
      "Train loss: 32.760|Train Rec: -0.521 | Val loss: 32.632, Val Rec: -0.640\n",
      "\n",
      "Epoch 78\n",
      "--------------------\n",
      "Train loss: 32.764|Train Rec: -0.517 | Val loss: 32.629, Val Rec: -0.643\n",
      "\n",
      "Epoch 79\n",
      "--------------------\n",
      "Train loss: 32.759|Train Rec: -0.522 | Val loss: 32.624, Val Rec: -0.647\n",
      "\n",
      "Epoch 80\n",
      "--------------------\n",
      "Train loss: 32.758|Train Rec: -0.522 | Val loss: 32.625, Val Rec: -0.646\n",
      "\n",
      "Epoch 81\n",
      "--------------------\n",
      "Train loss: 32.762|Train Rec: -0.519 | Val loss: 32.632, Val Rec: -0.640\n",
      "\n",
      "Epoch 82\n",
      "--------------------\n",
      "Train loss: 32.762|Train Rec: -0.520 | Val loss: 32.625, Val Rec: -0.647\n",
      "\n",
      "Epoch 83\n",
      "--------------------\n",
      "Train loss: 32.759|Train Rec: -0.522 | Val loss: 32.631, Val Rec: -0.641\n",
      "\n",
      "Epoch 84\n",
      "--------------------\n",
      "Train loss: 32.761|Train Rec: -0.520 | Val loss: 32.630, Val Rec: -0.641\n",
      "\n",
      "Epoch 85\n",
      "--------------------\n",
      "Train loss: 32.760|Train Rec: -0.521 | Val loss: 32.628, Val Rec: -0.644\n",
      "\n",
      "Epoch 86\n",
      "--------------------\n",
      "Train loss: 32.760|Train Rec: -0.521 | Val loss: 32.624, Val Rec: -0.648\n",
      "\n",
      "Epoch 87\n",
      "--------------------\n",
      "Train loss: 32.755|Train Rec: -0.526 | Val loss: 32.622, Val Rec: -0.650\n",
      "\n",
      "Epoch 88\n",
      "--------------------\n",
      "Train loss: 32.762|Train Rec: -0.519 | Val loss: 32.634, Val Rec: -0.638\n",
      "\n",
      "Epoch 89\n",
      "--------------------\n",
      "Train loss: 32.760|Train Rec: -0.521 | Val loss: 32.628, Val Rec: -0.644\n",
      "\n",
      "Epoch 90\n",
      "--------------------\n",
      "Train loss: 32.759|Train Rec: -0.521 | Val loss: 32.630, Val Rec: -0.642\n",
      "\n",
      "Epoch 91\n",
      "--------------------\n",
      "Train loss: 32.759|Train Rec: -0.522 | Val loss: 32.623, Val Rec: -0.649\n",
      "\n",
      "Epoch 92\n",
      "--------------------\n",
      "Train loss: 32.757|Train Rec: -0.524 | Val loss: 32.624, Val Rec: -0.647\n",
      "\n",
      "Epoch 93\n",
      "--------------------\n",
      "Train loss: 32.759|Train Rec: -0.522 | Val loss: 32.623, Val Rec: -0.648\n",
      "\n",
      "Epoch 94\n",
      "--------------------\n",
      "Train loss: 32.762|Train Rec: -0.520 | Val loss: 32.626, Val Rec: -0.646\n",
      "\n",
      "Epoch 95\n",
      "--------------------\n",
      "Train loss: 32.756|Train Rec: -0.525 | Val loss: 32.626, Val Rec: -0.646\n",
      "\n",
      "Epoch 96\n",
      "--------------------\n",
      "Train loss: 32.761|Train Rec: -0.520 | Val loss: 32.626, Val Rec: -0.645\n",
      "\n",
      "Epoch 97\n",
      "--------------------\n",
      "Train loss: 32.758|Train Rec: -0.523 | Val loss: 32.625, Val Rec: -0.646\n",
      "\n",
      "Epoch 98\n",
      "--------------------\n",
      "Train loss: 32.759|Train Rec: -0.522 | Val loss: 32.625, Val Rec: -0.646\n",
      "\n",
      "Early stopping at epoch 98 with last checkpoint saved at 87\n",
      "Model saved at: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\modelscbc_4_54_30\n",
      "Model: modelscbc_4_54_30_ep87_norm0_bits1.6_bs128_lr0.001 has been trained\n",
      "Using this model modelscbc_4_54_30_ep87_norm0_bits1.6_bs128_lr0.001\n",
      "Directory already exists: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\n",
      "model path c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\modelscbc_5_54_30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e749ca8fda9b450897f61ce39cb486df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing pre-training evaluation on the model in epoch 0\n",
      "\n",
      "Val loss: 42.536| Val KL: 41.58882522583008 | Val Rec: 0.947\n",
      "\n",
      "Epoch 1\n",
      "--------------------\n",
      "Train loss: 41.768|Train Rec: 0.171 | Val loss: 41.499, Val Rec: -0.102\n",
      "\n",
      "Epoch 2\n",
      "--------------------\n",
      "Train loss: 41.458|Train Rec: -0.149 | Val loss: 41.362, Val Rec: -0.233\n",
      "\n",
      "Epoch 3\n",
      "--------------------\n",
      "Train loss: 41.368|Train Rec: -0.243 | Val loss: 41.303, Val Rec: -0.286\n",
      "\n",
      "Epoch 4\n",
      "--------------------\n",
      "Train loss: 41.308|Train Rec: -0.303 | Val loss: 41.245, Val Rec: -0.349\n",
      "\n",
      "Epoch 5\n",
      "--------------------\n",
      "Train loss: 41.274|Train Rec: -0.337 | Val loss: 41.196, Val Rec: -0.394\n",
      "\n",
      "Epoch 6\n",
      "--------------------\n",
      "Train loss: 41.237|Train Rec: -0.373 | Val loss: 41.162, Val Rec: -0.430\n",
      "\n",
      "Epoch 7\n",
      "--------------------\n",
      "Train loss: 41.216|Train Rec: -0.394 | Val loss: 41.125, Val Rec: -0.465\n",
      "\n",
      "Epoch 8\n",
      "--------------------\n",
      "Train loss: 41.191|Train Rec: -0.418 | Val loss: 41.089, Val Rec: -0.502\n",
      "\n",
      "Epoch 9\n",
      "--------------------\n",
      "Train loss: 41.180|Train Rec: -0.429 | Val loss: 41.083, Val Rec: -0.509\n",
      "\n",
      "Epoch 10\n",
      "--------------------\n",
      "Train loss: 41.164|Train Rec: -0.444 | Val loss: 41.085, Val Rec: -0.506\n",
      "\n",
      "Epoch 11\n",
      "--------------------\n",
      "Train loss: 41.149|Train Rec: -0.458 | Val loss: 41.086, Val Rec: -0.506\n",
      "\n",
      "Epoch 12\n",
      "--------------------\n",
      "Train loss: 41.142|Train Rec: -0.466 | Val loss: 41.027, Val Rec: -0.566\n",
      "\n",
      "Epoch 13\n",
      "--------------------\n",
      "Train loss: 41.138|Train Rec: -0.470 | Val loss: 41.007, Val Rec: -0.586\n",
      "\n",
      "Epoch 14\n",
      "--------------------\n",
      "Train loss: 41.123|Train Rec: -0.485 | Val loss: 41.004, Val Rec: -0.591\n",
      "\n",
      "Epoch 15\n",
      "--------------------\n",
      "Train loss: 41.121|Train Rec: -0.488 | Val loss: 40.989, Val Rec: -0.604\n",
      "\n",
      "Epoch 16\n",
      "--------------------\n",
      "Train loss: 41.116|Train Rec: -0.492 | Val loss: 40.990, Val Rec: -0.604\n",
      "\n",
      "Epoch 17\n",
      "--------------------\n",
      "Train loss: 41.107|Train Rec: -0.501 | Val loss: 40.996, Val Rec: -0.598\n",
      "\n",
      "Epoch 18\n",
      "--------------------\n",
      "Train loss: 41.101|Train Rec: -0.507 | Val loss: 40.992, Val Rec: -0.603\n",
      "\n",
      "Epoch 19\n",
      "--------------------\n",
      "Train loss: 41.099|Train Rec: -0.508 | Val loss: 40.960, Val Rec: -0.640\n",
      "\n",
      "Epoch 20\n",
      "--------------------\n",
      "Train loss: 41.096|Train Rec: -0.512 | Val loss: 40.964, Val Rec: -0.631\n",
      "\n",
      "Epoch 21\n",
      "--------------------\n",
      "Train loss: 41.091|Train Rec: -0.516 | Val loss: 40.957, Val Rec: -0.634\n",
      "\n",
      "Epoch 22\n",
      "--------------------\n",
      "Train loss: 41.083|Train Rec: -0.524 | Val loss: 40.924, Val Rec: -0.669\n",
      "\n",
      "Epoch 23\n",
      "--------------------\n",
      "Train loss: 41.081|Train Rec: -0.526 | Val loss: 40.949, Val Rec: -0.645\n",
      "\n",
      "Epoch 24\n",
      "--------------------\n",
      "Train loss: 41.078|Train Rec: -0.529 | Val loss: 40.930, Val Rec: -0.664\n",
      "\n",
      "Epoch 25\n",
      "--------------------\n",
      "Train loss: 41.077|Train Rec: -0.529 | Val loss: 40.945, Val Rec: -0.648\n",
      "\n",
      "Epoch 26\n",
      "--------------------\n",
      "Train loss: 41.071|Train Rec: -0.535 | Val loss: 40.958, Val Rec: -0.643\n",
      "\n",
      "Epoch 27\n",
      "--------------------\n",
      "Train loss: 41.071|Train Rec: -0.533 | Val loss: 40.917, Val Rec: -0.676\n",
      "\n",
      "Epoch 28\n",
      "--------------------\n",
      "Train loss: 41.069|Train Rec: -0.536 | Val loss: 40.932, Val Rec: -0.663\n",
      "\n",
      "Epoch 29\n",
      "--------------------\n",
      "Train loss: 41.066|Train Rec: -0.538 | Val loss: 40.912, Val Rec: -0.682\n",
      "\n",
      "Epoch 30\n",
      "--------------------\n",
      "Train loss: 41.062|Train Rec: -0.543 | Val loss: 40.932, Val Rec: -0.661\n",
      "\n",
      "Epoch 31\n",
      "--------------------\n",
      "Train loss: 41.063|Train Rec: -0.541 | Val loss: 40.931, Val Rec: -0.661\n",
      "\n",
      "Epoch 32\n",
      "--------------------\n",
      "Train loss: 41.057|Train Rec: -0.548 | Val loss: 40.911, Val Rec: -0.681\n",
      "\n",
      "Epoch 33\n",
      "--------------------\n",
      "Train loss: 41.056|Train Rec: -0.548 | Val loss: 40.907, Val Rec: -0.684\n",
      "\n",
      "Epoch 34\n",
      "--------------------\n",
      "Train loss: 41.052|Train Rec: -0.551 | Val loss: 40.917, Val Rec: -0.675\n",
      "\n",
      "Epoch 35\n",
      "--------------------\n",
      "Train loss: 41.054|Train Rec: -0.551 | Val loss: 40.912, Val Rec: -0.680\n",
      "\n",
      "Epoch 36\n",
      "--------------------\n",
      "Train loss: 41.051|Train Rec: -0.552 | Val loss: 40.898, Val Rec: -0.694\n",
      "\n",
      "Epoch 37\n",
      "--------------------\n",
      "Train loss: 41.048|Train Rec: -0.555 | Val loss: 40.909, Val Rec: -0.683\n",
      "\n",
      "Epoch 38\n",
      "--------------------\n",
      "Train loss: 41.048|Train Rec: -0.555 | Val loss: 40.903, Val Rec: -0.687\n",
      "\n",
      "Epoch 39\n",
      "--------------------\n",
      "Train loss: 41.046|Train Rec: -0.556 | Val loss: 40.895, Val Rec: -0.696\n",
      "\n",
      "Epoch 40\n",
      "--------------------\n",
      "Train loss: 41.041|Train Rec: -0.562 | Val loss: 40.885, Val Rec: -0.705\n",
      "\n",
      "Epoch 41\n",
      "--------------------\n",
      "Train loss: 41.038|Train Rec: -0.564 | Val loss: 40.897, Val Rec: -0.693\n",
      "\n",
      "Epoch 42\n",
      "--------------------\n",
      "Train loss: 41.039|Train Rec: -0.564 | Val loss: 40.901, Val Rec: -0.689\n",
      "\n",
      "Epoch 43\n",
      "--------------------\n",
      "Train loss: 41.039|Train Rec: -0.563 | Val loss: 40.895, Val Rec: -0.695\n",
      "\n",
      "Epoch 44\n",
      "--------------------\n",
      "Train loss: 41.040|Train Rec: -0.563 | Val loss: 40.897, Val Rec: -0.693\n",
      "\n",
      "Epoch 45\n",
      "--------------------\n",
      "Train loss: 41.035|Train Rec: -0.568 | Val loss: 40.885, Val Rec: -0.707\n",
      "\n",
      "Epoch 46\n",
      "--------------------\n",
      "Train loss: 41.030|Train Rec: -0.572 | Val loss: 40.873, Val Rec: -0.718\n",
      "\n",
      "Epoch 47\n",
      "--------------------\n",
      "Train loss: 41.030|Train Rec: -0.572 | Val loss: 40.880, Val Rec: -0.710\n",
      "\n",
      "Epoch 48\n",
      "--------------------\n",
      "Train loss: 41.034|Train Rec: -0.567 | Val loss: 40.886, Val Rec: -0.704\n",
      "\n",
      "Epoch 49\n",
      "--------------------\n",
      "Train loss: 41.030|Train Rec: -0.572 | Val loss: 40.875, Val Rec: -0.715\n",
      "\n",
      "Epoch 50\n",
      "--------------------\n",
      "Train loss: 41.026|Train Rec: -0.576 | Val loss: 40.872, Val Rec: -0.720\n",
      "\n",
      "Epoch 51\n",
      "--------------------\n",
      "Train loss: 41.024|Train Rec: -0.577 | Val loss: 40.875, Val Rec: -0.714\n",
      "\n",
      "Epoch 52\n",
      "--------------------\n",
      "Train loss: 41.025|Train Rec: -0.576 | Val loss: 40.874, Val Rec: -0.715\n",
      "\n",
      "Epoch 53\n",
      "--------------------\n",
      "Train loss: 41.022|Train Rec: -0.579 | Val loss: 40.868, Val Rec: -0.722\n",
      "\n",
      "Epoch 54\n",
      "--------------------\n",
      "Train loss: 41.019|Train Rec: -0.582 | Val loss: 40.855, Val Rec: -0.735\n",
      "\n",
      "Epoch 55\n",
      "--------------------\n",
      "Train loss: 41.023|Train Rec: -0.579 | Val loss: 40.865, Val Rec: -0.726\n",
      "\n",
      "Epoch 56\n",
      "--------------------\n",
      "Train loss: 41.023|Train Rec: -0.578 | Val loss: 40.866, Val Rec: -0.723\n",
      "\n",
      "Epoch 57\n",
      "--------------------\n",
      "Train loss: 41.019|Train Rec: -0.582 | Val loss: 40.864, Val Rec: -0.725\n",
      "\n",
      "Epoch 58\n",
      "--------------------\n",
      "Train loss: 41.015|Train Rec: -0.586 | Val loss: 40.851, Val Rec: -0.739\n",
      "\n",
      "Epoch 59\n",
      "--------------------\n",
      "Train loss: 41.020|Train Rec: -0.581 | Val loss: 40.856, Val Rec: -0.734\n",
      "\n",
      "Epoch 60\n",
      "--------------------\n",
      "Train loss: 41.017|Train Rec: -0.584 | Val loss: 40.864, Val Rec: -0.725\n",
      "\n",
      "Epoch 61\n",
      "--------------------\n",
      "Train loss: 41.018|Train Rec: -0.583 | Val loss: 40.863, Val Rec: -0.727\n",
      "\n",
      "Epoch 62\n",
      "--------------------\n",
      "Train loss: 41.018|Train Rec: -0.583 | Val loss: 40.869, Val Rec: -0.720\n",
      "\n",
      "Epoch 63\n",
      "--------------------\n",
      "Train loss: 41.019|Train Rec: -0.582 | Val loss: 40.853, Val Rec: -0.736\n",
      "\n",
      "Epoch 64\n",
      "--------------------\n",
      "Train loss: 41.017|Train Rec: -0.584 | Val loss: 40.862, Val Rec: -0.728\n",
      "\n",
      "Epoch 65\n",
      "--------------------\n",
      "Train loss: 41.011|Train Rec: -0.588 | Val loss: 40.858, Val Rec: -0.731\n",
      "\n",
      "Epoch 66\n",
      "--------------------\n",
      "Train loss: 41.012|Train Rec: -0.589 | Val loss: 40.859, Val Rec: -0.731\n",
      "\n",
      "Patience exceeded at 66 with last checkpoint saved at 58\n",
      "changed learning rate to 0.001\n",
      "Epoch 67\n",
      "--------------------\n",
      "Train loss: 40.997|Train Rec: -0.602 | Val loss: 40.827, Val Rec: -0.762\n",
      "\n",
      "Epoch 68\n",
      "--------------------\n",
      "Train loss: 40.991|Train Rec: -0.608 | Val loss: 40.827, Val Rec: -0.762\n",
      "\n",
      "Epoch 69\n",
      "--------------------\n",
      "Train loss: 40.995|Train Rec: -0.605 | Val loss: 40.826, Val Rec: -0.763\n",
      "\n",
      "Epoch 70\n",
      "--------------------\n",
      "Train loss: 40.988|Train Rec: -0.610 | Val loss: 40.824, Val Rec: -0.765\n",
      "\n",
      "Epoch 71\n",
      "--------------------\n",
      "Train loss: 40.993|Train Rec: -0.605 | Val loss: 40.819, Val Rec: -0.770\n",
      "\n",
      "Epoch 72\n",
      "--------------------\n",
      "Train loss: 40.989|Train Rec: -0.610 | Val loss: 40.825, Val Rec: -0.764\n",
      "\n",
      "Epoch 73\n",
      "--------------------\n",
      "Train loss: 40.989|Train Rec: -0.610 | Val loss: 40.821, Val Rec: -0.768\n",
      "\n",
      "Epoch 74\n",
      "--------------------\n",
      "Train loss: 40.987|Train Rec: -0.612 | Val loss: 40.821, Val Rec: -0.769\n",
      "\n",
      "Epoch 75\n",
      "--------------------\n",
      "Train loss: 40.987|Train Rec: -0.612 | Val loss: 40.816, Val Rec: -0.773\n",
      "\n",
      "Epoch 76\n",
      "--------------------\n",
      "Train loss: 40.989|Train Rec: -0.610 | Val loss: 40.824, Val Rec: -0.765\n",
      "\n",
      "Epoch 77\n",
      "--------------------\n",
      "Train loss: 40.986|Train Rec: -0.613 | Val loss: 40.823, Val Rec: -0.766\n",
      "\n",
      "Epoch 78\n",
      "--------------------\n",
      "Train loss: 40.987|Train Rec: -0.612 | Val loss: 40.819, Val Rec: -0.770\n",
      "\n",
      "Epoch 79\n",
      "--------------------\n",
      "Train loss: 40.989|Train Rec: -0.610 | Val loss: 40.818, Val Rec: -0.771\n",
      "\n",
      "Epoch 80\n",
      "--------------------\n",
      "Train loss: 40.987|Train Rec: -0.612 | Val loss: 40.819, Val Rec: -0.770\n",
      "\n",
      "Epoch 81\n",
      "--------------------\n",
      "Train loss: 40.987|Train Rec: -0.612 | Val loss: 40.823, Val Rec: -0.766\n",
      "\n",
      "Epoch 82\n",
      "--------------------\n",
      "Train loss: 40.987|Train Rec: -0.611 | Val loss: 40.825, Val Rec: -0.764\n",
      "\n",
      "Epoch 83\n",
      "--------------------\n",
      "Train loss: 40.987|Train Rec: -0.612 | Val loss: 40.820, Val Rec: -0.769\n",
      "\n",
      "Epoch 84\n",
      "--------------------\n",
      "Train loss: 40.989|Train Rec: -0.611 | Val loss: 40.822, Val Rec: -0.767\n",
      "\n",
      "Epoch 85\n",
      "--------------------\n",
      "Train loss: 40.989|Train Rec: -0.609 | Val loss: 40.821, Val Rec: -0.769\n",
      "\n",
      "Epoch 86\n",
      "--------------------\n",
      "Train loss: 40.986|Train Rec: -0.613 | Val loss: 40.820, Val Rec: -0.769\n",
      "\n",
      "Early stopping at epoch 86 with last checkpoint saved at 75\n",
      "Model saved at: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\modelscbc_5_54_30\n",
      "Model: modelscbc_5_54_30_ep75_norm0_bits2.0_bs128_lr0.001 has been trained\n",
      "Using this model modelscbc_5_54_30_ep75_norm0_bits2.0_bs128_lr0.001\n",
      "Directory already exists: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\n",
      "model path c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\modelscbc_6_54_30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d322097ba104285a89acdce01cdfd44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing pre-training evaluation on the model in epoch 0\n",
      "\n",
      "Val loss: 50.875| Val KL: 49.906609005398224 | Val Rec: 0.968\n",
      "\n",
      "Epoch 1\n",
      "--------------------\n",
      "Train loss: 50.092|Train Rec: 0.176 | Val loss: 49.813, Val Rec: -0.095\n",
      "\n",
      "Epoch 2\n",
      "--------------------\n",
      "Train loss: 49.752|Train Rec: -0.171 | Val loss: 49.650, Val Rec: -0.264\n",
      "\n",
      "Epoch 3\n",
      "--------------------\n",
      "Train loss: 49.640|Train Rec: -0.286 | Val loss: 49.516, Val Rec: -0.392\n",
      "\n",
      "Epoch 4\n",
      "--------------------\n",
      "Train loss: 49.573|Train Rec: -0.354 | Val loss: 49.478, Val Rec: -0.430\n",
      "\n",
      "Epoch 5\n",
      "--------------------\n",
      "Train loss: 49.522|Train Rec: -0.404 | Val loss: 49.420, Val Rec: -0.488\n",
      "\n",
      "Epoch 6\n",
      "--------------------\n",
      "Train loss: 49.482|Train Rec: -0.443 | Val loss: 49.416, Val Rec: -0.491\n",
      "\n",
      "Epoch 7\n",
      "--------------------\n",
      "Train loss: 49.461|Train Rec: -0.464 | Val loss: 49.355, Val Rec: -0.552\n",
      "\n",
      "Epoch 8\n",
      "--------------------\n",
      "Train loss: 49.435|Train Rec: -0.489 | Val loss: 49.316, Val Rec: -0.592\n",
      "\n",
      "Epoch 9\n",
      "--------------------\n",
      "Train loss: 49.414|Train Rec: -0.509 | Val loss: 49.279, Val Rec: -0.632\n",
      "\n",
      "Epoch 10\n",
      "--------------------\n",
      "Train loss: 49.406|Train Rec: -0.519 | Val loss: 49.286, Val Rec: -0.622\n",
      "\n",
      "Epoch 11\n",
      "--------------------\n",
      "Train loss: 49.395|Train Rec: -0.529 | Val loss: 49.256, Val Rec: -0.652\n",
      "\n",
      "Epoch 12\n",
      "--------------------\n",
      "Train loss: 49.386|Train Rec: -0.538 | Val loss: 49.257, Val Rec: -0.658\n",
      "\n",
      "Epoch 13\n",
      "--------------------\n",
      "Train loss: 49.380|Train Rec: -0.544 | Val loss: 49.234, Val Rec: -0.677\n",
      "\n",
      "Epoch 14\n",
      "--------------------\n",
      "Train loss: 49.372|Train Rec: -0.551 | Val loss: 49.223, Val Rec: -0.687\n",
      "\n",
      "Epoch 15\n",
      "--------------------\n",
      "Train loss: 49.365|Train Rec: -0.558 | Val loss: 49.234, Val Rec: -0.677\n",
      "\n",
      "Epoch 16\n",
      "--------------------\n",
      "Train loss: 49.360|Train Rec: -0.563 | Val loss: 49.204, Val Rec: -0.710\n",
      "\n",
      "Epoch 17\n",
      "--------------------\n",
      "Train loss: 49.356|Train Rec: -0.569 | Val loss: 49.224, Val Rec: -0.689\n",
      "\n",
      "Epoch 18\n",
      "--------------------\n",
      "Train loss: 49.351|Train Rec: -0.573 | Val loss: 49.230, Val Rec: -0.684\n",
      "\n",
      "Epoch 19\n",
      "--------------------\n",
      "Train loss: 49.348|Train Rec: -0.576 | Val loss: 49.184, Val Rec: -0.727\n",
      "\n",
      "Epoch 20\n",
      "--------------------\n",
      "Train loss: 49.347|Train Rec: -0.576 | Val loss: 49.194, Val Rec: -0.718\n",
      "\n",
      "Epoch 21\n",
      "--------------------\n",
      "Train loss: 49.343|Train Rec: -0.580 | Val loss: 49.186, Val Rec: -0.726\n",
      "\n",
      "Epoch 22\n",
      "--------------------\n",
      "Train loss: 49.340|Train Rec: -0.583 | Val loss: 49.174, Val Rec: -0.736\n",
      "\n",
      "Epoch 23\n",
      "--------------------\n",
      "Train loss: 49.337|Train Rec: -0.586 | Val loss: 49.221, Val Rec: -0.690\n",
      "\n",
      "Epoch 24\n",
      "--------------------\n",
      "Train loss: 49.333|Train Rec: -0.590 | Val loss: 49.172, Val Rec: -0.742\n",
      "\n",
      "Epoch 25\n",
      "--------------------\n",
      "Train loss: 49.332|Train Rec: -0.590 | Val loss: 49.166, Val Rec: -0.746\n",
      "\n",
      "Epoch 26\n",
      "--------------------\n",
      "Train loss: 49.331|Train Rec: -0.592 | Val loss: 49.168, Val Rec: -0.744\n",
      "\n",
      "Epoch 27\n",
      "--------------------\n",
      "Train loss: 49.329|Train Rec: -0.593 | Val loss: 49.169, Val Rec: -0.741\n",
      "\n",
      "Epoch 28\n",
      "--------------------\n",
      "Train loss: 49.326|Train Rec: -0.596 | Val loss: 49.187, Val Rec: -0.727\n",
      "\n",
      "Epoch 29\n",
      "--------------------\n",
      "Train loss: 49.322|Train Rec: -0.600 | Val loss: 49.151, Val Rec: -0.760\n",
      "\n",
      "Epoch 30\n",
      "--------------------\n",
      "Train loss: 49.322|Train Rec: -0.600 | Val loss: 49.156, Val Rec: -0.754\n",
      "\n",
      "Epoch 31\n",
      "--------------------\n",
      "Train loss: 49.320|Train Rec: -0.602 | Val loss: 49.157, Val Rec: -0.754\n",
      "\n",
      "Epoch 32\n",
      "--------------------\n",
      "Train loss: 49.320|Train Rec: -0.602 | Val loss: 49.179, Val Rec: -0.730\n",
      "\n",
      "Epoch 33\n",
      "--------------------\n",
      "Train loss: 49.318|Train Rec: -0.604 | Val loss: 49.154, Val Rec: -0.756\n",
      "\n",
      "Epoch 34\n",
      "--------------------\n",
      "Train loss: 49.316|Train Rec: -0.606 | Val loss: 49.174, Val Rec: -0.734\n",
      "\n",
      "Epoch 35\n",
      "--------------------\n",
      "Train loss: 49.314|Train Rec: -0.607 | Val loss: 49.136, Val Rec: -0.772\n",
      "\n",
      "Epoch 36\n",
      "--------------------\n",
      "Train loss: 49.308|Train Rec: -0.613 | Val loss: 49.148, Val Rec: -0.760\n",
      "\n",
      "Epoch 37\n",
      "--------------------\n",
      "Train loss: 49.311|Train Rec: -0.609 | Val loss: 49.151, Val Rec: -0.759\n",
      "\n",
      "Epoch 38\n",
      "--------------------\n",
      "Train loss: 49.310|Train Rec: -0.610 | Val loss: 49.154, Val Rec: -0.755\n",
      "\n",
      "Epoch 39\n",
      "--------------------\n",
      "Train loss: 49.307|Train Rec: -0.613 | Val loss: 49.126, Val Rec: -0.786\n",
      "\n",
      "Epoch 40\n",
      "--------------------\n",
      "Train loss: 49.304|Train Rec: -0.617 | Val loss: 49.122, Val Rec: -0.787\n",
      "\n",
      "Epoch 41\n",
      "--------------------\n",
      "Train loss: 49.306|Train Rec: -0.615 | Val loss: 49.129, Val Rec: -0.781\n",
      "\n",
      "Epoch 42\n",
      "--------------------\n",
      "Train loss: 49.304|Train Rec: -0.616 | Val loss: 49.127, Val Rec: -0.783\n",
      "\n",
      "Epoch 43\n",
      "--------------------\n",
      "Train loss: 49.305|Train Rec: -0.616 | Val loss: 49.129, Val Rec: -0.780\n",
      "\n",
      "Epoch 44\n",
      "--------------------\n",
      "Train loss: 49.301|Train Rec: -0.620 | Val loss: 49.134, Val Rec: -0.775\n",
      "\n",
      "Epoch 45\n",
      "--------------------\n",
      "Train loss: 49.304|Train Rec: -0.617 | Val loss: 49.135, Val Rec: -0.773\n",
      "\n",
      "Epoch 46\n",
      "--------------------\n",
      "Train loss: 49.304|Train Rec: -0.616 | Val loss: 49.151, Val Rec: -0.758\n",
      "\n",
      "Epoch 47\n",
      "--------------------\n",
      "Train loss: 49.306|Train Rec: -0.614 | Val loss: 49.142, Val Rec: -0.768\n",
      "\n",
      "Epoch 48\n",
      "--------------------\n",
      "Train loss: 49.297|Train Rec: -0.623 | Val loss: 49.130, Val Rec: -0.778\n",
      "\n",
      "Epoch 49\n",
      "--------------------\n",
      "Train loss: 49.295|Train Rec: -0.625 | Val loss: 49.158, Val Rec: -0.751\n",
      "\n",
      "Epoch 50\n",
      "--------------------\n",
      "Train loss: 49.298|Train Rec: -0.622 | Val loss: 49.109, Val Rec: -0.799\n",
      "\n",
      "Epoch 51\n",
      "--------------------\n",
      "Train loss: 49.291|Train Rec: -0.628 | Val loss: 49.127, Val Rec: -0.781\n",
      "\n",
      "Epoch 52\n",
      "--------------------\n",
      "Train loss: 49.296|Train Rec: -0.623 | Val loss: 49.115, Val Rec: -0.793\n",
      "\n",
      "Epoch 53\n",
      "--------------------\n",
      "Train loss: 49.293|Train Rec: -0.627 | Val loss: 49.135, Val Rec: -0.774\n",
      "\n",
      "Epoch 54\n",
      "--------------------\n",
      "Train loss: 49.291|Train Rec: -0.628 | Val loss: 49.119, Val Rec: -0.791\n",
      "\n",
      "Epoch 55\n",
      "--------------------\n",
      "Train loss: 49.292|Train Rec: -0.628 | Val loss: 49.120, Val Rec: -0.788\n",
      "\n",
      "Epoch 56\n",
      "--------------------\n",
      "Train loss: 49.293|Train Rec: -0.626 | Val loss: 49.119, Val Rec: -0.790\n",
      "\n",
      "Epoch 57\n",
      "--------------------\n",
      "Train loss: 49.291|Train Rec: -0.628 | Val loss: 49.136, Val Rec: -0.772\n",
      "\n",
      "Epoch 58\n",
      "--------------------\n",
      "Train loss: 49.289|Train Rec: -0.631 | Val loss: 49.106, Val Rec: -0.803\n",
      "\n",
      "Epoch 59\n",
      "--------------------\n",
      "Train loss: 49.290|Train Rec: -0.629 | Val loss: 49.104, Val Rec: -0.804\n",
      "\n",
      "Epoch 60\n",
      "--------------------\n",
      "Train loss: 49.294|Train Rec: -0.625 | Val loss: 49.110, Val Rec: -0.799\n",
      "\n",
      "Epoch 61\n",
      "--------------------\n",
      "Train loss: 49.291|Train Rec: -0.629 | Val loss: 49.132, Val Rec: -0.776\n",
      "\n",
      "Epoch 62\n",
      "--------------------\n",
      "Train loss: 49.285|Train Rec: -0.634 | Val loss: 49.107, Val Rec: -0.801\n",
      "\n",
      "Epoch 63\n",
      "--------------------\n",
      "Train loss: 49.289|Train Rec: -0.631 | Val loss: 49.128, Val Rec: -0.780\n",
      "\n",
      "Epoch 64\n",
      "--------------------\n",
      "Train loss: 49.285|Train Rec: -0.634 | Val loss: 49.115, Val Rec: -0.793\n",
      "\n",
      "Epoch 65\n",
      "--------------------\n",
      "Train loss: 49.288|Train Rec: -0.631 | Val loss: 49.117, Val Rec: -0.791\n",
      "\n",
      "Epoch 66\n",
      "--------------------\n",
      "Train loss: 49.284|Train Rec: -0.635 | Val loss: 49.115, Val Rec: -0.794\n",
      "\n",
      "Epoch 67\n",
      "--------------------\n",
      "Train loss: 49.287|Train Rec: -0.632 | Val loss: 49.110, Val Rec: -0.798\n",
      "\n",
      "Patience exceeded at 67 with last checkpoint saved at 59\n",
      "changed learning rate to 0.001\n",
      "Epoch 68\n",
      "--------------------\n",
      "Train loss: 49.266|Train Rec: -0.652 | Val loss: 49.085, Val Rec: -0.822\n",
      "\n",
      "Epoch 69\n",
      "--------------------\n",
      "Train loss: 49.264|Train Rec: -0.654 | Val loss: 49.080, Val Rec: -0.827\n",
      "\n",
      "Epoch 70\n",
      "--------------------\n",
      "Train loss: 49.262|Train Rec: -0.657 | Val loss: 49.078, Val Rec: -0.829\n",
      "\n",
      "Epoch 71\n",
      "--------------------\n",
      "Train loss: 49.260|Train Rec: -0.657 | Val loss: 49.076, Val Rec: -0.831\n",
      "\n",
      "Epoch 72\n",
      "--------------------\n",
      "Train loss: 49.259|Train Rec: -0.658 | Val loss: 49.077, Val Rec: -0.830\n",
      "\n",
      "Epoch 73\n",
      "--------------------\n",
      "Train loss: 49.261|Train Rec: -0.656 | Val loss: 49.079, Val Rec: -0.829\n",
      "\n",
      "Epoch 74\n",
      "--------------------\n",
      "Train loss: 49.259|Train Rec: -0.658 | Val loss: 49.074, Val Rec: -0.833\n",
      "\n",
      "Epoch 75\n",
      "--------------------\n",
      "Train loss: 49.258|Train Rec: -0.660 | Val loss: 49.074, Val Rec: -0.833\n",
      "\n",
      "Epoch 76\n",
      "--------------------\n",
      "Train loss: 49.261|Train Rec: -0.656 | Val loss: 49.078, Val Rec: -0.829\n",
      "\n",
      "Epoch 77\n",
      "--------------------\n",
      "Train loss: 49.262|Train Rec: -0.656 | Val loss: 49.083, Val Rec: -0.825\n",
      "\n",
      "Epoch 78\n",
      "--------------------\n",
      "Train loss: 49.264|Train Rec: -0.653 | Val loss: 49.076, Val Rec: -0.832\n",
      "\n",
      "Epoch 79\n",
      "--------------------\n",
      "Train loss: 49.260|Train Rec: -0.657 | Val loss: 49.076, Val Rec: -0.831\n",
      "\n",
      "Epoch 80\n",
      "--------------------\n",
      "Train loss: 49.260|Train Rec: -0.658 | Val loss: 49.072, Val Rec: -0.835\n",
      "\n",
      "Epoch 81\n",
      "--------------------\n",
      "Train loss: 49.260|Train Rec: -0.658 | Val loss: 49.081, Val Rec: -0.826\n",
      "\n",
      "Epoch 82\n",
      "--------------------\n",
      "Train loss: 49.256|Train Rec: -0.662 | Val loss: 49.073, Val Rec: -0.834\n",
      "\n",
      "Epoch 83\n",
      "--------------------\n",
      "Train loss: 49.258|Train Rec: -0.659 | Val loss: 49.075, Val Rec: -0.833\n",
      "\n",
      "Epoch 84\n",
      "--------------------\n",
      "Train loss: 49.262|Train Rec: -0.656 | Val loss: 49.078, Val Rec: -0.829\n",
      "\n",
      "Epoch 85\n",
      "--------------------\n",
      "Train loss: 49.260|Train Rec: -0.657 | Val loss: 49.073, Val Rec: -0.835\n",
      "\n",
      "Epoch 86\n",
      "--------------------\n",
      "Train loss: 49.260|Train Rec: -0.657 | Val loss: 49.082, Val Rec: -0.826\n",
      "\n",
      "Epoch 87\n",
      "--------------------\n",
      "Train loss: 49.261|Train Rec: -0.656 | Val loss: 49.076, Val Rec: -0.831\n",
      "\n",
      "Epoch 88\n",
      "--------------------\n",
      "Train loss: 49.258|Train Rec: -0.660 | Val loss: 49.076, Val Rec: -0.831\n",
      "\n",
      "Epoch 89\n",
      "--------------------\n",
      "Train loss: 49.259|Train Rec: -0.658 | Val loss: 49.072, Val Rec: -0.835\n",
      "\n",
      "Epoch 90\n",
      "--------------------\n",
      "Train loss: 49.262|Train Rec: -0.656 | Val loss: 49.074, Val Rec: -0.833\n",
      "\n",
      "Epoch 91\n",
      "--------------------\n",
      "Train loss: 49.258|Train Rec: -0.660 | Val loss: 49.075, Val Rec: -0.833\n",
      "\n",
      "Early stopping at epoch 91 with last checkpoint saved at 80\n",
      "Model saved at: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\modelscbc_6_54_30\n",
      "Model: modelscbc_6_54_30_ep80_norm0_bits2.4_bs128_lr0.001 has been trained\n",
      "Using this model modelscbc_6_54_30_ep80_norm0_bits2.4_bs128_lr0.001\n",
      "Directory already exists: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\n",
      "model path c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\modelscbc_7_54_30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a151e3939f1f4d63bba82fd55d9ec7cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing pre-training evaluation on the model in epoch 0\n",
      "\n",
      "Val loss: 59.197| Val KL: 58.224369049072266 | Val Rec: 0.973\n",
      "\n",
      "Epoch 1\n",
      "--------------------\n",
      "Train loss: 58.407|Train Rec: 0.173 | Val loss: 58.144, Val Rec: -0.082\n",
      "\n",
      "Epoch 2\n",
      "--------------------\n",
      "Train loss: 58.057|Train Rec: -0.183 | Val loss: 57.943, Val Rec: -0.287\n",
      "\n",
      "Epoch 3\n",
      "--------------------\n",
      "Train loss: 57.930|Train Rec: -0.313 | Val loss: 57.841, Val Rec: -0.384\n",
      "\n",
      "Epoch 4\n",
      "--------------------\n",
      "Train loss: 57.862|Train Rec: -0.381 | Val loss: 57.783, Val Rec: -0.443\n",
      "\n",
      "Epoch 5\n",
      "--------------------\n",
      "Train loss: 57.805|Train Rec: -0.438 | Val loss: 57.719, Val Rec: -0.505\n",
      "\n",
      "Epoch 6\n",
      "--------------------\n",
      "Train loss: 57.770|Train Rec: -0.471 | Val loss: 57.637, Val Rec: -0.590\n",
      "\n",
      "Epoch 7\n",
      "--------------------\n",
      "Train loss: 57.738|Train Rec: -0.502 | Val loss: 57.623, Val Rec: -0.602\n",
      "\n",
      "Epoch 8\n",
      "--------------------\n",
      "Train loss: 57.710|Train Rec: -0.530 | Val loss: 57.608, Val Rec: -0.618\n",
      "\n",
      "Epoch 9\n",
      "--------------------\n",
      "Train loss: 57.685|Train Rec: -0.554 | Val loss: 57.542, Val Rec: -0.683\n",
      "\n",
      "Epoch 10\n",
      "--------------------\n",
      "Train loss: 57.677|Train Rec: -0.561 | Val loss: 57.555, Val Rec: -0.671\n",
      "\n",
      "Epoch 11\n",
      "--------------------\n",
      "Train loss: 57.665|Train Rec: -0.574 | Val loss: 57.495, Val Rec: -0.731\n",
      "\n",
      "Epoch 12\n",
      "--------------------\n",
      "Train loss: 57.654|Train Rec: -0.583 | Val loss: 57.482, Val Rec: -0.747\n",
      "\n",
      "Epoch 13\n",
      "--------------------\n",
      "Train loss: 57.645|Train Rec: -0.593 | Val loss: 57.486, Val Rec: -0.741\n",
      "\n",
      "Epoch 14\n",
      "--------------------\n",
      "Train loss: 57.640|Train Rec: -0.597 | Val loss: 57.517, Val Rec: -0.710\n",
      "\n",
      "Epoch 15\n",
      "--------------------\n",
      "Train loss: 57.630|Train Rec: -0.608 | Val loss: 57.495, Val Rec: -0.735\n",
      "\n",
      "Epoch 16\n",
      "--------------------\n",
      "Train loss: 57.631|Train Rec: -0.606 | Val loss: 57.460, Val Rec: -0.771\n",
      "\n",
      "Epoch 17\n",
      "--------------------\n",
      "Train loss: 57.621|Train Rec: -0.616 | Val loss: 57.452, Val Rec: -0.777\n",
      "\n",
      "Epoch 18\n",
      "--------------------\n",
      "Train loss: 57.611|Train Rec: -0.627 | Val loss: 57.462, Val Rec: -0.769\n",
      "\n",
      "Epoch 19\n",
      "--------------------\n",
      "Train loss: 57.606|Train Rec: -0.631 | Val loss: 57.461, Val Rec: -0.774\n",
      "\n",
      "Epoch 20\n",
      "--------------------\n",
      "Train loss: 57.608|Train Rec: -0.630 | Val loss: 57.426, Val Rec: -0.806\n",
      "\n",
      "Epoch 21\n",
      "--------------------\n",
      "Train loss: 57.606|Train Rec: -0.632 | Val loss: 57.415, Val Rec: -0.817\n",
      "\n",
      "Epoch 22\n",
      "--------------------\n",
      "Train loss: 57.599|Train Rec: -0.640 | Val loss: 57.436, Val Rec: -0.796\n",
      "\n",
      "Epoch 23\n",
      "--------------------\n",
      "Train loss: 57.597|Train Rec: -0.641 | Val loss: 57.403, Val Rec: -0.826\n",
      "\n",
      "Epoch 24\n",
      "--------------------\n",
      "Train loss: 57.595|Train Rec: -0.643 | Val loss: 57.409, Val Rec: -0.820\n",
      "\n",
      "Epoch 25\n",
      "--------------------\n",
      "Train loss: 57.593|Train Rec: -0.645 | Val loss: 57.430, Val Rec: -0.799\n",
      "\n",
      "Epoch 26\n",
      "--------------------\n",
      "Train loss: 57.588|Train Rec: -0.650 | Val loss: 57.427, Val Rec: -0.806\n",
      "\n",
      "Epoch 27\n",
      "--------------------\n",
      "Train loss: 57.587|Train Rec: -0.650 | Val loss: 57.403, Val Rec: -0.824\n",
      "\n",
      "Epoch 28\n",
      "--------------------\n",
      "Train loss: 57.587|Train Rec: -0.650 | Val loss: 57.431, Val Rec: -0.798\n",
      "\n",
      "Epoch 29\n",
      "--------------------\n",
      "Train loss: 57.584|Train Rec: -0.653 | Val loss: 57.387, Val Rec: -0.845\n",
      "\n",
      "Epoch 30\n",
      "--------------------\n",
      "Train loss: 57.583|Train Rec: -0.654 | Val loss: 57.438, Val Rec: -0.799\n",
      "\n",
      "Epoch 31\n",
      "--------------------\n",
      "Train loss: 57.586|Train Rec: -0.651 | Val loss: 57.404, Val Rec: -0.823\n",
      "\n",
      "Epoch 32\n",
      "--------------------\n",
      "Train loss: 57.580|Train Rec: -0.658 | Val loss: 57.399, Val Rec: -0.830\n",
      "\n",
      "Epoch 33\n",
      "--------------------\n",
      "Train loss: 57.577|Train Rec: -0.659 | Val loss: 57.381, Val Rec: -0.848\n",
      "\n",
      "Epoch 34\n",
      "--------------------\n",
      "Train loss: 57.578|Train Rec: -0.658 | Val loss: 57.390, Val Rec: -0.839\n",
      "\n",
      "Epoch 35\n",
      "--------------------\n",
      "Train loss: 57.577|Train Rec: -0.660 | Val loss: 57.396, Val Rec: -0.833\n",
      "\n",
      "Epoch 36\n",
      "--------------------\n",
      "Train loss: 57.574|Train Rec: -0.662 | Val loss: 57.396, Val Rec: -0.832\n",
      "\n",
      "Epoch 37\n",
      "--------------------\n",
      "Train loss: 57.573|Train Rec: -0.663 | Val loss: 57.387, Val Rec: -0.842\n",
      "\n",
      "Epoch 38\n",
      "--------------------\n",
      "Train loss: 57.573|Train Rec: -0.663 | Val loss: 57.393, Val Rec: -0.834\n",
      "\n",
      "Epoch 39\n",
      "--------------------\n",
      "Train loss: 57.577|Train Rec: -0.660 | Val loss: 57.386, Val Rec: -0.843\n",
      "\n",
      "Epoch 40\n",
      "--------------------\n",
      "Train loss: 57.574|Train Rec: -0.662 | Val loss: 57.380, Val Rec: -0.849\n",
      "\n",
      "Epoch 41\n",
      "--------------------\n",
      "Train loss: 57.571|Train Rec: -0.665 | Val loss: 57.384, Val Rec: -0.846\n",
      "\n",
      "Epoch 42\n",
      "--------------------\n",
      "Train loss: 57.571|Train Rec: -0.665 | Val loss: 57.373, Val Rec: -0.856\n",
      "\n",
      "Epoch 43\n",
      "--------------------\n",
      "Train loss: 57.568|Train Rec: -0.668 | Val loss: 57.376, Val Rec: -0.854\n",
      "\n",
      "Epoch 44\n",
      "--------------------\n",
      "Train loss: 57.564|Train Rec: -0.672 | Val loss: 57.389, Val Rec: -0.840\n",
      "\n",
      "Epoch 45\n",
      "--------------------\n",
      "Train loss: 57.565|Train Rec: -0.671 | Val loss: 57.355, Val Rec: -0.872\n",
      "\n",
      "Epoch 46\n",
      "--------------------\n",
      "Train loss: 57.565|Train Rec: -0.671 | Val loss: 57.393, Val Rec: -0.834\n",
      "\n",
      "Epoch 47\n",
      "--------------------\n",
      "Train loss: 57.560|Train Rec: -0.675 | Val loss: 57.368, Val Rec: -0.860\n",
      "\n",
      "Epoch 48\n",
      "--------------------\n",
      "Train loss: 57.562|Train Rec: -0.673 | Val loss: 57.363, Val Rec: -0.864\n",
      "\n",
      "Epoch 49\n",
      "--------------------\n",
      "Train loss: 57.561|Train Rec: -0.675 | Val loss: 57.374, Val Rec: -0.854\n",
      "\n",
      "Epoch 50\n",
      "--------------------\n",
      "Train loss: 57.559|Train Rec: -0.677 | Val loss: 57.377, Val Rec: -0.850\n",
      "\n",
      "Epoch 51\n",
      "--------------------\n",
      "Train loss: 57.563|Train Rec: -0.672 | Val loss: 57.384, Val Rec: -0.843\n",
      "\n",
      "Epoch 52\n",
      "--------------------\n",
      "Train loss: 57.561|Train Rec: -0.675 | Val loss: 57.392, Val Rec: -0.834\n",
      "\n",
      "Epoch 53\n",
      "--------------------\n",
      "Train loss: 57.557|Train Rec: -0.678 | Val loss: 57.370, Val Rec: -0.855\n",
      "\n",
      "Patience exceeded at 53 with last checkpoint saved at 45\n",
      "changed learning rate to 0.001\n",
      "Epoch 54\n",
      "--------------------\n",
      "Train loss: 57.536|Train Rec: -0.698 | Val loss: 57.332, Val Rec: -0.894\n",
      "\n",
      "Epoch 55\n",
      "--------------------\n",
      "Train loss: 57.535|Train Rec: -0.699 | Val loss: 57.325, Val Rec: -0.901\n",
      "\n",
      "Epoch 56\n",
      "--------------------\n",
      "Train loss: 57.529|Train Rec: -0.704 | Val loss: 57.328, Val Rec: -0.899\n",
      "\n",
      "Epoch 57\n",
      "--------------------\n",
      "Train loss: 57.531|Train Rec: -0.703 | Val loss: 57.326, Val Rec: -0.901\n",
      "\n",
      "Epoch 58\n",
      "--------------------\n",
      "Train loss: 57.533|Train Rec: -0.702 | Val loss: 57.331, Val Rec: -0.895\n",
      "\n",
      "Epoch 59\n",
      "--------------------\n",
      "Train loss: 57.531|Train Rec: -0.703 | Val loss: 57.325, Val Rec: -0.902\n",
      "\n",
      "Epoch 60\n",
      "--------------------\n",
      "Train loss: 57.531|Train Rec: -0.703 | Val loss: 57.317, Val Rec: -0.910\n",
      "\n",
      "Epoch 61\n",
      "--------------------\n",
      "Train loss: 57.531|Train Rec: -0.704 | Val loss: 57.324, Val Rec: -0.902\n",
      "\n",
      "Epoch 62\n",
      "--------------------\n",
      "Train loss: 57.529|Train Rec: -0.705 | Val loss: 57.322, Val Rec: -0.904\n",
      "\n",
      "Epoch 63\n",
      "--------------------\n",
      "Train loss: 57.532|Train Rec: -0.703 | Val loss: 57.326, Val Rec: -0.900\n",
      "\n",
      "Epoch 64\n",
      "--------------------\n",
      "Train loss: 57.533|Train Rec: -0.701 | Val loss: 57.327, Val Rec: -0.899\n",
      "\n",
      "Epoch 65\n",
      "--------------------\n",
      "Train loss: 57.531|Train Rec: -0.703 | Val loss: 57.325, Val Rec: -0.901\n",
      "\n",
      "Epoch 66\n",
      "--------------------\n",
      "Train loss: 57.529|Train Rec: -0.705 | Val loss: 57.327, Val Rec: -0.899\n",
      "\n",
      "Epoch 67\n",
      "--------------------\n",
      "Train loss: 57.527|Train Rec: -0.707 | Val loss: 57.316, Val Rec: -0.910\n",
      "\n",
      "Epoch 68\n",
      "--------------------\n",
      "Train loss: 57.532|Train Rec: -0.702 | Val loss: 57.322, Val Rec: -0.903\n",
      "\n",
      "Epoch 69\n",
      "--------------------\n",
      "Train loss: 57.529|Train Rec: -0.704 | Val loss: 57.324, Val Rec: -0.903\n",
      "\n",
      "Epoch 70\n",
      "--------------------\n",
      "Train loss: 57.530|Train Rec: -0.704 | Val loss: 57.327, Val Rec: -0.901\n",
      "\n",
      "Epoch 71\n",
      "--------------------\n",
      "Train loss: 57.529|Train Rec: -0.705 | Val loss: 57.325, Val Rec: -0.901\n",
      "\n",
      "Epoch 72\n",
      "--------------------\n",
      "Train loss: 57.529|Train Rec: -0.705 | Val loss: 57.322, Val Rec: -0.905\n",
      "\n",
      "Epoch 73\n",
      "--------------------\n",
      "Train loss: 57.530|Train Rec: -0.704 | Val loss: 57.324, Val Rec: -0.902\n",
      "\n",
      "Epoch 74\n",
      "--------------------\n",
      "Train loss: 57.530|Train Rec: -0.704 | Val loss: 57.322, Val Rec: -0.905\n",
      "\n",
      "Epoch 75\n",
      "--------------------\n",
      "Train loss: 57.531|Train Rec: -0.703 | Val loss: 57.320, Val Rec: -0.906\n",
      "\n",
      "Epoch 76\n",
      "--------------------\n",
      "Train loss: 57.529|Train Rec: -0.705 | Val loss: 57.326, Val Rec: -0.900\n",
      "\n",
      "Epoch 77\n",
      "--------------------\n",
      "Train loss: 57.532|Train Rec: -0.702 | Val loss: 57.321, Val Rec: -0.905\n",
      "\n",
      "Epoch 78\n",
      "--------------------\n",
      "Train loss: 57.530|Train Rec: -0.705 | Val loss: 57.324, Val Rec: -0.902\n",
      "\n",
      "Early stopping at epoch 78 with last checkpoint saved at 67\n",
      "Model saved at: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\modelscbc_7_54_30\n",
      "Model: modelscbc_7_54_30_ep67_norm0_bits2.8_bs128_lr0.001 has been trained\n",
      "Using this model modelscbc_7_54_30_ep67_norm0_bits2.8_bs128_lr0.001\n",
      "Directory already exists: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\n",
      "model path c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\modelscbc_8_54_30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e038754b9a99472888e220412a1d3f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing pre-training evaluation on the model in epoch 0\n",
      "\n",
      "Val loss: 67.497| Val KL: 66.54212273491754 | Val Rec: 0.955\n",
      "\n",
      "Epoch 1\n",
      "--------------------\n",
      "Train loss: 66.679|Train Rec: 0.128 | Val loss: 66.409, Val Rec: -0.134\n",
      "\n",
      "Epoch 2\n",
      "--------------------\n",
      "Train loss: 66.321|Train Rec: -0.236 | Val loss: 66.163, Val Rec: -0.382\n",
      "\n",
      "Epoch 3\n",
      "--------------------\n",
      "Train loss: 66.191|Train Rec: -0.369 | Val loss: 66.110, Val Rec: -0.434\n",
      "\n",
      "Epoch 4\n",
      "--------------------\n",
      "Train loss: 66.120|Train Rec: -0.438 | Val loss: 65.999, Val Rec: -0.546\n",
      "\n",
      "Epoch 5\n",
      "--------------------\n",
      "Train loss: 66.076|Train Rec: -0.482 | Val loss: 65.957, Val Rec: -0.585\n",
      "\n",
      "Epoch 6\n",
      "--------------------\n",
      "Train loss: 66.030|Train Rec: -0.527 | Val loss: 65.878, Val Rec: -0.665\n",
      "\n",
      "Epoch 7\n",
      "--------------------\n",
      "Train loss: 65.998|Train Rec: -0.557 | Val loss: 65.825, Val Rec: -0.718\n",
      "\n",
      "Epoch 8\n",
      "--------------------\n",
      "Train loss: 65.965|Train Rec: -0.589 | Val loss: 65.805, Val Rec: -0.739\n",
      "\n",
      "Epoch 9\n",
      "--------------------\n",
      "Train loss: 65.949|Train Rec: -0.605 | Val loss: 65.771, Val Rec: -0.773\n",
      "\n",
      "Epoch 10\n",
      "--------------------\n",
      "Train loss: 65.935|Train Rec: -0.619 | Val loss: 65.786, Val Rec: -0.759\n",
      "\n",
      "Epoch 11\n",
      "--------------------\n",
      "Train loss: 65.926|Train Rec: -0.627 | Val loss: 65.724, Val Rec: -0.820\n",
      "\n",
      "Epoch 12\n",
      "--------------------\n",
      "Train loss: 65.914|Train Rec: -0.639 | Val loss: 65.735, Val Rec: -0.810\n",
      "\n",
      "Epoch 13\n",
      "--------------------\n",
      "Train loss: 65.908|Train Rec: -0.645 | Val loss: 65.760, Val Rec: -0.786\n",
      "\n",
      "Epoch 14\n",
      "--------------------\n",
      "Train loss: 65.903|Train Rec: -0.650 | Val loss: 65.743, Val Rec: -0.807\n",
      "\n",
      "Epoch 15\n",
      "--------------------\n",
      "Train loss: 65.898|Train Rec: -0.655 | Val loss: 65.705, Val Rec: -0.841\n",
      "\n",
      "Epoch 16\n",
      "--------------------\n",
      "Train loss: 65.892|Train Rec: -0.662 | Val loss: 65.712, Val Rec: -0.834\n",
      "\n",
      "Epoch 17\n",
      "--------------------\n",
      "Train loss: 65.893|Train Rec: -0.660 | Val loss: 65.704, Val Rec: -0.843\n",
      "\n",
      "Epoch 18\n",
      "--------------------\n",
      "Train loss: 65.884|Train Rec: -0.668 | Val loss: 65.683, Val Rec: -0.862\n",
      "\n",
      "Epoch 19\n",
      "--------------------\n",
      "Train loss: 65.880|Train Rec: -0.673 | Val loss: 65.710, Val Rec: -0.837\n",
      "\n",
      "Epoch 20\n",
      "--------------------\n",
      "Train loss: 65.881|Train Rec: -0.671 | Val loss: 65.714, Val Rec: -0.831\n",
      "\n",
      "Epoch 21\n",
      "--------------------\n",
      "Train loss: 65.874|Train Rec: -0.678 | Val loss: 65.705, Val Rec: -0.842\n",
      "\n",
      "Epoch 22\n",
      "--------------------\n",
      "Train loss: 65.874|Train Rec: -0.678 | Val loss: 65.661, Val Rec: -0.884\n",
      "\n",
      "Epoch 23\n",
      "--------------------\n",
      "Train loss: 65.873|Train Rec: -0.680 | Val loss: 65.704, Val Rec: -0.842\n",
      "\n",
      "Epoch 24\n",
      "--------------------\n",
      "Train loss: 65.866|Train Rec: -0.686 | Val loss: 65.674, Val Rec: -0.873\n",
      "\n",
      "Epoch 25\n",
      "--------------------\n",
      "Train loss: 65.861|Train Rec: -0.691 | Val loss: 65.660, Val Rec: -0.888\n",
      "\n",
      "Epoch 26\n",
      "--------------------\n",
      "Train loss: 65.869|Train Rec: -0.683 | Val loss: 65.663, Val Rec: -0.882\n",
      "\n",
      "Epoch 27\n",
      "--------------------\n",
      "Train loss: 65.863|Train Rec: -0.689 | Val loss: 65.676, Val Rec: -0.870\n",
      "\n",
      "Epoch 28\n",
      "--------------------\n",
      "Train loss: 65.855|Train Rec: -0.696 | Val loss: 65.687, Val Rec: -0.858\n",
      "\n",
      "Epoch 29\n",
      "--------------------\n",
      "Train loss: 65.857|Train Rec: -0.695 | Val loss: 65.671, Val Rec: -0.877\n",
      "\n",
      "Epoch 30\n",
      "--------------------\n",
      "Train loss: 65.857|Train Rec: -0.695 | Val loss: 65.655, Val Rec: -0.891\n",
      "\n",
      "Epoch 31\n",
      "--------------------\n",
      "Train loss: 65.855|Train Rec: -0.697 | Val loss: 65.714, Val Rec: -0.832\n",
      "\n",
      "Epoch 32\n",
      "--------------------\n",
      "Train loss: 65.856|Train Rec: -0.696 | Val loss: 65.684, Val Rec: -0.861\n",
      "\n",
      "Epoch 33\n",
      "--------------------\n",
      "Train loss: 65.856|Train Rec: -0.696 | Val loss: 65.696, Val Rec: -0.851\n",
      "\n",
      "Epoch 34\n",
      "--------------------\n",
      "Train loss: 65.850|Train Rec: -0.702 | Val loss: 65.656, Val Rec: -0.891\n",
      "\n",
      "Epoch 35\n",
      "--------------------\n",
      "Train loss: 65.852|Train Rec: -0.700 | Val loss: 65.669, Val Rec: -0.876\n",
      "\n",
      "Epoch 36\n",
      "--------------------\n",
      "Train loss: 65.853|Train Rec: -0.698 | Val loss: 65.651, Val Rec: -0.893\n",
      "\n",
      "Epoch 37\n",
      "--------------------\n",
      "Train loss: 65.850|Train Rec: -0.701 | Val loss: 65.670, Val Rec: -0.875\n",
      "\n",
      "Epoch 38\n",
      "--------------------\n",
      "Train loss: 65.847|Train Rec: -0.704 | Val loss: 65.656, Val Rec: -0.890\n",
      "\n",
      "Epoch 39\n",
      "--------------------\n",
      "Train loss: 65.851|Train Rec: -0.700 | Val loss: 65.647, Val Rec: -0.897\n",
      "\n",
      "Epoch 40\n",
      "--------------------\n",
      "Train loss: 65.849|Train Rec: -0.703 | Val loss: 65.655, Val Rec: -0.890\n",
      "\n",
      "Epoch 41\n",
      "--------------------\n",
      "Train loss: 65.851|Train Rec: -0.701 | Val loss: 65.656, Val Rec: -0.889\n",
      "\n",
      "Epoch 42\n",
      "--------------------\n",
      "Train loss: 65.849|Train Rec: -0.703 | Val loss: 65.643, Val Rec: -0.902\n",
      "\n",
      "Epoch 43\n",
      "--------------------\n",
      "Train loss: 65.849|Train Rec: -0.702 | Val loss: 65.647, Val Rec: -0.897\n",
      "\n",
      "Epoch 44\n",
      "--------------------\n",
      "Train loss: 65.842|Train Rec: -0.709 | Val loss: 65.695, Val Rec: -0.851\n",
      "\n",
      "Epoch 45\n",
      "--------------------\n",
      "Train loss: 65.844|Train Rec: -0.708 | Val loss: 65.663, Val Rec: -0.884\n",
      "\n",
      "Epoch 46\n",
      "--------------------\n",
      "Train loss: 65.845|Train Rec: -0.706 | Val loss: 65.655, Val Rec: -0.890\n",
      "\n",
      "Epoch 47\n",
      "--------------------\n",
      "Train loss: 65.840|Train Rec: -0.711 | Val loss: 65.622, Val Rec: -0.923\n",
      "\n",
      "Epoch 48\n",
      "--------------------\n",
      "Train loss: 65.841|Train Rec: -0.710 | Val loss: 65.644, Val Rec: -0.900\n",
      "\n",
      "Epoch 49\n",
      "--------------------\n",
      "Train loss: 65.838|Train Rec: -0.713 | Val loss: 65.655, Val Rec: -0.889\n",
      "\n",
      "Epoch 50\n",
      "--------------------\n",
      "Train loss: 65.834|Train Rec: -0.716 | Val loss: 65.643, Val Rec: -0.900\n",
      "\n",
      "Epoch 51\n",
      "--------------------\n",
      "Train loss: 65.837|Train Rec: -0.713 | Val loss: 65.631, Val Rec: -0.914\n",
      "\n",
      "Epoch 52\n",
      "--------------------\n",
      "Train loss: 65.843|Train Rec: -0.708 | Val loss: 65.628, Val Rec: -0.917\n",
      "\n",
      "Epoch 53\n",
      "--------------------\n",
      "Train loss: 65.835|Train Rec: -0.716 | Val loss: 65.633, Val Rec: -0.911\n",
      "\n",
      "Epoch 54\n",
      "--------------------\n",
      "Train loss: 65.842|Train Rec: -0.709 | Val loss: 65.623, Val Rec: -0.921\n",
      "\n",
      "Epoch 55\n",
      "--------------------\n",
      "Train loss: 65.839|Train Rec: -0.712 | Val loss: 65.648, Val Rec: -0.896\n",
      "\n",
      "Patience exceeded at 55 with last checkpoint saved at 47\n",
      "changed learning rate to 0.001\n",
      "Epoch 56\n",
      "--------------------\n",
      "Train loss: 65.817|Train Rec: -0.732 | Val loss: 65.590, Val Rec: -0.954\n",
      "\n",
      "Epoch 57\n",
      "--------------------\n",
      "Train loss: 65.812|Train Rec: -0.738 | Val loss: 65.595, Val Rec: -0.949\n",
      "\n",
      "Epoch 58\n",
      "--------------------\n",
      "Train loss: 65.813|Train Rec: -0.737 | Val loss: 65.600, Val Rec: -0.944\n",
      "\n",
      "Epoch 59\n",
      "--------------------\n",
      "Train loss: 65.811|Train Rec: -0.738 | Val loss: 65.599, Val Rec: -0.945\n",
      "\n",
      "Epoch 60\n",
      "--------------------\n",
      "Train loss: 65.814|Train Rec: -0.736 | Val loss: 65.597, Val Rec: -0.946\n",
      "\n",
      "Epoch 61\n",
      "--------------------\n",
      "Train loss: 65.812|Train Rec: -0.738 | Val loss: 65.595, Val Rec: -0.949\n",
      "\n",
      "Epoch 62\n",
      "--------------------\n",
      "Train loss: 65.811|Train Rec: -0.739 | Val loss: 65.589, Val Rec: -0.955\n",
      "\n",
      "Epoch 63\n",
      "--------------------\n",
      "Train loss: 65.813|Train Rec: -0.736 | Val loss: 65.596, Val Rec: -0.947\n",
      "\n",
      "Epoch 64\n",
      "--------------------\n",
      "Train loss: 65.811|Train Rec: -0.739 | Val loss: 65.593, Val Rec: -0.951\n",
      "\n",
      "Epoch 65\n",
      "--------------------\n",
      "Train loss: 65.811|Train Rec: -0.738 | Val loss: 65.588, Val Rec: -0.956\n",
      "\n",
      "Epoch 66\n",
      "--------------------\n",
      "Train loss: 65.809|Train Rec: -0.740 | Val loss: 65.600, Val Rec: -0.944\n",
      "\n",
      "Epoch 67\n",
      "--------------------\n",
      "Train loss: 65.811|Train Rec: -0.739 | Val loss: 65.596, Val Rec: -0.948\n",
      "\n",
      "Epoch 68\n",
      "--------------------\n",
      "Train loss: 65.811|Train Rec: -0.739 | Val loss: 65.592, Val Rec: -0.952\n",
      "\n",
      "Epoch 69\n",
      "--------------------\n",
      "Train loss: 65.814|Train Rec: -0.736 | Val loss: 65.591, Val Rec: -0.952\n",
      "\n",
      "Epoch 70\n",
      "--------------------\n",
      "Train loss: 65.808|Train Rec: -0.741 | Val loss: 65.591, Val Rec: -0.954\n",
      "\n",
      "Epoch 71\n",
      "--------------------\n",
      "Train loss: 65.808|Train Rec: -0.742 | Val loss: 65.584, Val Rec: -0.959\n",
      "\n",
      "Epoch 72\n",
      "--------------------\n",
      "Train loss: 65.810|Train Rec: -0.739 | Val loss: 65.583, Val Rec: -0.961\n",
      "\n",
      "Epoch 73\n",
      "--------------------\n",
      "Train loss: 65.808|Train Rec: -0.742 | Val loss: 65.591, Val Rec: -0.953\n",
      "\n",
      "Epoch 74\n",
      "--------------------\n",
      "Train loss: 65.809|Train Rec: -0.741 | Val loss: 65.597, Val Rec: -0.947\n",
      "\n",
      "Epoch 75\n",
      "--------------------\n",
      "Train loss: 65.811|Train Rec: -0.739 | Val loss: 65.603, Val Rec: -0.940\n",
      "\n",
      "Epoch 76\n",
      "--------------------\n",
      "Train loss: 65.811|Train Rec: -0.739 | Val loss: 65.588, Val Rec: -0.955\n",
      "\n",
      "Epoch 77\n",
      "--------------------\n",
      "Train loss: 65.809|Train Rec: -0.740 | Val loss: 65.591, Val Rec: -0.953\n",
      "\n",
      "Epoch 78\n",
      "--------------------\n",
      "Train loss: 65.811|Train Rec: -0.738 | Val loss: 65.591, Val Rec: -0.953\n",
      "\n",
      "Epoch 79\n",
      "--------------------\n",
      "Train loss: 65.807|Train Rec: -0.743 | Val loss: 65.586, Val Rec: -0.958\n",
      "\n",
      "Epoch 80\n",
      "--------------------\n",
      "Train loss: 65.808|Train Rec: -0.741 | Val loss: 65.586, Val Rec: -0.958\n",
      "\n",
      "Epoch 81\n",
      "--------------------\n",
      "Train loss: 65.807|Train Rec: -0.743 | Val loss: 65.602, Val Rec: -0.942\n",
      "\n",
      "Epoch 82\n",
      "--------------------\n",
      "Train loss: 65.809|Train Rec: -0.740 | Val loss: 65.595, Val Rec: -0.949\n",
      "\n",
      "Epoch 83\n",
      "--------------------\n",
      "Train loss: 65.807|Train Rec: -0.742 | Val loss: 65.588, Val Rec: -0.956\n",
      "\n",
      "Early stopping at epoch 83 with last checkpoint saved at 72\n",
      "Model saved at: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\modelscbc_8_54_30\n",
      "Model: modelscbc_8_54_30_ep72_norm0_bits3.2_bs128_lr0.001 has been trained\n",
      "Using this model modelscbc_8_54_30_ep72_norm0_bits3.2_bs128_lr0.001\n",
      "Directory already exists: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\n",
      "model path c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\modelscbc_9_54_30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f04698c6ccef4eca836eab536e4f030d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing pre-training evaluation on the model in epoch 0\n",
      "\n",
      "Val loss: 75.823| Val KL: 74.85989464653863 | Val Rec: 0.964\n",
      "\n",
      "Epoch 1\n",
      "--------------------\n",
      "Train loss: 75.009|Train Rec: 0.138 | Val loss: 74.721, Val Rec: -0.141\n",
      "\n",
      "Epoch 2\n",
      "--------------------\n",
      "Train loss: 74.636|Train Rec: -0.237 | Val loss: 74.507, Val Rec: -0.357\n",
      "\n",
      "Epoch 3\n",
      "--------------------\n",
      "Train loss: 74.505|Train Rec: -0.368 | Val loss: 74.405, Val Rec: -0.456\n",
      "\n",
      "Epoch 4\n",
      "--------------------\n",
      "Train loss: 74.425|Train Rec: -0.448 | Val loss: 74.307, Val Rec: -0.553\n",
      "\n",
      "Epoch 5\n",
      "--------------------\n",
      "Train loss: 74.373|Train Rec: -0.499 | Val loss: 74.231, Val Rec: -0.631\n",
      "\n",
      "Epoch 6\n",
      "--------------------\n",
      "Train loss: 74.334|Train Rec: -0.537 | Val loss: 74.215, Val Rec: -0.646\n",
      "\n",
      "Epoch 7\n",
      "--------------------\n",
      "Train loss: 74.301|Train Rec: -0.569 | Val loss: 74.158, Val Rec: -0.702\n",
      "\n",
      "Epoch 8\n",
      "--------------------\n",
      "Train loss: 74.264|Train Rec: -0.605 | Val loss: 74.111, Val Rec: -0.750\n",
      "\n",
      "Epoch 9\n",
      "--------------------\n",
      "Train loss: 74.238|Train Rec: -0.631 | Val loss: 74.068, Val Rec: -0.794\n",
      "\n",
      "Epoch 10\n",
      "--------------------\n",
      "Train loss: 74.221|Train Rec: -0.647 | Val loss: 74.069, Val Rec: -0.793\n",
      "\n",
      "Epoch 11\n",
      "--------------------\n",
      "Train loss: 74.211|Train Rec: -0.657 | Val loss: 73.997, Val Rec: -0.867\n",
      "\n",
      "Epoch 12\n",
      "--------------------\n",
      "Train loss: 74.202|Train Rec: -0.665 | Val loss: 74.037, Val Rec: -0.827\n",
      "\n",
      "Epoch 13\n",
      "--------------------\n",
      "Train loss: 74.190|Train Rec: -0.677 | Val loss: 73.988, Val Rec: -0.876\n",
      "\n",
      "Epoch 14\n",
      "--------------------\n",
      "Train loss: 74.185|Train Rec: -0.683 | Val loss: 74.010, Val Rec: -0.854\n",
      "\n",
      "Epoch 15\n",
      "--------------------\n",
      "Train loss: 74.187|Train Rec: -0.681 | Val loss: 73.971, Val Rec: -0.892\n",
      "\n",
      "Epoch 16\n",
      "--------------------\n",
      "Train loss: 74.180|Train Rec: -0.688 | Val loss: 74.011, Val Rec: -0.852\n",
      "\n",
      "Epoch 17\n",
      "--------------------\n",
      "Train loss: 74.177|Train Rec: -0.691 | Val loss: 73.983, Val Rec: -0.882\n",
      "\n",
      "Epoch 18\n",
      "--------------------\n",
      "Train loss: 74.175|Train Rec: -0.693 | Val loss: 74.002, Val Rec: -0.863\n",
      "\n",
      "Epoch 19\n",
      "--------------------\n",
      "Train loss: 74.171|Train Rec: -0.696 | Val loss: 74.006, Val Rec: -0.857\n",
      "\n",
      "Epoch 20\n",
      "--------------------\n",
      "Train loss: 74.164|Train Rec: -0.704 | Val loss: 73.947, Val Rec: -0.915\n",
      "\n",
      "Epoch 21\n",
      "--------------------\n",
      "Train loss: 74.164|Train Rec: -0.703 | Val loss: 73.959, Val Rec: -0.904\n",
      "\n",
      "Epoch 22\n",
      "--------------------\n",
      "Train loss: 74.162|Train Rec: -0.705 | Val loss: 73.965, Val Rec: -0.897\n",
      "\n",
      "Epoch 23\n",
      "--------------------\n",
      "Train loss: 74.167|Train Rec: -0.701 | Val loss: 73.967, Val Rec: -0.895\n",
      "\n",
      "Epoch 24\n",
      "--------------------\n",
      "Train loss: 74.157|Train Rec: -0.710 | Val loss: 73.972, Val Rec: -0.892\n",
      "\n",
      "Epoch 25\n",
      "--------------------\n",
      "Train loss: 74.154|Train Rec: -0.713 | Val loss: 73.986, Val Rec: -0.876\n",
      "\n",
      "Epoch 26\n",
      "--------------------\n",
      "Train loss: 74.157|Train Rec: -0.710 | Val loss: 73.954, Val Rec: -0.909\n",
      "\n",
      "Epoch 27\n",
      "--------------------\n",
      "Train loss: 74.150|Train Rec: -0.717 | Val loss: 73.953, Val Rec: -0.911\n",
      "\n",
      "Epoch 28\n",
      "--------------------\n",
      "Train loss: 74.153|Train Rec: -0.714 | Val loss: 73.957, Val Rec: -0.906\n",
      "\n",
      "Epoch 29\n",
      "--------------------\n",
      "Train loss: 74.147|Train Rec: -0.719 | Val loss: 73.961, Val Rec: -0.903\n",
      "\n",
      "Epoch 30\n",
      "--------------------\n",
      "Train loss: 74.147|Train Rec: -0.720 | Val loss: 73.950, Val Rec: -0.912\n",
      "\n",
      "Epoch 31\n",
      "--------------------\n",
      "Train loss: 74.147|Train Rec: -0.719 | Val loss: 73.947, Val Rec: -0.916\n",
      "\n",
      "Epoch 32\n",
      "--------------------\n",
      "Train loss: 74.141|Train Rec: -0.725 | Val loss: 73.944, Val Rec: -0.918\n",
      "\n",
      "Epoch 33\n",
      "--------------------\n",
      "Train loss: 74.144|Train Rec: -0.722 | Val loss: 73.946, Val Rec: -0.916\n",
      "\n",
      "Epoch 34\n",
      "--------------------\n",
      "Train loss: 74.147|Train Rec: -0.720 | Val loss: 73.957, Val Rec: -0.905\n",
      "\n",
      "Epoch 35\n",
      "--------------------\n",
      "Train loss: 74.138|Train Rec: -0.729 | Val loss: 73.947, Val Rec: -0.915\n",
      "\n",
      "Epoch 36\n",
      "--------------------\n",
      "Train loss: 74.141|Train Rec: -0.726 | Val loss: 73.944, Val Rec: -0.918\n",
      "\n",
      "Epoch 37\n",
      "--------------------\n",
      "Train loss: 74.142|Train Rec: -0.724 | Val loss: 73.949, Val Rec: -0.914\n",
      "\n",
      "Epoch 38\n",
      "--------------------\n",
      "Train loss: 74.144|Train Rec: -0.723 | Val loss: 73.955, Val Rec: -0.907\n",
      "\n",
      "Epoch 39\n",
      "--------------------\n",
      "Train loss: 74.139|Train Rec: -0.728 | Val loss: 73.904, Val Rec: -0.959\n",
      "\n",
      "Epoch 40\n",
      "--------------------\n",
      "Train loss: 74.138|Train Rec: -0.728 | Val loss: 73.971, Val Rec: -0.891\n",
      "\n",
      "Epoch 41\n",
      "--------------------\n",
      "Train loss: 74.137|Train Rec: -0.730 | Val loss: 73.910, Val Rec: -0.953\n",
      "\n",
      "Epoch 42\n",
      "--------------------\n",
      "Train loss: 74.138|Train Rec: -0.728 | Val loss: 73.959, Val Rec: -0.904\n",
      "\n",
      "Epoch 43\n",
      "--------------------\n",
      "Train loss: 74.138|Train Rec: -0.728 | Val loss: 73.947, Val Rec: -0.916\n",
      "\n",
      "Epoch 44\n",
      "--------------------\n",
      "Train loss: 74.139|Train Rec: -0.727 | Val loss: 73.940, Val Rec: -0.923\n",
      "\n",
      "Epoch 45\n",
      "--------------------\n",
      "Train loss: 74.131|Train Rec: -0.734 | Val loss: 73.933, Val Rec: -0.929\n",
      "\n",
      "Epoch 46\n",
      "--------------------\n",
      "Train loss: 74.137|Train Rec: -0.729 | Val loss: 73.919, Val Rec: -0.943\n",
      "\n",
      "Epoch 47\n",
      "--------------------\n",
      "Train loss: 74.136|Train Rec: -0.731 | Val loss: 73.915, Val Rec: -0.948\n",
      "\n",
      "Epoch 48\n",
      "--------------------\n",
      "Train loss: 74.130|Train Rec: -0.737 | Val loss: 73.920, Val Rec: -0.942\n",
      "\n",
      "Epoch 49\n",
      "--------------------\n",
      "Train loss: 74.132|Train Rec: -0.734 | Val loss: 73.920, Val Rec: -0.941\n",
      "\n",
      "Patience exceeded at 49 with last checkpoint saved at 41\n",
      "changed learning rate to 0.001\n",
      "Epoch 50\n",
      "--------------------\n",
      "Train loss: 74.114|Train Rec: -0.751 | Val loss: 73.870, Val Rec: -0.992\n",
      "\n",
      "Epoch 51\n",
      "--------------------\n",
      "Train loss: 74.107|Train Rec: -0.758 | Val loss: 73.879, Val Rec: -0.982\n",
      "\n",
      "Epoch 52\n",
      "--------------------\n",
      "Train loss: 74.106|Train Rec: -0.760 | Val loss: 73.867, Val Rec: -0.993\n",
      "\n",
      "Epoch 53\n",
      "--------------------\n",
      "Train loss: 74.104|Train Rec: -0.761 | Val loss: 73.881, Val Rec: -0.980\n",
      "\n",
      "Epoch 54\n",
      "--------------------\n",
      "Train loss: 74.106|Train Rec: -0.759 | Val loss: 73.871, Val Rec: -0.990\n",
      "\n",
      "Epoch 55\n",
      "--------------------\n",
      "Train loss: 74.105|Train Rec: -0.760 | Val loss: 73.868, Val Rec: -0.993\n",
      "\n",
      "Epoch 56\n",
      "--------------------\n",
      "Train loss: 74.108|Train Rec: -0.757 | Val loss: 73.866, Val Rec: -0.995\n",
      "\n",
      "Epoch 57\n",
      "--------------------\n",
      "Train loss: 74.105|Train Rec: -0.760 | Val loss: 73.869, Val Rec: -0.993\n",
      "\n",
      "Epoch 58\n",
      "--------------------\n",
      "Train loss: 74.106|Train Rec: -0.759 | Val loss: 73.870, Val Rec: -0.991\n",
      "\n",
      "Epoch 59\n",
      "--------------------\n",
      "Train loss: 74.103|Train Rec: -0.762 | Val loss: 73.863, Val Rec: -0.998\n",
      "\n",
      "Epoch 60\n",
      "--------------------\n",
      "Train loss: 74.101|Train Rec: -0.765 | Val loss: 73.872, Val Rec: -0.990\n",
      "\n",
      "Epoch 61\n",
      "--------------------\n",
      "Train loss: 74.104|Train Rec: -0.761 | Val loss: 73.867, Val Rec: -0.994\n",
      "\n",
      "Epoch 62\n",
      "--------------------\n",
      "Train loss: 74.105|Train Rec: -0.760 | Val loss: 73.870, Val Rec: -0.992\n",
      "\n",
      "Epoch 63\n",
      "--------------------\n",
      "Train loss: 74.104|Train Rec: -0.761 | Val loss: 73.874, Val Rec: -0.987\n",
      "\n",
      "Epoch 64\n",
      "--------------------\n",
      "Train loss: 74.107|Train Rec: -0.757 | Val loss: 73.870, Val Rec: -0.991\n",
      "\n",
      "Epoch 65\n",
      "--------------------\n",
      "Train loss: 74.104|Train Rec: -0.761 | Val loss: 73.863, Val Rec: -0.998\n",
      "\n",
      "Epoch 66\n",
      "--------------------\n",
      "Train loss: 74.102|Train Rec: -0.764 | Val loss: 73.872, Val Rec: -0.989\n",
      "\n",
      "Epoch 67\n",
      "--------------------\n",
      "Train loss: 74.104|Train Rec: -0.761 | Val loss: 73.865, Val Rec: -0.997\n",
      "\n",
      "Epoch 68\n",
      "--------------------\n",
      "Train loss: 74.107|Train Rec: -0.757 | Val loss: 73.875, Val Rec: -0.987\n",
      "\n",
      "Epoch 69\n",
      "--------------------\n",
      "Train loss: 74.104|Train Rec: -0.761 | Val loss: 73.864, Val Rec: -0.998\n",
      "\n",
      "Epoch 70\n",
      "--------------------\n",
      "Train loss: 74.106|Train Rec: -0.759 | Val loss: 73.864, Val Rec: -0.998\n",
      "\n",
      "Early stopping at epoch 70 with last checkpoint saved at 59\n",
      "Model saved at: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\modelscbc_9_54_30\n",
      "Model: modelscbc_9_54_30_ep59_norm0_bits3.6_bs128_lr0.001 has been trained\n",
      "Using this model modelscbc_9_54_30_ep59_norm0_bits3.6_bs128_lr0.001\n",
      "Directory already exists: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\n",
      "model path c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\modelscbc_10_54_30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53eba678f5764660bbfdf94605d79f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing pre-training evaluation on the model in epoch 0\n",
      "\n",
      "Val loss: 84.145| Val KL: 83.17765045166016 | Val Rec: 0.968\n",
      "\n",
      "Epoch 1\n",
      "--------------------\n",
      "Train loss: 83.306|Train Rec: 0.119 | Val loss: 83.019, Val Rec: -0.161\n",
      "\n",
      "Epoch 2\n",
      "--------------------\n",
      "Train loss: 82.919|Train Rec: -0.271 | Val loss: 82.814, Val Rec: -0.364\n",
      "\n",
      "Epoch 3\n",
      "--------------------\n",
      "Train loss: 82.789|Train Rec: -0.400 | Val loss: 82.648, Val Rec: -0.533\n",
      "\n",
      "Epoch 4\n",
      "--------------------\n",
      "Train loss: 82.720|Train Rec: -0.470 | Val loss: 82.663, Val Rec: -0.515\n",
      "\n",
      "Epoch 5\n",
      "--------------------\n",
      "Train loss: 82.663|Train Rec: -0.525 | Val loss: 82.528, Val Rec: -0.650\n",
      "\n",
      "Epoch 6\n",
      "--------------------\n",
      "Train loss: 82.616|Train Rec: -0.571 | Val loss: 82.496, Val Rec: -0.682\n",
      "\n",
      "Epoch 7\n",
      "--------------------\n",
      "Train loss: 82.584|Train Rec: -0.602 | Val loss: 82.420, Val Rec: -0.758\n",
      "\n",
      "Epoch 8\n",
      "--------------------\n",
      "Train loss: 82.554|Train Rec: -0.630 | Val loss: 82.431, Val Rec: -0.747\n",
      "\n",
      "Epoch 9\n",
      "--------------------\n",
      "Train loss: 82.541|Train Rec: -0.643 | Val loss: 82.347, Val Rec: -0.831\n",
      "\n",
      "Epoch 10\n",
      "--------------------\n",
      "Train loss: 82.520|Train Rec: -0.664 | Val loss: 82.406, Val Rec: -0.772\n",
      "\n",
      "Epoch 11\n",
      "--------------------\n",
      "Train loss: 82.517|Train Rec: -0.666 | Val loss: 82.334, Val Rec: -0.845\n",
      "\n",
      "Epoch 12\n",
      "--------------------\n",
      "Train loss: 82.495|Train Rec: -0.688 | Val loss: 82.291, Val Rec: -0.888\n",
      "\n",
      "Epoch 13\n",
      "--------------------\n",
      "Train loss: 82.491|Train Rec: -0.692 | Val loss: 82.347, Val Rec: -0.833\n",
      "\n",
      "Epoch 14\n",
      "--------------------\n",
      "Train loss: 82.485|Train Rec: -0.699 | Val loss: 82.289, Val Rec: -0.890\n",
      "\n",
      "Epoch 15\n",
      "--------------------\n",
      "Train loss: 82.472|Train Rec: -0.711 | Val loss: 82.250, Val Rec: -0.929\n",
      "\n",
      "Epoch 16\n",
      "--------------------\n",
      "Train loss: 82.466|Train Rec: -0.717 | Val loss: 82.223, Val Rec: -0.957\n",
      "\n",
      "Epoch 17\n",
      "--------------------\n",
      "Train loss: 82.460|Train Rec: -0.723 | Val loss: 82.219, Val Rec: -0.960\n",
      "\n",
      "Epoch 18\n",
      "--------------------\n",
      "Train loss: 82.459|Train Rec: -0.724 | Val loss: 82.263, Val Rec: -0.917\n",
      "\n",
      "Epoch 19\n",
      "--------------------\n",
      "Train loss: 82.451|Train Rec: -0.732 | Val loss: 82.251, Val Rec: -0.929\n",
      "\n",
      "Epoch 20\n",
      "--------------------\n",
      "Train loss: 82.456|Train Rec: -0.727 | Val loss: 82.272, Val Rec: -0.908\n",
      "\n",
      "Epoch 21\n",
      "--------------------\n",
      "Train loss: 82.444|Train Rec: -0.738 | Val loss: 82.235, Val Rec: -0.945\n",
      "\n",
      "Epoch 22\n",
      "--------------------\n",
      "Train loss: 82.443|Train Rec: -0.739 | Val loss: 82.190, Val Rec: -0.990\n",
      "\n",
      "Epoch 23\n",
      "--------------------\n",
      "Train loss: 82.448|Train Rec: -0.735 | Val loss: 82.254, Val Rec: -0.927\n",
      "\n",
      "Epoch 24\n",
      "--------------------\n",
      "Train loss: 82.439|Train Rec: -0.744 | Val loss: 82.247, Val Rec: -0.934\n",
      "\n",
      "Epoch 25\n",
      "--------------------\n",
      "Train loss: 82.439|Train Rec: -0.745 | Val loss: 82.190, Val Rec: -0.990\n",
      "\n",
      "Epoch 26\n",
      "--------------------\n",
      "Train loss: 82.439|Train Rec: -0.744 | Val loss: 82.204, Val Rec: -0.976\n",
      "\n",
      "Epoch 27\n",
      "--------------------\n",
      "Train loss: 82.425|Train Rec: -0.758 | Val loss: 82.211, Val Rec: -0.970\n",
      "\n",
      "Epoch 28\n",
      "--------------------\n",
      "Train loss: 82.429|Train Rec: -0.754 | Val loss: 82.273, Val Rec: -0.906\n",
      "\n",
      "Epoch 29\n",
      "--------------------\n",
      "Train loss: 82.434|Train Rec: -0.749 | Val loss: 82.205, Val Rec: -0.975\n",
      "\n",
      "Epoch 30\n",
      "--------------------\n",
      "Train loss: 82.431|Train Rec: -0.752 | Val loss: 82.196, Val Rec: -0.985\n",
      "\n",
      "Epoch 31\n",
      "--------------------\n",
      "Train loss: 82.424|Train Rec: -0.759 | Val loss: 82.205, Val Rec: -0.975\n",
      "\n",
      "Epoch 32\n",
      "--------------------\n",
      "Train loss: 82.426|Train Rec: -0.757 | Val loss: 82.222, Val Rec: -0.958\n",
      "\n",
      "Epoch 33\n",
      "--------------------\n",
      "Train loss: 82.423|Train Rec: -0.760 | Val loss: 82.178, Val Rec: -1.001\n",
      "\n",
      "Epoch 34\n",
      "--------------------\n",
      "Train loss: 82.423|Train Rec: -0.760 | Val loss: 82.211, Val Rec: -0.967\n",
      "\n",
      "Epoch 35\n",
      "--------------------\n",
      "Train loss: 82.426|Train Rec: -0.757 | Val loss: 82.193, Val Rec: -0.985\n",
      "\n",
      "Epoch 36\n",
      "--------------------\n",
      "Train loss: 82.420|Train Rec: -0.763 | Val loss: 82.204, Val Rec: -0.978\n",
      "\n",
      "Epoch 37\n",
      "--------------------\n",
      "Train loss: 82.419|Train Rec: -0.763 | Val loss: 82.190, Val Rec: -0.990\n",
      "\n",
      "Epoch 38\n",
      "--------------------\n",
      "Train loss: 82.426|Train Rec: -0.756 | Val loss: 82.202, Val Rec: -0.976\n",
      "\n",
      "Epoch 39\n",
      "--------------------\n",
      "Train loss: 82.421|Train Rec: -0.762 | Val loss: 82.191, Val Rec: -0.990\n",
      "\n",
      "Epoch 40\n",
      "--------------------\n",
      "Train loss: 82.422|Train Rec: -0.761 | Val loss: 82.193, Val Rec: -0.987\n",
      "\n",
      "Epoch 41\n",
      "--------------------\n",
      "Train loss: 82.418|Train Rec: -0.764 | Val loss: 82.252, Val Rec: -0.927\n",
      "\n",
      "Epoch 42\n",
      "--------------------\n",
      "Train loss: 82.429|Train Rec: -0.754 | Val loss: 82.228, Val Rec: -0.951\n",
      "\n",
      "Epoch 43\n",
      "--------------------\n",
      "Train loss: 82.415|Train Rec: -0.767 | Val loss: 82.183, Val Rec: -0.996\n",
      "\n",
      "Epoch 44\n",
      "--------------------\n",
      "Train loss: 82.416|Train Rec: -0.766 | Val loss: 82.186, Val Rec: -0.992\n",
      "\n",
      "Epoch 45\n",
      "--------------------\n",
      "Train loss: 82.414|Train Rec: -0.768 | Val loss: 82.200, Val Rec: -0.979\n",
      "\n",
      "Epoch 46\n",
      "--------------------\n",
      "Train loss: 82.423|Train Rec: -0.760 | Val loss: 82.203, Val Rec: -0.976\n",
      "\n",
      "Epoch 47\n",
      "--------------------\n",
      "Train loss: 82.417|Train Rec: -0.765 | Val loss: 82.184, Val Rec: -0.994\n",
      "\n",
      "Epoch 48\n",
      "--------------------\n",
      "Train loss: 82.411|Train Rec: -0.772 | Val loss: 82.240, Val Rec: -0.941\n",
      "\n",
      "Epoch 49\n",
      "--------------------\n",
      "Train loss: 82.414|Train Rec: -0.768 | Val loss: 82.196, Val Rec: -0.983\n",
      "\n",
      "Epoch 50\n",
      "--------------------\n",
      "Train loss: 82.416|Train Rec: -0.767 | Val loss: 82.214, Val Rec: -0.965\n",
      "\n",
      "Epoch 51\n",
      "--------------------\n",
      "Train loss: 82.411|Train Rec: -0.771 | Val loss: 82.213, Val Rec: -0.968\n",
      "\n",
      "Patience exceeded at 51 with last checkpoint saved at 43\n",
      "changed learning rate to 0.001\n",
      "Epoch 52\n",
      "--------------------\n",
      "Train loss: 82.396|Train Rec: -0.785 | Val loss: 82.137, Val Rec: -1.042\n",
      "\n",
      "Epoch 53\n",
      "--------------------\n",
      "Train loss: 82.387|Train Rec: -0.795 | Val loss: 82.131, Val Rec: -1.048\n",
      "\n",
      "Epoch 54\n",
      "--------------------\n",
      "Train loss: 82.386|Train Rec: -0.796 | Val loss: 82.127, Val Rec: -1.052\n",
      "\n",
      "Epoch 55\n",
      "--------------------\n",
      "Train loss: 82.384|Train Rec: -0.797 | Val loss: 82.140, Val Rec: -1.039\n",
      "\n",
      "Epoch 56\n",
      "--------------------\n",
      "Train loss: 82.381|Train Rec: -0.800 | Val loss: 82.125, Val Rec: -1.054\n",
      "\n",
      "Epoch 57\n",
      "--------------------\n",
      "Train loss: 82.385|Train Rec: -0.796 | Val loss: 82.132, Val Rec: -1.048\n",
      "\n",
      "Epoch 58\n",
      "--------------------\n",
      "Train loss: 82.387|Train Rec: -0.794 | Val loss: 82.128, Val Rec: -1.051\n",
      "\n",
      "Epoch 59\n",
      "--------------------\n",
      "Train loss: 82.386|Train Rec: -0.795 | Val loss: 82.125, Val Rec: -1.054\n",
      "\n",
      "Epoch 60\n",
      "--------------------\n",
      "Train loss: 82.386|Train Rec: -0.796 | Val loss: 82.123, Val Rec: -1.057\n",
      "\n",
      "Epoch 61\n",
      "--------------------\n",
      "Train loss: 82.386|Train Rec: -0.796 | Val loss: 82.127, Val Rec: -1.053\n",
      "\n",
      "Epoch 62\n",
      "--------------------\n",
      "Train loss: 82.383|Train Rec: -0.799 | Val loss: 82.129, Val Rec: -1.050\n",
      "\n",
      "Epoch 63\n",
      "--------------------\n",
      "Train loss: 82.382|Train Rec: -0.800 | Val loss: 82.126, Val Rec: -1.053\n",
      "\n",
      "Epoch 64\n",
      "--------------------\n",
      "Train loss: 82.385|Train Rec: -0.797 | Val loss: 82.127, Val Rec: -1.052\n",
      "\n",
      "Epoch 65\n",
      "--------------------\n",
      "Train loss: 82.385|Train Rec: -0.797 | Val loss: 82.134, Val Rec: -1.045\n",
      "\n",
      "Epoch 66\n",
      "--------------------\n",
      "Train loss: 82.384|Train Rec: -0.798 | Val loss: 82.129, Val Rec: -1.050\n",
      "\n",
      "Epoch 67\n",
      "--------------------\n",
      "Train loss: 82.384|Train Rec: -0.797 | Val loss: 82.123, Val Rec: -1.056\n",
      "\n",
      "Epoch 68\n",
      "--------------------\n",
      "Train loss: 82.385|Train Rec: -0.796 | Val loss: 82.142, Val Rec: -1.037\n",
      "\n",
      "Epoch 69\n",
      "--------------------\n",
      "Train loss: 82.385|Train Rec: -0.796 | Val loss: 82.126, Val Rec: -1.053\n",
      "\n",
      "Epoch 70\n",
      "--------------------\n",
      "Train loss: 82.381|Train Rec: -0.800 | Val loss: 82.125, Val Rec: -1.054\n",
      "\n",
      "Epoch 71\n",
      "--------------------\n",
      "Train loss: 82.384|Train Rec: -0.797 | Val loss: 82.137, Val Rec: -1.043\n",
      "\n",
      "Early stopping at epoch 71 with last checkpoint saved at 60\n",
      "Model saved at: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\modelscbc_10_54_30\n",
      "Model: modelscbc_10_54_30_ep60_norm0_bits4.0_bs128_lr0.001 has been trained\n",
      "Using this model modelscbc_10_54_30_ep60_norm0_bits4.0_bs128_lr0.001\n"
     ]
    }
   ],
   "source": [
    "bitlst = np.linspace(0,4,11).round(2).tolist()\n",
    "final_df = None\n",
    "for i, bits in enumerate(bitlst):\n",
    "    hidden_dim = 54\n",
    "    latent_dim = 30\n",
    "    n=i\n",
    "\n",
    "    # Instantiate the model\n",
    "    model1 = v1.VAE(\n",
    "        n_features=54,\n",
    "        latent_dim=latent_dim,\n",
    "        hidden_layer=True,\n",
    "        hidden_dim=hidden_dim,\n",
    "        output_activ=nn.Sigmoid(),\n",
    "    ).to(device)\n",
    "\n",
    "    # need to set the model name with the layers - usefull for creating its unique folder \n",
    "    model_name = f\"modelscbc_{n}_{hidden_dim}_{latent_dim}\"\n",
    "\n",
    "    # the optimizer is in the train-val loop \n",
    "\n",
    "\n",
    "    ## Create a \"models\" folder and the specifics model's directory to save figures  \n",
    "\n",
    "    # create the models directory path \n",
    "    path_dir = os.getcwd() + \"\\\\models\"\n",
    "\n",
    "    # Check if the models directory exists, if not, create it\n",
    "    if not os.path.exists(path_dir):\n",
    "        os.makedirs(path_dir)\n",
    "        print(f\"Created directory: {path_dir}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists: {path_dir}\")\n",
    "\n",
    "    # create a subdirectory for each model based on #number and name (the dims of layers)\n",
    "    model_path = os.path.join(path_dir,model_name)\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    model_path\n",
    "\n",
    "\n",
    "    ## Final Important part\n",
    "\n",
    "    # set all the parameters to variables because all functions depend on them\n",
    "    model = model1\n",
    "    loss_fun = cf.loss_fun\n",
    "    model_name=model_name\n",
    "    path=model_path\n",
    "    epoch = 200\n",
    "    learn_r = 0.005\n",
    "    freebits = bits\n",
    "    batch_size = 128\n",
    "    norm = 0\n",
    "\n",
    "    # the path where this model is going to be saved \n",
    "    print(f\"model path {path}\")\n",
    "\n",
    "    # run the training for the model\n",
    "    # Run the loop - see the parameters \n",
    "\n",
    "    batch_dict, epoch_dict,hyperparam_str = uf.train_val_loop_v2(\n",
    "    model = model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader, \n",
    "    loss_fun = loss_fun,\n",
    "    model_name=model_name,\n",
    "    model_path=path,\n",
    "    epoch = epoch,\n",
    "    patience = 7,\n",
    "    learn_r = learn_r,\n",
    "    freebits = freebits,\n",
    "    batch_size = batch_size,\n",
    "    norm = norm\n",
    "    )\n",
    "\n",
    "    # write the full model id\n",
    "    model_id = model_name + \"_\" + hyperparam_str\n",
    "    print(f\"Model: {model_id} has been trained\")\n",
    "\n",
    "\n",
    "    # next run the test set analysis for the eaxh model and get the results test_iter_dict and test_metrics \n",
    "    test_iter_dict, test_metrics = uf.test_set_analysis(\n",
    "    model=model,\n",
    "    test_loader=test_loader,\n",
    "    loss_fun = loss_fun,\n",
    "    freebits=freebits,\n",
    "    model_id=model_id\n",
    ")\n",
    "    test_df = pd.DataFrame([test_metrics])\n",
    "    if final_df is None:\n",
    "        final_df = test_df\n",
    "    else:\n",
    "        final_df = pd.concat([final_df,test_df],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>bits</th>\n",
       "      <th>avg_total_loss</th>\n",
       "      <th>avg_kl_loss</th>\n",
       "      <th>avg_rl_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modelscbc_0_54_30_ep65_norm0_bits0.0_bs128_lr0...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062917</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.062916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modelscbc_1_54_30_ep61_norm0_bits0.4_bs128_lr0...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>8.149564</td>\n",
       "      <td>8.318919</td>\n",
       "      <td>-0.169354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modelscbc_2_54_30_ep91_norm0_bits0.8_bs128_lr0...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>16.274271</td>\n",
       "      <td>16.635646</td>\n",
       "      <td>-0.361374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modelscbc_3_54_30_ep83_norm0_bits1.2_bs128_lr0...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>24.447544</td>\n",
       "      <td>24.953459</td>\n",
       "      <td>-0.505915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modelscbc_4_54_30_ep87_norm0_bits1.6_bs128_lr0...</td>\n",
       "      <td>1.6</td>\n",
       "      <td>32.635786</td>\n",
       "      <td>33.271460</td>\n",
       "      <td>-0.635674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modelscbc_5_54_30_ep75_norm0_bits2.0_bs128_lr0...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>40.831840</td>\n",
       "      <td>41.589343</td>\n",
       "      <td>-0.757503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modelscbc_6_54_30_ep80_norm0_bits2.4_bs128_lr0...</td>\n",
       "      <td>2.4</td>\n",
       "      <td>49.087279</td>\n",
       "      <td>49.907851</td>\n",
       "      <td>-0.820573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modelscbc_7_54_30_ep67_norm0_bits2.8_bs128_lr0...</td>\n",
       "      <td>2.8</td>\n",
       "      <td>57.337079</td>\n",
       "      <td>58.225405</td>\n",
       "      <td>-0.888327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modelscbc_8_54_30_ep72_norm0_bits3.2_bs128_lr0...</td>\n",
       "      <td>3.2</td>\n",
       "      <td>65.602178</td>\n",
       "      <td>66.543991</td>\n",
       "      <td>-0.941814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modelscbc_9_54_30_ep59_norm0_bits3.6_bs128_lr0...</td>\n",
       "      <td>3.6</td>\n",
       "      <td>73.874283</td>\n",
       "      <td>74.860704</td>\n",
       "      <td>-0.986421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>modelscbc_10_54_30_ep60_norm0_bits4.0_bs128_lr...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>82.145669</td>\n",
       "      <td>83.178548</td>\n",
       "      <td>-1.032879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            model_id  bits  avg_total_loss  \\\n",
       "0  modelscbc_0_54_30_ep65_norm0_bits0.0_bs128_lr0...   0.0        0.062917   \n",
       "0  modelscbc_1_54_30_ep61_norm0_bits0.4_bs128_lr0...   0.4        8.149564   \n",
       "0  modelscbc_2_54_30_ep91_norm0_bits0.8_bs128_lr0...   0.8       16.274271   \n",
       "0  modelscbc_3_54_30_ep83_norm0_bits1.2_bs128_lr0...   1.2       24.447544   \n",
       "0  modelscbc_4_54_30_ep87_norm0_bits1.6_bs128_lr0...   1.6       32.635786   \n",
       "0  modelscbc_5_54_30_ep75_norm0_bits2.0_bs128_lr0...   2.0       40.831840   \n",
       "0  modelscbc_6_54_30_ep80_norm0_bits2.4_bs128_lr0...   2.4       49.087279   \n",
       "0  modelscbc_7_54_30_ep67_norm0_bits2.8_bs128_lr0...   2.8       57.337079   \n",
       "0  modelscbc_8_54_30_ep72_norm0_bits3.2_bs128_lr0...   3.2       65.602178   \n",
       "0  modelscbc_9_54_30_ep59_norm0_bits3.6_bs128_lr0...   3.6       73.874283   \n",
       "0  modelscbc_10_54_30_ep60_norm0_bits4.0_bs128_lr...   4.0       82.145669   \n",
       "\n",
       "   avg_kl_loss  avg_rl_loss  \n",
       "0     0.000002     0.062916  \n",
       "0     8.318919    -0.169354  \n",
       "0    16.635646    -0.361374  \n",
       "0    24.953459    -0.505915  \n",
       "0    33.271460    -0.635674  \n",
       "0    41.589343    -0.757503  \n",
       "0    49.907851    -0.820573  \n",
       "0    58.225405    -0.888327  \n",
       "0    66.543991    -0.941814  \n",
       "0    74.860704    -0.986421  \n",
       "0    83.178548    -1.032879  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plots Rec los vs bits \n",
    "fig, ax1 = plt.subplots(figsize=(12,8))\n",
    "ax1 = sns.lineplot(\n",
    "            final_df,x=final_df[\"bits\"], y=final_df[\"avg_rl_loss\"],\n",
    "            lw = 2, color = \"royalblue\", alpha = 0.7, label=\"Rec Error\",\n",
    "            marker = \"o\", markerfacecolor=\"black\")\n",
    "\n",
    "ax1.set_ylim([\n",
    "            np.min(final_df[\"avg_rl_loss\"])*1.05,\n",
    "            np.max(final_df[\"avg_rl_loss\"])*1.05\n",
    "            ])\n",
    "\n",
    "ax1.legend(frameon = False, ncol=2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scbc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
