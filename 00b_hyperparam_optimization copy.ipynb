{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries and import modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the vanila libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.random as nrd\n",
    "import os\n",
    "import pathlib \n",
    "import sys\n",
    "from typing import Callable\n",
    "import itertools\n",
    "import gc\n",
    "import time\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "import umap\n",
    "\n",
    "# Pytorch modules \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# this for the custom Dataset \n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, pairwise_distances\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# for timing functions\n",
    "from timeit import default_timer as timer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Project Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\gpano\\\\Desktop\\\\github_py\\\\proteomics_latent_space'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check your current directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** Run the configuration file first `configs.py`. Importing this script and setting the seed and device parameters before importing any of the other modules ensures that evereything is sync.\n",
    "\n",
    "**Important** If you want *change the configuration parameters*, change them before importing and running the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing models_util.configs module\n",
      "First set device and seed for reproducibility.\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from models_util import configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seed: None, Device: None'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs.get_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n"
     ]
    }
   ],
   "source": [
    "# print the global variables\n",
    "print(configs.project_seed, configs.project_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During configuration random seed 789 has been set.\n",
      "789 cpu\n"
     ]
    }
   ],
   "source": [
    "configs.set_seed(789)\n",
    "device = configs.set_device(force_cpu=True)\n",
    "\n",
    "# global variables have changed too\n",
    "print(configs.project_seed, configs.project_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seed: 789, Device: cpu'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see if the get function also agrees:\n",
    "configs.get_configs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the configurations values are assigned globally, we can import the modules. If this is working, we expect each module to access the **same** **seed** and **device** we set. We are also expecting generated numbers **inside the modules** to be reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During configuration random seed 789 has been set.\n",
      "Importing models_util.utility_functions, running in cpu with seed: 789\n"
     ]
    }
   ],
   "source": [
    "# Load home modules and check the device where they are running \n",
    "from models_util import utility_functions as uf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During configuration random seed 789 has been set.\n",
      "Importing models_util.custom_dataset, running in cpu with seed: 789\n"
     ]
    }
   ],
   "source": [
    "from models_util import custom_dataset as cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During configuration random seed 789 has been set.\n",
      "Importing models_util.cost_functions, running in cpu with seed: 789\n"
     ]
    }
   ],
   "source": [
    "from models_util import cost_functions as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During configuration random seed 789 has been set.\n",
      "Importing models_util.VAE1, running in cpu with seed: 789\n"
     ]
    }
   ],
   "source": [
    "from models_util import VAE1 as v1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data scale and split for VAE\n",
    "- We will perform min-max scaling to the TMT-Ratios of the proteomic SCBC data. <br>\n",
    "- We will scale the array version of our scbc data, the `npdata` matrix.\n",
    "- Then we will copy this scaled matrix and reshuffle the copy. The `npscbc_scaled_shuffled` will be used for the model training and performance evaluattion. <br>\n",
    "- The `npdata_scaled` matrix with the original order of rows will be used later for the validation of the latent variables. <br> \n",
    "- It is important to use the non-missing min and max values of dataset row-by-row <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create path and read the scbc data\n",
    "data_path = os.getcwd() + \"\\\\data\\\\processed\\\\\" \n",
    "data = pd.read_csv(data_path+\"prot_abms_norm.txt\",delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(15306)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to numpy \n",
    "npdata = data.to_numpy()\n",
    "np.isnan(npdata).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11209, 1) (11209, 1) 0 0\n"
     ]
    }
   ],
   "source": [
    "# Get extreme values (non-missing) frome ach row. \n",
    "data_min = np.nanmin(npdata, axis=1, keepdims=True)  # minimum among non-NaN\n",
    "data_max = np.nanmax(npdata, axis=1,keepdims=True)  # maximum among non-NaN\n",
    "\n",
    "# check that that shapes and values are as expected \n",
    "print(data_max.shape,data_min.shape,np.isnan(data_max).sum(), np.isnan(data_min).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11209, 54)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scale data \n",
    "npdata_scaled = (npdata - data_min) /(data_max - data_min + 1e-8)\n",
    "npdata_scaled.shape\n",
    "\n",
    "# npscbc_scaled[0:2,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the rows but keep scaled original\n",
    "npdata_scaled_shuffled = npdata_scaled.copy()\n",
    "np.random.shuffle(npdata_scaled_shuffled)\n",
    "# npscbc_scaled[1,],scbc.iloc[1,:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Split Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8406, 54), (1121, 54), (1682, 54))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, val_data, test_data = uf.create_data_partition(\n",
    "    npdata_scaled_shuffled, test_perc=0.15, val=True, val_perc=0.1\n",
    ")\n",
    "train_data.shape, val_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test reproducibility by re-runing the function and checking the data in the first index of the matrix. We expect it to be the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass data to Custom Dataset and DataLoaders \n",
    "- check that your data is numpy matrix.\n",
    "- check if data is scaled to (0,1).\n",
    "- create three custom dataset instances.\n",
    "- the custom dataset will save all the data to memory and create a mask where NaNs are located.\n",
    "- the numpy arrays will be converted to tensors of appropriate dimensions and NaNs to zeroes.\n",
    "- then we pass the custom dataset to the dataloader object.\n",
    "- The DataLoader object contains for each row (training example) i) a tensor of 1 x 130 columns with 0-1 scaled values, ii) a 1x130 mask indicating NA positions and iii) index of the examples per batch (could be 64, 128,..., batch_size). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protein Dataset is passed to memory\n",
      "No Protein Symbols were identified\n",
      "Protein Dataset is passed to memory\n",
      "No Protein Symbols were identified\n",
      "Protein Dataset is passed to memory\n",
      "No Protein Symbols were identified\n",
      "Protein Dataset is passed to memory\n",
      "No Protein Symbols were identified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = cd.ProteinDataset(train_data)\n",
    "val_dataset = cd.ProteinDataset(val_data)\n",
    "test_dataset = cd.ProteinDataset(test_data)\n",
    "whole_dataset = cd.ProteinDataset(npdata_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass data to the dataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False,drop_last=True)\n",
    "whole_loader = DataLoader(whole_dataset, batch_size=128, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.8556, 0.8802, 0.8614,  ..., 0.6423, 0.6196, 0.5990],\n",
       "         [0.0229, 0.0020, 0.0356,  ..., 0.2527, 0.2068, 0.2431],\n",
       "         [0.4199, 0.4289, 0.4633,  ..., 0.1165, 0.0860, 0.1325],\n",
       "         ...,\n",
       "         [0.0666, 0.0915, 0.0735,  ..., 0.0984, 0.1327, 0.1055],\n",
       "         [0.1673, 0.2089, 0.2423,  ..., 0.2401, 0.2461, 0.2781],\n",
       "         [0.0000, 0.1242, 0.0000,  ..., 0.0000, 0.1622, 0.0000]]),\n",
       " tensor([[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [ True, False,  True,  ...,  True, False,  True]]),\n",
       " tensor([4533, 8395, 1556, 4706, 3183, 6129, 8275, 4601, 7598, 4491,  766, 7960,\n",
       "         2285, 7439, 2658, 3824, 1335, 3125, 8246, 3767, 7144, 1220, 3377, 6766,\n",
       "         6048, 3638, 1801, 2293, 5473, 2720, 6120,  415, 6202, 6007, 5096, 8245,\n",
       "         6653, 7750, 1644, 3893, 4998, 3806, 4079, 2995, 3704, 7694,  898, 3979,\n",
       "          191,   31,  382, 2216, 1540, 5797, 5459, 3412, 6571, 1179, 2269, 2734,\n",
       "         6694, 7077, 1854, 8063, 5492, 1308, 4066, 3282, 7407, 7792,  141, 7114,\n",
       "          790, 7845, 4341, 3417, 1460, 8324,  297, 7711, 1567, 8082, 3391, 1180,\n",
       "         7588, 7108, 5914, 2362, 5541, 2582, 3944, 5864, 1373, 1421, 7413, 4679,\n",
       "         3115, 2608,  780, 4264, 4289, 2155, 1895,  239, 7934, 6637, 1322, 4842,\n",
       "         7287, 7129, 2206, 2484,  438, 1086, 5280, 7628, 2986, 1600, 1496, 7038,\n",
       "         6087, 7989, 2904, 2690, 5343, 1337, 2824, 3632], dtype=torch.int32)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the train loader is not reproducible bcs it shuffles but it is not seeded yet. \n",
    "# here is one batch of training examples \n",
    "# torch.manual_seed(888)\n",
    "\n",
    "\n",
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE dimensions optimization Loop with secondary task \n",
    "It comprises the run of the training and validation set. VAE inherently have a tendency to overfit, so it is important to keep the test set after training loop. In this tutorial we run one model. The name is based on a simple numbering system and its layers to track it down. Furthermore the train_val_loop creates a hyperparameter string to track other parameters. The whole loop is parametrized in a function: <br>\n",
    "- The function starts with a pre-training evaluation to initialize metrics at epoch = 0 <br>\n",
    "- Then training of the model begins and after each epoch, the validation set is passed through the model to get the validation - epoch metrics.<br>\n",
    "\n",
    "\n",
    "During training, these are computed:\n",
    "- KL, Gaussian Logliklihood error, and Total Error are monitored per training batch, and also averaged every n batches.\n",
    "- KL, Gaussian Logliklihood error, and Total Error are monitored per validation round (per epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the combinations of the VAE layers to check. \n",
    "comb_list = list()\n",
    "for comb in list(itertools.product([54,50, 45,40,30],[30,25,20,15])):\n",
    "    if comb[0] >= 1.5*comb[1]:\n",
    "        comb_list.append(comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>complex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EP300</td>\n",
       "      <td>CREBBP</td>\n",
       "      <td>Multisubunit ACTR coactivator complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KAT2B</td>\n",
       "      <td>EP300</td>\n",
       "      <td>Multisubunit ACTR coactivator complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCOA3</td>\n",
       "      <td>EP300</td>\n",
       "      <td>Multisubunit ACTR coactivator complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KAT2B</td>\n",
       "      <td>CREBBP</td>\n",
       "      <td>Multisubunit ACTR coactivator complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NCOA3</td>\n",
       "      <td>KAT2B</td>\n",
       "      <td>Multisubunit ACTR coactivator complex</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      V1      V2                                complex\n",
       "1  EP300  CREBBP  Multisubunit ACTR coactivator complex\n",
       "2  KAT2B   EP300  Multisubunit ACTR coactivator complex\n",
       "3  NCOA3   EP300  Multisubunit ACTR coactivator complex\n",
       "4  KAT2B  CREBBP  Multisubunit ACTR coactivator complex\n",
       "5  NCOA3   KAT2B  Multisubunit ACTR coactivator complex"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load groudtruth pairs \n",
    "pairs_df = pd.read_csv(os.getcwd() + \"\\\\data\\\\processed\\\\\" + \"merged_pairs.txt\", delimiter=\"\\t\")\n",
    "pairs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 54 30\n",
      "Directory already exists: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\n",
      "model path c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\abms_iter_0_54_30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5ac3d1c1ec498090e17d1ed40c8ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing pre-training evaluation on the model in epoch 0\n",
      "\n",
      "Val loss: 84.131| Val KL: 83.17765045166016 | Val Rec: 0.953\n",
      "\n",
      "Patience exceeded at 60 with last checkpoint saved at 52\n",
      "changed learning rate to 0.001\n",
      "Early stopping at epoch 94 with last checkpoint saved at 83\n",
      "Model saved at: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\abms_iter_0_54_30\n",
      "Model: abms_iter_0_54_30_ep83_norm0_bits4_bs128_lr0.001 has been trained\n",
      "Using this model abms_iter_0_54_30_ep83_norm0_bits4_bs128_lr0.001\n",
      "The decoder output is transformed with an activation function, so reoconstructions are scaled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gpano\\anaconda3\\envs\\scbc\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 54 25\n",
      "Directory already exists: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\n",
      "model path c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\abms_iter_1_54_25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2aae5aa018b454fbb5682398a2b9909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing pre-training evaluation on the model in epoch 0\n",
      "\n",
      "Val loss: 70.273| Val KL: 69.31470489501953 | Val Rec: 0.958\n",
      "\n",
      "Patience exceeded at 54 with last checkpoint saved at 46\n",
      "changed learning rate to 0.001\n",
      "Early stopping at epoch 69 with last checkpoint saved at 58\n",
      "Model saved at: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\abms_iter_1_54_25\n",
      "Model: abms_iter_1_54_25_ep58_norm0_bits4_bs128_lr0.001 has been trained\n",
      "Using this model abms_iter_1_54_25_ep58_norm0_bits4_bs128_lr0.001\n",
      "The decoder output is transformed with an activation function, so reoconstructions are scaled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gpano\\anaconda3\\envs\\scbc\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 54 20\n",
      "abms_iter_0_54_30\n",
      "abms_iter_1_54_25\n",
      "cor_base\n",
      "0 54 30\n",
      "Directory already exists: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\n",
      "model path c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\abms_iter_0_54_30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149b884723a74f6bbd317f6465fc4cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing pre-training evaluation on the model in epoch 0\n",
      "\n",
      "Val loss: 84.129| Val KL: 83.17765045166016 | Val Rec: 0.952\n",
      "\n",
      "Patience exceeded at 56 with last checkpoint saved at 48\n",
      "changed learning rate to 0.001\n",
      "Early stopping at epoch 76 with last checkpoint saved at 65\n",
      "Model saved at: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\abms_iter_0_54_30\n",
      "Model: abms_iter_0_54_30_ep65_norm0_bits4_bs128_lr0.001 has been trained\n",
      "Using this model abms_iter_0_54_30_ep65_norm0_bits4_bs128_lr0.001\n",
      "The decoder output is transformed with an activation function, so reoconstructions are scaled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gpano\\anaconda3\\envs\\scbc\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 54 25\n",
      "Directory already exists: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\n",
      "model path c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\abms_iter_1_54_25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c272e4ef027427cbbf2eac46072c6d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing pre-training evaluation on the model in epoch 0\n",
      "\n",
      "Val loss: 70.277| Val KL: 69.31470489501953 | Val Rec: 0.962\n",
      "\n",
      "Patience exceeded at 52 with last checkpoint saved at 44\n",
      "changed learning rate to 0.001\n",
      "Early stopping at epoch 73 with last checkpoint saved at 62\n",
      "Model saved at: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\abms_iter_1_54_25\n",
      "Model: abms_iter_1_54_25_ep62_norm0_bits4_bs128_lr0.001 has been trained\n",
      "Using this model abms_iter_1_54_25_ep62_norm0_bits4_bs128_lr0.001\n",
      "The decoder output is transformed with an activation function, so reoconstructions are scaled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gpano\\anaconda3\\envs\\scbc\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 54 20\n",
      "abms_iter_0_54_30\n",
      "abms_iter_1_54_25\n",
      "cor_base\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((4, 8), (6, 4))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dimlist = [(54,30)]\n",
    "# future for loop for at least 10 different seeds \n",
    "outer_final_df = None\n",
    "outer_metrics_df = None\n",
    "\n",
    "for seed in [12,13,14,15,16,17,18,19,20,22]:\n",
    "\n",
    "    # here we concatanate all the dataframes that are generated per seed (with all the different combinations)\n",
    "    # set the umap function for the embeddings \n",
    "    umap_model = umap.UMAP(n_neighbors=20,\n",
    "                           min_dist=0.1,\n",
    "                           n_components=3,\n",
    "                           metric=\"euclidean\",\n",
    "                           random_state=seed)\n",
    "\n",
    "    seed = seed\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # here we store the metrics of the models for one seed \n",
    "    final_df = None\n",
    "    final_feature_df = None\n",
    "    model_list = []\n",
    "\n",
    "    # iterate over the combinations of the VAE layers \n",
    "    for i, tup in enumerate(comb_list):\n",
    "        hidden_dim, latent_dim, n = tup[0], tup[1], i\n",
    "        print(i, hidden_dim, latent_dim)\n",
    "\n",
    "        # Instantiate the model\n",
    "        model1 = v1.VAE(\n",
    "            n_features=54,\n",
    "            latent_dim=latent_dim,\n",
    "            hidden_layer=True,\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_activ=nn.Sigmoid(),\n",
    "        ).to(device)\n",
    "\n",
    "        # need to set the model name with the layers - usefull for creating its unique folder \n",
    "        model_name = f\"abms_iter_{n}_{hidden_dim}_{latent_dim}\"\n",
    "        model_list.append(model_name)\n",
    "        # the optimizer is in the train-val loop \n",
    "\n",
    "\n",
    "        ## Create a \"models\" folder and the specifics model's directory to save figures  \n",
    "\n",
    "        # create the models directory path \n",
    "        path_dir = os.getcwd() + \"\\\\models\"\n",
    "\n",
    "        # Check if the models directory exists, if not, create it\n",
    "        if not os.path.exists(path_dir):\n",
    "            os.makedirs(path_dir)\n",
    "            print(f\"Created directory: {path_dir}\")\n",
    "        else:\n",
    "            print(f\"Directory already exists: {path_dir}\")\n",
    "\n",
    "        # create a subdirectory for each model based on #number and name (the dims of layers)\n",
    "        model_path = os.path.join(path_dir,model_name)\n",
    "        if not os.path.exists(model_path):\n",
    "            os.makedirs(model_path)\n",
    "        model_path\n",
    "\n",
    "\n",
    "        ## Final Important part\n",
    "\n",
    "        # set all the parameters to variables because all functions depend on them\n",
    "        model = model1\n",
    "        loss_fun = cf.loss_fun_gauss\n",
    "        model_name=model_name\n",
    "        path=model_path\n",
    "        epoch = 200\n",
    "        learn_r = 0.005\n",
    "        freebits = 4\n",
    "        batch_size = 128\n",
    "        norm = 0\n",
    "\n",
    "        # the path where this model is going to be saved \n",
    "        print(f\"model path {path}\")\n",
    "\n",
    "        # run the training for the model\n",
    "        # Run the loop - see the parameters \n",
    "\n",
    "        batch_dict, epoch_dict,hyperparam_str = uf.train_val_loop_v3(\n",
    "        model = model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader, \n",
    "        loss_fun = loss_fun,\n",
    "        model_name=model_name,\n",
    "        model_path=path,\n",
    "        epoch = epoch,\n",
    "        patience = 7,\n",
    "        learn_r = learn_r,\n",
    "        freebits = freebits,\n",
    "        batch_size = batch_size,\n",
    "        norm = norm\n",
    "        )\n",
    "\n",
    "        # write the full model id\n",
    "        model_id = model_name + \"_\" + hyperparam_str\n",
    "        print(f\"Model: {model_id} has been trained\")\n",
    "\n",
    "\n",
    "        # next run the test set analysis for the eaxh model and get the results test_iter_dict and test_metrics \n",
    "        test_iter_dict, test_metrics = uf.test_set_analysis(\n",
    "        model=model,\n",
    "        test_loader=test_loader,\n",
    "        loss_fun = loss_fun,\n",
    "        freebits=freebits,\n",
    "        model_id=model_id\n",
    "    )\n",
    "        # create the the dataframe to store per model metrics \n",
    "        test_df = pd.DataFrame([test_metrics])\n",
    "        test_df[\"hidden_dim\"] = hidden_dim\n",
    "        test_df[\"latent_dim\"] = latent_dim\n",
    "        test_df[\"seed\"] = seed\n",
    "        \n",
    "        # extract latent variables from the whole dataset using the trained model\n",
    "        # latent list\n",
    "        latent_list = list()\n",
    "        with torch.inference_mode():\n",
    "            model.eval()\n",
    "            for batch, _, _ in whole_loader:\n",
    "                batch = batch.to(device)\n",
    "                latent, _ = model.get_latent_variables(batch, detach=True)\n",
    "                latent_list.append(latent.cpu().detach().numpy())\n",
    "            \n",
    "        latent_arr = np.vstack(latent_list)\n",
    "        latent_df = pd.DataFrame(latent_arr, index=data.index, columns=[f\"z{str(i)}\" for i in range(latent_dim)])\n",
    "\n",
    "        # after gettng the latent variables perform umap on them \n",
    "        umap_latent = umap_model.fit_transform(latent_df)\n",
    "        umap_df = pd.DataFrame(umap_latent, index=data.index, columns=[f\"dim1\",f\"dim2\",f\"dim3\"])\n",
    "\n",
    "        # create the classifier and use the latent dimension and the umap embeddings to the predictor - use df_pairs as ground truth, \n",
    "\n",
    "        # thing to remove after the iteration is finished umap_latent, latent_df, umap_dist \n",
    "        if final_df is None:\n",
    "            final_df = test_df\n",
    "        else:\n",
    "            final_df = pd.concat([final_df,test_df],axis=0)\n",
    "        \n",
    "        gc.collect()\n",
    "\n",
    "        # get the feature from the specific model parameters\n",
    "    \n",
    "        # proteins that exist withing ground truth databases\n",
    "        proteins_in_pairs = set(pairs_df.iloc[:, 0]).union(set(pairs_df.iloc[:, 1]))\n",
    "\n",
    "        # subset by what is in the ground truth pairs\n",
    "        latent_df_sub = latent_df.loc[latent_df.index.intersection(proteins_in_pairs)]\n",
    "        umap_df_sub = umap_df.loc[umap_df.index.intersection(proteins_in_pairs)]\n",
    "\n",
    "\n",
    "        # perform corralation analysis for latent and euclidean distance calculation for umap embeddings\n",
    "\n",
    "\n",
    "\n",
    "        corr_matrix = pd.DataFrame(np.corrcoef(latent_df_sub),\n",
    "                                index=latent_df_sub.index,\n",
    "                                columns=latent_df_sub.index)\n",
    "\n",
    "        # fix the matrices of each feature type \n",
    "\n",
    "        cor_feat = (corr_matrix\n",
    "                    .reset_index()\n",
    "                    .melt(id_vars=\"index\", var_name=\"Var2\", value_name=f\"cor_feature_{model_name}\")\n",
    "                    .rename(columns={\"index\":\"Var1\"})\n",
    "        )\n",
    "\n",
    "        ##\n",
    "        umap_dist = pd.DataFrame(pairwise_distances(umap_df_sub, metric=\"euclidean\"),\n",
    "                                index = umap_df_sub.index,\n",
    "                                columns=umap_df_sub.index)\n",
    "\n",
    "        umap_dist = umap_dist.reset_index().melt(id_vars=\"index\", var_name=\"Var2\", value_name=f\"euc_feature_{model_name}\").rename(columns={\"index\":\"Var1\"})\n",
    "\n",
    "\n",
    "\n",
    "        ## filter duplicates and self-correlations/distances, keep only Var1>Var2\n",
    "        cor_feat = cor_feat[cor_feat[\"Var1\"]!=cor_feat[\"Var2\"]]\n",
    "        cor_feat = cor_feat[cor_feat[\"Var1\"]>cor_feat[\"Var2\"]]\n",
    "\n",
    "        umap_dist = umap_dist[umap_dist[\"Var1\"]!=umap_dist[\"Var2\"]]\n",
    "        umap_dist = umap_dist[umap_dist[\"Var1\"]>umap_dist[\"Var2\"]]\n",
    "\n",
    "\n",
    "        ## get a true protein pairs character vector\n",
    "        pair_chars = pairs_df[\"V1\"].astype(str) + \"_\" + pairs_df[\"V2\"].astype(str)\n",
    "\n",
    "        ## get ground truth classes of each feature category\n",
    "        cor_feat[\"db\"] = np.where((cor_feat[\"Var1\"].astype(str) + \"_\" + cor_feat[\"Var2\"].astype(str)).isin(pair_chars), 1, 0)\n",
    "        umap_dist[\"db\"] = np.where((umap_dist[\"Var1\"].astype(str) + \"_\" + umap_dist[\"Var2\"].astype(str)).isin(pair_chars), 1, 0)\n",
    "\n",
    "        # merge both dataframes using reduce and lambda function\n",
    "        feature_df = reduce(lambda left, right: pd.merge(left, right, on=[\"Var1\", \"Var2\", \"db\"], how=\"inner\"), [cor_feat, umap_dist])\n",
    "\n",
    "        if final_feature_df is None:\n",
    "            final_feature_df = feature_df\n",
    "        else:\n",
    "            final_feature_df = reduce(lambda left, right: pd.merge(left, right, on=[\"Var1\", \"Var2\", \"db\"], how=\"inner\"), [final_feature_df, feature_df])\n",
    "        \n",
    "        del cor_feat, umap_dist, pair_chars, corr_matrix, latent_df_sub, umap_df_sub, umap_latent, feature_df\n",
    "        \n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    # out of the first loop \n",
    "\n",
    "\n",
    "    # create cor features from the base \n",
    "    proteins_in_pairs = set(pairs_df.iloc[:, 0]).union(set(pairs_df.iloc[:, 1]))\n",
    "\n",
    "    data_sub = data.loc[data.index.intersection(proteins_in_pairs)]\n",
    "    corr_matrix_base = data_sub.T.corr(method=\"pearson\")\n",
    "\n",
    "    cor_feat_base = (corr_matrix_base\n",
    "                .reset_index()\n",
    "                .melt(id_vars=\"index\", var_name=\"Var2\", value_name=f\"cor_base\")\n",
    "                .rename(columns={\"index\":\"Var1\"})\n",
    "    )\n",
    "    cor_feat_base = cor_feat_base[cor_feat_base[\"Var1\"]!=cor_feat_base[\"Var2\"]]\n",
    "    cor_feat_base = cor_feat_base[cor_feat_base[\"Var1\"]>cor_feat_base[\"Var2\"]]\n",
    "\n",
    "    pair_chars = pairs_df[\"V1\"].astype(str) + \"_\" + pairs_df[\"V2\"].astype(str)\n",
    "    cor_feat_base[\"db\"] = np.where((cor_feat_base[\"Var1\"].astype(str) + \"_\" + cor_feat_base[\"Var2\"].astype(str)).isin(pair_chars), 1, 0) \n",
    "\n",
    "    # add the base cor-predictor to the feature list as well as to the model list\n",
    "    model_list.append(\"cor_base\")\n",
    "    final_feature_df= final_feature_df.merge(cor_feat_base, on=[\"Var1\",\"Var2\",\"db\"], how=\"inner\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ###### from this part and so on feature extraction for all combinations ######\n",
    "    metrics_df = None\n",
    "    # all combinations should be based on the same sampling procedure for more accurate comparisons \n",
    "\n",
    "    # begin with the L2 regression \n",
    "    pos_df = final_feature_df[final_feature_df['db'] == 1]\n",
    "    neg_df = final_feature_df[final_feature_df['db'] == 0]\n",
    "\n",
    "    # sample the negatives with randomization and create final dataset df\n",
    "    neg_df = neg_df.sample(n=pos_df.shape[0], random_state=seed)\n",
    "    sample_df = pd.concat([pos_df,neg_df]).sample(frac=1, random_state=seed)\n",
    "    del pos_df, neg_df\n",
    "\n",
    "    X = sample_df.drop([\"Var1\", \"Var2\", \"db\"], axis=1) \n",
    "    # umapX = sample_df[[\"euc_feature\"]]\n",
    "    y = sample_df[\"db\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "    classifier_pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"logreg\", LogisticRegression(penalty=\"l2\", solver=\"liblinear\", random_state=seed))\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "        'logreg__C': [0.01]\n",
    "    }\n",
    "\n",
    "    for model_n in model_list:\n",
    "        print(model_n)\n",
    "        if model_n != \"cor_base\":\n",
    "\n",
    "            # base_cor_grid = GridSearchCV(classifier_pipe, param_grid, cv=10, scoring='roc_auc', n_jobs=1)\n",
    "            cor_grid = GridSearchCV(classifier_pipe, param_grid, cv=10, scoring='roc_auc', n_jobs=1)\n",
    "            euc_grid = GridSearchCV(classifier_pipe, param_grid, cv=10, scoring='roc_auc', n_jobs=1)\n",
    "\n",
    "\n",
    "            # base_cor_grid.fit(X_train[[\"cor_base\"]], y_train)\n",
    "            cor_grid.fit(X_train[[f\"cor_feature_{model_n}\"]], y_train)\n",
    "            euc_grid.fit(X_train[[f\"euc_feature_{model_n}\"]], y_train)\n",
    "\n",
    "            # basecor_cv_auc = base_cor_grid.best_score_\n",
    "            cor_cv_auc = cor_grid.best_score_\n",
    "            euc_cv_auc = euc_grid.best_score_\n",
    "\n",
    "            # y_pred_basecor = base_cor_grid.predict_proba(X_test[[\"cor_base\"]])[:, 1]\n",
    "            y_pred_cor = cor_grid.predict_proba(X_test[[f\"cor_feature_{model_n}\"]])[:, 1]\n",
    "            y_pred_euc = euc_grid.predict_proba(X_test[[f\"euc_feature_{model_n}\"]])[:, 1]\n",
    "\n",
    "            # basecor_test_auc = roc_auc_score(y_test, y_pred_basecor)\n",
    "            cor_test_auc = roc_auc_score(y_test, y_pred_cor)\n",
    "            euc_test_auc = roc_auc_score(y_test, y_pred_euc)\n",
    "\n",
    "            # final_df[\"test_basecor\"] = round(float(basecor_test_auc),3)\n",
    "            res_df = pd.DataFrame({\n",
    "                    \"model\" : [model_n],\n",
    "                    \"test_cor\" : [round(float(cor_test_auc),3)],\n",
    "                    \"test_umap\" : [round(float(euc_test_auc),3)],\n",
    "                    \"seed\" : seed\n",
    "                })\n",
    "\n",
    "            if metrics_df is None:\n",
    "                metrics_df = res_df\n",
    "            else:\n",
    "                metrics_df = pd.concat([metrics_df,res_df],axis=0)\n",
    "            \n",
    "            gc.collect()\n",
    "        else:\n",
    "            cor_grid = GridSearchCV(classifier_pipe, param_grid, cv=10, scoring='roc_auc', n_jobs=1)\n",
    "            cor_grid.fit(X_train[[model_n]], y_train)\n",
    "            y_pred_cor = cor_grid.predict_proba(X_test[[model_n]])[:, 1]\n",
    "            cor_test_auc = roc_auc_score(y_test, y_pred_cor)\n",
    "            res_df = pd.DataFrame({\n",
    "                    \"model\" : [model_n],\n",
    "                    \"test_cor\" : [round(float(cor_test_auc),3)],\n",
    "                    \"test_umap\" : 0,\n",
    "                    \"seed\" : seed\n",
    "                })\n",
    "            \n",
    "            if metrics_df is None:\n",
    "                metrics_df = res_df\n",
    "            else:\n",
    "                metrics_df = pd.concat([metrics_df,res_df],axis=0)\n",
    "\n",
    "    del X_train, X_test, y_train, y_test, param_grid, classifier_pipe, res_df\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    if outer_final_df is None:\n",
    "        outer_final_df = final_df\n",
    "    else:\n",
    "        outer_final_df = pd.concat([outer_final_df, final_df], axis=0)\n",
    "\n",
    "    if outer_metrics_df is None:\n",
    "        outer_metrics_df = metrics_df\n",
    "    else:\n",
    "        outer_metrics_df = pd.concat([outer_metrics_df, metrics_df], axis=0)\n",
    "    # last step - make predictions for this batch of features\n",
    "\n",
    "\n",
    "outer_final_df.shape, outer_metrics_df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_metrics_df.to_csv(os.getcwd()+\"\\\\data\\\\processed\\\\abms_iter_minmax_metrics.csv\")\n",
    "outer_final_df.to_csv(os.getcwd()+\"\\\\data\\\\processed\\\\abms_iter_minmax_finaldf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>model_id</th>\n",
       "      <th>bits</th>\n",
       "      <th>avg_total_loss</th>\n",
       "      <th>avg_kl_loss</th>\n",
       "      <th>avg_rl_loss</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>latent_dim</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>abms_iter_0_54_30_ep83_norm0_bits4_bs128_lr0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>82.131075</td>\n",
       "      <td>83.178672</td>\n",
       "      <td>-1.047598</td>\n",
       "      <td>54</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>abms_iter_1_54_25_ep58_norm0_bits4_bs128_lr0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>68.286223</td>\n",
       "      <td>69.315650</td>\n",
       "      <td>-1.029426</td>\n",
       "      <td>54</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>abms_iter_0_54_30_ep65_norm0_bits4_bs128_lr0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>82.148430</td>\n",
       "      <td>83.179176</td>\n",
       "      <td>-1.030745</td>\n",
       "      <td>54</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>abms_iter_1_54_25_ep62_norm0_bits4_bs128_lr0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>68.286371</td>\n",
       "      <td>69.316413</td>\n",
       "      <td>-1.030042</td>\n",
       "      <td>54</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          model_id  bits  \\\n",
       "0           0  abms_iter_0_54_30_ep83_norm0_bits4_bs128_lr0.001     4   \n",
       "1           0  abms_iter_1_54_25_ep58_norm0_bits4_bs128_lr0.001     4   \n",
       "2           0  abms_iter_0_54_30_ep65_norm0_bits4_bs128_lr0.001     4   \n",
       "3           0  abms_iter_1_54_25_ep62_norm0_bits4_bs128_lr0.001     4   \n",
       "\n",
       "   avg_total_loss  avg_kl_loss  avg_rl_loss  hidden_dim  latent_dim  seed  \n",
       "0       82.131075    83.178672    -1.047598          54          30    12  \n",
       "1       68.286223    69.315650    -1.029426          54          25    12  \n",
       "2       82.148430    83.179176    -1.030745          54          30    13  \n",
       "3       68.286371    69.316413    -1.030042          54          25    13  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(os.getcwd()+\"\\\\data\\\\processed\\\\abms_iter_minmax_finaldf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data scale and split for VAE\n",
    "- We will perform min-max scaling to the TMT-Ratios of the proteomic SCBC data. <br>\n",
    "- We will scale the array version of our scbc data, the `npdata` matrix.\n",
    "- Then we will copy this scaled matrix and reshuffle the copy. The `npscbc_scaled_shuffled` will be used for the model training and performance evaluattion. <br>\n",
    "- The `npdata_scaled` matrix with the original order of rows will be used later for the validation of the latent variables. <br> \n",
    "- It is important to use the non-missing min and max values of dataset row-by-row <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create path and read the scbc data\n",
    "data_path = os.getcwd() + \"\\\\data\\\\processed\\\\\" \n",
    "data = pd.read_csv(data_path+\"protein_quant_merged.txt\",delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(104200)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to numpy \n",
    "npdata = data.to_numpy()\n",
    "np.isnan(npdata).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10439, 1) (10439, 1) 0 0\n"
     ]
    }
   ],
   "source": [
    "# Get extreme values (non-missing) frome ach row. \n",
    "data_min = np.nanmin(npdata, axis=1, keepdims=True)  # minimum among non-NaN\n",
    "data_max = np.nanmax(npdata, axis=1,keepdims=True)  # maximum among non-NaN\n",
    "\n",
    "# check that that shapes and values are as expected \n",
    "print(data_max.shape,data_min.shape,np.isnan(data_max).sum(), np.isnan(data_min).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10439, 130)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scale data \n",
    "npdata_scaled = (npdata - data_min) /(data_max - data_min + 1e-8)\n",
    "npdata_scaled.shape\n",
    "\n",
    "# npscbc_scaled[0:2,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the rows but keep scaled original\n",
    "npdata_scaled_shuffled = npdata_scaled.copy()\n",
    "np.random.shuffle(npdata_scaled_shuffled)\n",
    "# npscbc_scaled[1,],scbc.iloc[1,:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Split Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7829, 130), (1044, 130), (1566, 130))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, val_data, test_data = uf.create_data_partition(\n",
    "    npdata_scaled_shuffled, test_perc=0.15, val=True, val_perc=0.1\n",
    ")\n",
    "train_data.shape, val_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test reproducibility by re-runing the function and checking the data in the first index of the matrix. We expect it to be the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass data to Custom Dataset and DataLoaders \n",
    "- check that your data is numpy matrix.\n",
    "- check if data is scaled to (0,1).\n",
    "- create three custom dataset instances.\n",
    "- the custom dataset will save all the data to memory and create a mask where NaNs are located.\n",
    "- the numpy arrays will be converted to tensors of appropriate dimensions and NaNs to zeroes.\n",
    "- then we pass the custom dataset to the dataloader object.\n",
    "- The DataLoader object contains for each row (training example) i) a tensor of 1 x 130 columns with 0-1 scaled values, ii) a 1x130 mask indicating NA positions and iii) index of the examples per batch (could be 64, 128,..., batch_size). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protein Dataset is passed to memory\n",
      "No Protein Symbols were identified\n",
      "Protein Dataset is passed to memory\n",
      "No Protein Symbols were identified\n",
      "Protein Dataset is passed to memory\n",
      "No Protein Symbols were identified\n",
      "Protein Dataset is passed to memory\n",
      "No Protein Symbols were identified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = cd.ProteinDataset(train_data)\n",
    "val_dataset = cd.ProteinDataset(val_data)\n",
    "test_dataset = cd.ProteinDataset(test_data)\n",
    "whole_dataset = cd.ProteinDataset(npdata_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass data to the dataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False,drop_last=True)\n",
    "whole_loader = DataLoader(whole_dataset, batch_size=128, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE dimensions optimization Loop with secondary task \n",
    "It comprises the run of the training and validation set. VAE inherently have a tendency to overfit, so it is important to keep the test set after training loop. In this tutorial we run one model. The name is based on a simple numbering system and its layers to track it down. Furthermore the train_val_loop creates a hyperparameter string to track other parameters. The whole loop is parametrized in a function: <br>\n",
    "- The function starts with a pre-training evaluation to initialize metrics at epoch = 0 <br>\n",
    "- Then training of the model begins and after each epoch, the validation set is passed through the model to get the validation - epoch metrics.<br>\n",
    "\n",
    "\n",
    "During training, these are computed:\n",
    "- KL, Gaussian Logliklihood error, and Total Error are monitored per training batch, and also averaged every n batches.\n",
    "- KL, Gaussian Logliklihood error, and Total Error are monitored per validation round (per epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the combinations of the VAE layers to check. \n",
    "comb_list = list()\n",
    "for comb in list(itertools.product([90,75,65,50,40],[45,30,25,20,15,10])):\n",
    "    if comb[0] >= 1.5*comb[1]:\n",
    "        comb_list.append(comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(90, 45),\n",
       " (90, 30),\n",
       " (90, 25),\n",
       " (90, 20),\n",
       " (90, 15),\n",
       " (90, 10),\n",
       " (75, 45),\n",
       " (75, 30),\n",
       " (75, 25),\n",
       " (75, 20),\n",
       " (75, 15),\n",
       " (75, 10),\n",
       " (65, 30),\n",
       " (65, 25),\n",
       " (65, 20),\n",
       " (65, 15),\n",
       " (65, 10),\n",
       " (50, 30),\n",
       " (50, 25),\n",
       " (50, 20),\n",
       " (50, 15),\n",
       " (50, 10),\n",
       " (40, 25),\n",
       " (40, 20),\n",
       " (40, 15),\n",
       " (40, 10)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the umap function for the embeddings \n",
    "umap_model = umap.UMAP(n_neighbors=20,\n",
    "                           min_dist=0.1,\n",
    "                           n_components=3,\n",
    "                           metric=\"euclidean\",\n",
    "                           random_state=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>complex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EP300</td>\n",
       "      <td>CREBBP</td>\n",
       "      <td>Multisubunit ACTR coactivator complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KAT2B</td>\n",
       "      <td>EP300</td>\n",
       "      <td>Multisubunit ACTR coactivator complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NCOA3</td>\n",
       "      <td>EP300</td>\n",
       "      <td>Multisubunit ACTR coactivator complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KAT2B</td>\n",
       "      <td>CREBBP</td>\n",
       "      <td>Multisubunit ACTR coactivator complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NCOA3</td>\n",
       "      <td>KAT2B</td>\n",
       "      <td>Multisubunit ACTR coactivator complex</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      V1      V2                                complex\n",
       "1  EP300  CREBBP  Multisubunit ACTR coactivator complex\n",
       "2  KAT2B   EP300  Multisubunit ACTR coactivator complex\n",
       "3  NCOA3   EP300  Multisubunit ACTR coactivator complex\n",
       "4  KAT2B  CREBBP  Multisubunit ACTR coactivator complex\n",
       "5  NCOA3   KAT2B  Multisubunit ACTR coactivator complex"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load groudtruth pairs \n",
    "pairs_df = pd.read_csv(os.getcwd() + \"\\\\data\\\\processed\\\\\" + \"merged_pairs.txt\", delimiter=\"\\t\")\n",
    "pairs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 54 30\n",
      "Directory already exists: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\n",
      "model path c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\abms_iter_0_54_30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5ac3d1c1ec498090e17d1ed40c8ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing pre-training evaluation on the model in epoch 0\n",
      "\n",
      "Val loss: 84.131| Val KL: 83.17765045166016 | Val Rec: 0.953\n",
      "\n",
      "Patience exceeded at 60 with last checkpoint saved at 52\n",
      "changed learning rate to 0.001\n",
      "Early stopping at epoch 94 with last checkpoint saved at 83\n",
      "Model saved at: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\abms_iter_0_54_30\n",
      "Model: abms_iter_0_54_30_ep83_norm0_bits4_bs128_lr0.001 has been trained\n",
      "Using this model abms_iter_0_54_30_ep83_norm0_bits4_bs128_lr0.001\n",
      "The decoder output is transformed with an activation function, so reoconstructions are scaled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gpano\\anaconda3\\envs\\scbc\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 54 25\n",
      "Directory already exists: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\n",
      "model path c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\abms_iter_1_54_25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2aae5aa018b454fbb5682398a2b9909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing pre-training evaluation on the model in epoch 0\n",
      "\n",
      "Val loss: 70.273| Val KL: 69.31470489501953 | Val Rec: 0.958\n",
      "\n",
      "Patience exceeded at 54 with last checkpoint saved at 46\n",
      "changed learning rate to 0.001\n",
      "Early stopping at epoch 69 with last checkpoint saved at 58\n",
      "Model saved at: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\abms_iter_1_54_25\n",
      "Model: abms_iter_1_54_25_ep58_norm0_bits4_bs128_lr0.001 has been trained\n",
      "Using this model abms_iter_1_54_25_ep58_norm0_bits4_bs128_lr0.001\n",
      "The decoder output is transformed with an activation function, so reoconstructions are scaled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gpano\\anaconda3\\envs\\scbc\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 54 20\n",
      "abms_iter_0_54_30\n",
      "abms_iter_1_54_25\n",
      "cor_base\n",
      "0 54 30\n",
      "Directory already exists: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\n",
      "model path c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\abms_iter_0_54_30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149b884723a74f6bbd317f6465fc4cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing pre-training evaluation on the model in epoch 0\n",
      "\n",
      "Val loss: 84.129| Val KL: 83.17765045166016 | Val Rec: 0.952\n",
      "\n",
      "Patience exceeded at 56 with last checkpoint saved at 48\n",
      "changed learning rate to 0.001\n",
      "Early stopping at epoch 76 with last checkpoint saved at 65\n",
      "Model saved at: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\abms_iter_0_54_30\n",
      "Model: abms_iter_0_54_30_ep65_norm0_bits4_bs128_lr0.001 has been trained\n",
      "Using this model abms_iter_0_54_30_ep65_norm0_bits4_bs128_lr0.001\n",
      "The decoder output is transformed with an activation function, so reoconstructions are scaled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gpano\\anaconda3\\envs\\scbc\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 54 25\n",
      "Directory already exists: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\n",
      "model path c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\abms_iter_1_54_25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c272e4ef027427cbbf2eac46072c6d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing pre-training evaluation on the model in epoch 0\n",
      "\n",
      "Val loss: 70.277| Val KL: 69.31470489501953 | Val Rec: 0.962\n",
      "\n",
      "Patience exceeded at 52 with last checkpoint saved at 44\n",
      "changed learning rate to 0.001\n",
      "Early stopping at epoch 73 with last checkpoint saved at 62\n",
      "Model saved at: c:\\Users\\gpano\\Desktop\\github_py\\proteomics_latent_space\\models\\abms_iter_1_54_25\n",
      "Model: abms_iter_1_54_25_ep62_norm0_bits4_bs128_lr0.001 has been trained\n",
      "Using this model abms_iter_1_54_25_ep62_norm0_bits4_bs128_lr0.001\n",
      "The decoder output is transformed with an activation function, so reoconstructions are scaled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gpano\\anaconda3\\envs\\scbc\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 54 20\n",
      "abms_iter_0_54_30\n",
      "abms_iter_1_54_25\n",
      "cor_base\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((4, 8), (6, 4))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dimlist = [(54,30)]\n",
    "# future for loop for at least 10 different seeds \n",
    "outer_final_df = None\n",
    "outer_metrics_df = None\n",
    "\n",
    "for seed in [22,23,24,25,26,27,28,29,30,31]:\n",
    "\n",
    "    # here we concatanate all the dataframes that are generated per seed (with all the different combinations)\n",
    "\n",
    "    seed = seed\n",
    "    torch.manual_seed(seed)\n",
    "    umap_model = umap.UMAP(n_neighbors=20,\n",
    "                           min_dist=0.1,\n",
    "                           n_components=3,\n",
    "                           metric=\"euclidean\",\n",
    "                           random_state=seed)\n",
    "\n",
    "\n",
    "    # here we store the metrics of the models for one seed \n",
    "    final_df = None\n",
    "    final_feature_df = None\n",
    "    model_list = []\n",
    "\n",
    "    # iterate over the combinations of the VAE layers \n",
    "    for i, tup in enumerate(comb_list):\n",
    "        hidden_dim, latent_dim, n = tup[0], tup[1], i\n",
    "        print(i, hidden_dim, latent_dim)\n",
    "\n",
    "        # Instantiate the model\n",
    "        model1 = v1.VAE(\n",
    "            n_features=130,\n",
    "            latent_dim=latent_dim,\n",
    "            hidden_layer=True,\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_activ=nn.Sigmoid(),\n",
    "        ).to(device)\n",
    "\n",
    "        # need to set the model name with the layers - usefull for creating its unique folder \n",
    "        model_name = f\"abms_iter_{n}_{hidden_dim}_{latent_dim}\"\n",
    "        model_list.append(model_name)\n",
    "        # the optimizer is in the train-val loop \n",
    "\n",
    "\n",
    "        ## Create a \"models\" folder and the specifics model's directory to save figures  \n",
    "\n",
    "        # create the models directory path \n",
    "        path_dir = os.getcwd() + \"\\\\models\"\n",
    "\n",
    "        # Check if the models directory exists, if not, create it\n",
    "        if not os.path.exists(path_dir):\n",
    "            os.makedirs(path_dir)\n",
    "            print(f\"Created directory: {path_dir}\")\n",
    "        else:\n",
    "            print(f\"Directory already exists: {path_dir}\")\n",
    "\n",
    "        # create a subdirectory for each model based on #number and name (the dims of layers)\n",
    "        model_path = os.path.join(path_dir,model_name)\n",
    "        if not os.path.exists(model_path):\n",
    "            os.makedirs(model_path)\n",
    "        model_path\n",
    "\n",
    "\n",
    "        ## Final Important part\n",
    "\n",
    "        # set all the parameters to variables because all functions depend on them\n",
    "        model = model1\n",
    "        loss_fun = cf.loss_fun_gauss\n",
    "        model_name=model_name\n",
    "        path=model_path\n",
    "        epoch = 200\n",
    "        learn_r = 0.005\n",
    "        freebits = 4\n",
    "        batch_size = 128\n",
    "        norm = 0\n",
    "\n",
    "        # the path where this model is going to be saved \n",
    "        print(f\"model path {path}\")\n",
    "\n",
    "        # run the training for the model\n",
    "        # Run the loop - see the parameters \n",
    "\n",
    "        batch_dict, epoch_dict,hyperparam_str = uf.train_val_loop_v3(\n",
    "        model = model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader, \n",
    "        loss_fun = loss_fun,\n",
    "        model_name=model_name,\n",
    "        model_path=path,\n",
    "        epoch = epoch,\n",
    "        patience = 7,\n",
    "        learn_r = learn_r,\n",
    "        freebits = freebits,\n",
    "        batch_size = batch_size,\n",
    "        norm = norm\n",
    "        )\n",
    "\n",
    "        # write the full model id\n",
    "        model_id = model_name + \"_\" + hyperparam_str\n",
    "        print(f\"Model: {model_id} has been trained\")\n",
    "\n",
    "\n",
    "        # next run the test set analysis for the eaxh model and get the results test_iter_dict and test_metrics \n",
    "        test_iter_dict, test_metrics = uf.test_set_analysis(\n",
    "        model=model,\n",
    "        test_loader=test_loader,\n",
    "        loss_fun = loss_fun,\n",
    "        freebits=freebits,\n",
    "        model_id=model_id\n",
    "    )\n",
    "        # create the the dataframe to store per model metrics \n",
    "        test_df = pd.DataFrame([test_metrics])\n",
    "        test_df[\"hidden_dim\"] = hidden_dim\n",
    "        test_df[\"latent_dim\"] = latent_dim\n",
    "        test_df[\"seed\"] = seed\n",
    "        \n",
    "        # extract latent variables from the whole dataset using the trained model\n",
    "        # latent list\n",
    "        latent_list = list()\n",
    "        with torch.inference_mode():\n",
    "            model.eval()\n",
    "            for batch, _, _ in whole_loader:\n",
    "                batch = batch.to(device)\n",
    "                latent, _ = model.get_latent_variables(batch, detach=True)\n",
    "                latent_list.append(latent.cpu().detach().numpy())\n",
    "            \n",
    "        latent_arr = np.vstack(latent_list)\n",
    "        latent_df = pd.DataFrame(latent_arr, index=data.index, columns=[f\"z{str(i)}\" for i in range(latent_dim)])\n",
    "\n",
    "        # after gettng the latent variables perform umap on them \n",
    "        umap_latent = umap_model.fit_transform(latent_df)\n",
    "        umap_df = pd.DataFrame(umap_latent, index=data.index, columns=[f\"dim1\",f\"dim2\",f\"dim3\"])\n",
    "\n",
    "        # create the classifier and use the latent dimension and the umap embeddings to the predictor - use df_pairs as ground truth, \n",
    "\n",
    "        # thing to remove after the iteration is finished umap_latent, latent_df, umap_dist \n",
    "        if final_df is None:\n",
    "            final_df = test_df\n",
    "        else:\n",
    "            final_df = pd.concat([final_df,test_df],axis=0)\n",
    "        \n",
    "        gc.collect()\n",
    "\n",
    "        # get the feature from the specific model parameters\n",
    "    \n",
    "        # proteins that exist withing ground truth databases\n",
    "        proteins_in_pairs = set(pairs_df.iloc[:, 0]).union(set(pairs_df.iloc[:, 1]))\n",
    "\n",
    "        # subset by what is in the ground truth pairs\n",
    "        latent_df_sub = latent_df.loc[latent_df.index.intersection(proteins_in_pairs)]\n",
    "        umap_df_sub = umap_df.loc[umap_df.index.intersection(proteins_in_pairs)]\n",
    "\n",
    "\n",
    "        # perform corralation analysis for latent and euclidean distance calculation for umap embeddings\n",
    "\n",
    "\n",
    "\n",
    "        corr_matrix = pd.DataFrame(np.corrcoef(latent_df_sub),\n",
    "                                index=latent_df_sub.index,\n",
    "                                columns=latent_df_sub.index)\n",
    "\n",
    "        # fix the matrices of each feature type \n",
    "\n",
    "        cor_feat = (corr_matrix\n",
    "                    .reset_index()\n",
    "                    .melt(id_vars=\"index\", var_name=\"Var2\", value_name=f\"cor_feature_{model_name}\")\n",
    "                    .rename(columns={\"index\":\"Var1\"})\n",
    "        )\n",
    "\n",
    "        ##\n",
    "        umap_dist = pd.DataFrame(pairwise_distances(umap_df_sub, metric=\"euclidean\"),\n",
    "                                index = umap_df_sub.index,\n",
    "                                columns=umap_df_sub.index)\n",
    "\n",
    "        umap_dist = umap_dist.reset_index().melt(id_vars=\"index\", var_name=\"Var2\", value_name=f\"euc_feature_{model_name}\").rename(columns={\"index\":\"Var1\"})\n",
    "\n",
    "\n",
    "\n",
    "        ## filter duplicates and self-correlations/distances, keep only Var1>Var2\n",
    "        cor_feat = cor_feat[cor_feat[\"Var1\"]!=cor_feat[\"Var2\"]]\n",
    "        cor_feat = cor_feat[cor_feat[\"Var1\"]>cor_feat[\"Var2\"]]\n",
    "\n",
    "        umap_dist = umap_dist[umap_dist[\"Var1\"]!=umap_dist[\"Var2\"]]\n",
    "        umap_dist = umap_dist[umap_dist[\"Var1\"]>umap_dist[\"Var2\"]]\n",
    "\n",
    "\n",
    "        ## get a true protein pairs character vector\n",
    "        pair_chars = pairs_df[\"V1\"].astype(str) + \"_\" + pairs_df[\"V2\"].astype(str)\n",
    "\n",
    "        ## get ground truth classes of each feature category\n",
    "        cor_feat[\"db\"] = np.where((cor_feat[\"Var1\"].astype(str) + \"_\" + cor_feat[\"Var2\"].astype(str)).isin(pair_chars), 1, 0)\n",
    "        umap_dist[\"db\"] = np.where((umap_dist[\"Var1\"].astype(str) + \"_\" + umap_dist[\"Var2\"].astype(str)).isin(pair_chars), 1, 0)\n",
    "\n",
    "        # merge both dataframes using reduce and lambda function\n",
    "        feature_df = reduce(lambda left, right: pd.merge(left, right, on=[\"Var1\", \"Var2\", \"db\"], how=\"inner\"), [cor_feat, umap_dist])\n",
    "\n",
    "        if final_feature_df is None:\n",
    "            final_feature_df = feature_df\n",
    "        else:\n",
    "            final_feature_df = reduce(lambda left, right: pd.merge(left, right, on=[\"Var1\", \"Var2\", \"db\"], how=\"inner\"), [final_feature_df, feature_df])\n",
    "        \n",
    "        del cor_feat, umap_dist, pair_chars, corr_matrix, latent_df_sub, umap_df_sub, umap_latent, feature_df\n",
    "        \n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    # out of the first loop \n",
    "\n",
    "\n",
    "    # create cor features from the base \n",
    "    proteins_in_pairs = set(pairs_df.iloc[:, 0]).union(set(pairs_df.iloc[:, 1]))\n",
    "\n",
    "    data_sub = data.loc[data.index.intersection(proteins_in_pairs)]\n",
    "    corr_matrix_base = data_sub.T.corr(method=\"pearson\")\n",
    "\n",
    "    cor_feat_base = (corr_matrix_base\n",
    "                .reset_index()\n",
    "                .melt(id_vars=\"index\", var_name=\"Var2\", value_name=f\"cor_base\")\n",
    "                .rename(columns={\"index\":\"Var1\"})\n",
    "    )\n",
    "    cor_feat_base = cor_feat_base[cor_feat_base[\"Var1\"]!=cor_feat_base[\"Var2\"]]\n",
    "    cor_feat_base = cor_feat_base[cor_feat_base[\"Var1\"]>cor_feat_base[\"Var2\"]]\n",
    "\n",
    "    pair_chars = pairs_df[\"V1\"].astype(str) + \"_\" + pairs_df[\"V2\"].astype(str)\n",
    "    cor_feat_base[\"db\"] = np.where((cor_feat_base[\"Var1\"].astype(str) + \"_\" + cor_feat_base[\"Var2\"].astype(str)).isin(pair_chars), 1, 0) \n",
    "\n",
    "    # add the base cor-predictor to the feature list as well as to the model list\n",
    "    model_list.append(\"cor_base\")\n",
    "    final_feature_df= final_feature_df.merge(cor_feat_base, on=[\"Var1\",\"Var2\",\"db\"], how=\"inner\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ###### from this part and so on feature extraction for all combinations ######\n",
    "    metrics_df = None\n",
    "    # all combinations should be based on the same sampling procedure for more accurate comparisons \n",
    "\n",
    "    # begin with the L2 regression \n",
    "    pos_df = final_feature_df[final_feature_df['db'] == 1]\n",
    "    neg_df = final_feature_df[final_feature_df['db'] == 0]\n",
    "\n",
    "    # sample the negatives with randomization and create final dataset df\n",
    "    neg_df = neg_df.sample(n=pos_df.shape[0], random_state=seed)\n",
    "    sample_df = pd.concat([pos_df,neg_df]).sample(frac=1, random_state=seed)\n",
    "    del pos_df, neg_df\n",
    "\n",
    "    X = sample_df.drop([\"Var1\", \"Var2\", \"db\"], axis=1) \n",
    "    # umapX = sample_df[[\"euc_feature\"]]\n",
    "    y = sample_df[\"db\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "    classifier_pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"logreg\", LogisticRegression(penalty=\"l2\", solver=\"liblinear\", random_state=seed))\n",
    "    ])\n",
    "\n",
    "    param_grid = {\n",
    "        'logreg__C': [0.01]\n",
    "    }\n",
    "\n",
    "    for model_n in model_list:\n",
    "        print(model_n)\n",
    "        if model_n != \"cor_base\":\n",
    "\n",
    "            # base_cor_grid = GridSearchCV(classifier_pipe, param_grid, cv=10, scoring='roc_auc', n_jobs=1)\n",
    "            cor_grid = GridSearchCV(classifier_pipe, param_grid, cv=10, scoring='roc_auc', n_jobs=1)\n",
    "            euc_grid = GridSearchCV(classifier_pipe, param_grid, cv=10, scoring='roc_auc', n_jobs=1)\n",
    "\n",
    "\n",
    "            # base_cor_grid.fit(X_train[[\"cor_base\"]], y_train)\n",
    "            cor_grid.fit(X_train[[f\"cor_feature_{model_n}\"]], y_train)\n",
    "            euc_grid.fit(X_train[[f\"euc_feature_{model_n}\"]], y_train)\n",
    "\n",
    "            # basecor_cv_auc = base_cor_grid.best_score_\n",
    "            cor_cv_auc = cor_grid.best_score_\n",
    "            euc_cv_auc = euc_grid.best_score_\n",
    "\n",
    "            # y_pred_basecor = base_cor_grid.predict_proba(X_test[[\"cor_base\"]])[:, 1]\n",
    "            y_pred_cor = cor_grid.predict_proba(X_test[[f\"cor_feature_{model_n}\"]])[:, 1]\n",
    "            y_pred_euc = euc_grid.predict_proba(X_test[[f\"euc_feature_{model_n}\"]])[:, 1]\n",
    "\n",
    "            # basecor_test_auc = roc_auc_score(y_test, y_pred_basecor)\n",
    "            cor_test_auc = roc_auc_score(y_test, y_pred_cor)\n",
    "            euc_test_auc = roc_auc_score(y_test, y_pred_euc)\n",
    "\n",
    "            # final_df[\"test_basecor\"] = round(float(basecor_test_auc),3)\n",
    "            res_df = pd.DataFrame({\n",
    "                    \"model\" : [model_n],\n",
    "                    \"test_cor\" : [round(float(cor_test_auc),3)],\n",
    "                    \"test_umap\" : [round(float(euc_test_auc),3)],\n",
    "                    \"seed\" : seed\n",
    "                })\n",
    "\n",
    "            if metrics_df is None:\n",
    "                metrics_df = res_df\n",
    "            else:\n",
    "                metrics_df = pd.concat([metrics_df,res_df],axis=0)\n",
    "            \n",
    "            gc.collect()\n",
    "        else:\n",
    "            cor_grid = GridSearchCV(classifier_pipe, param_grid, cv=10, scoring='roc_auc', n_jobs=1)\n",
    "            cor_grid.fit(X_train[[model_n]], y_train)\n",
    "            y_pred_cor = cor_grid.predict_proba(X_test[[model_n]])[:, 1]\n",
    "            cor_test_auc = roc_auc_score(y_test, y_pred_cor)\n",
    "            res_df = pd.DataFrame({\n",
    "                    \"model\" : [model_n],\n",
    "                    \"test_cor\" : [round(float(cor_test_auc),3)],\n",
    "                    \"test_umap\" : 0,\n",
    "                    \"seed\" : seed\n",
    "                })\n",
    "            \n",
    "            if metrics_df is None:\n",
    "                metrics_df = res_df\n",
    "            else:\n",
    "                metrics_df = pd.concat([metrics_df,res_df],axis=0)\n",
    "\n",
    "    del X_train, X_test, y_train, y_test, param_grid, classifier_pipe, res_df\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    if outer_final_df is None:\n",
    "        outer_final_df = final_df\n",
    "    else:\n",
    "        outer_final_df = pd.concat([outer_final_df, final_df], axis=0)\n",
    "\n",
    "    if outer_metrics_df is None:\n",
    "        outer_metrics_df = metrics_df\n",
    "    else:\n",
    "        outer_metrics_df = pd.concat([outer_metrics_df, metrics_df], axis=0)\n",
    "    # last step - make predictions for this batch of features\n",
    "\n",
    "\n",
    "outer_final_df.shape, outer_metrics_df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_metrics_df.to_csv(os.getcwd()+\"\\\\data\\\\processed\\\\scbc_iter_minmax_metrics.csv\")\n",
    "outer_final_df.to_csv(os.getcwd()+\"\\\\data\\\\processed\\\\scbc_iter_minmax_finaldf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>model_id</th>\n",
       "      <th>bits</th>\n",
       "      <th>avg_total_loss</th>\n",
       "      <th>avg_kl_loss</th>\n",
       "      <th>avg_rl_loss</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>latent_dim</th>\n",
       "      <th>seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>abms_iter_0_54_30_ep83_norm0_bits4_bs128_lr0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>82.131075</td>\n",
       "      <td>83.178672</td>\n",
       "      <td>-1.047598</td>\n",
       "      <td>54</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>abms_iter_1_54_25_ep58_norm0_bits4_bs128_lr0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>68.286223</td>\n",
       "      <td>69.315650</td>\n",
       "      <td>-1.029426</td>\n",
       "      <td>54</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>abms_iter_0_54_30_ep65_norm0_bits4_bs128_lr0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>82.148430</td>\n",
       "      <td>83.179176</td>\n",
       "      <td>-1.030745</td>\n",
       "      <td>54</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>abms_iter_1_54_25_ep62_norm0_bits4_bs128_lr0.001</td>\n",
       "      <td>4</td>\n",
       "      <td>68.286371</td>\n",
       "      <td>69.316413</td>\n",
       "      <td>-1.030042</td>\n",
       "      <td>54</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          model_id  bits  \\\n",
       "0           0  abms_iter_0_54_30_ep83_norm0_bits4_bs128_lr0.001     4   \n",
       "1           0  abms_iter_1_54_25_ep58_norm0_bits4_bs128_lr0.001     4   \n",
       "2           0  abms_iter_0_54_30_ep65_norm0_bits4_bs128_lr0.001     4   \n",
       "3           0  abms_iter_1_54_25_ep62_norm0_bits4_bs128_lr0.001     4   \n",
       "\n",
       "   avg_total_loss  avg_kl_loss  avg_rl_loss  hidden_dim  latent_dim  seed  \n",
       "0       82.131075    83.178672    -1.047598          54          30    12  \n",
       "1       68.286223    69.315650    -1.029426          54          25    12  \n",
       "2       82.148430    83.179176    -1.030745          54          30    13  \n",
       "3       68.286371    69.316413    -1.030042          54          25    13  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scbc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
