{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries and import modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy.random as nrd\n",
    "import os \n",
    "import sys\n",
    "\n",
    "# Pytorch modules \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# this for the custom Dataset \n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "# Import tqdm for progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# for timing functions\n",
    "from timeit import default_timer as timer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Project Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\gpano\\\\Desktop\\\\github_py\\\\proteomics_latent_space'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check your current directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** Run the configuration file first `configs.py`. Importing this script and setting the seed and device parameters before importing any of the other modules ensures that evereything is sync.\n",
    "\n",
    "**Important** If you want *change the configuration parameters*, change them before importing and running the pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing models_util.configs module\n",
      "First set device and seed for reproducibility.\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from models_util import configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seed: None, Device: None'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs.get_configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n"
     ]
    }
   ],
   "source": [
    "# print the global variables\n",
    "print(configs.project_seed, configs.project_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During configuration random seed 888 has been set.\n",
      "888 cpu\n"
     ]
    }
   ],
   "source": [
    "configs.set_seed(888)\n",
    "device = configs.set_device(force_cpu=True)\n",
    "\n",
    "# global variables have changed too\n",
    "print(configs.project_seed, configs.project_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seed: 888, Device: cpu'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see if the get function also agrees:\n",
    "configs.get_configs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the configurations values are assigned globally, we can import the modules. If this is working, we expect each module to access the **same** **seed** and **device** we set. We are also expecting generated numbers **inside the modules** to be reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During configuration random seed 888 has been set.\n",
      "Importing models_util.utility_functions, running in cpu with seed: 888\n"
     ]
    }
   ],
   "source": [
    "# Load home modules and check the device where they are running \n",
    "from models_util import utility_functions as uf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During configuration random seed 888 has been set.\n",
      "Importing models_util.custom_dataset, running in cpu with seed: 888\n"
     ]
    }
   ],
   "source": [
    "from models_util import custom_dataset as cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During configuration random seed 888 has been set.\n",
      "Importing models_util.cost_functions, running in cpu with seed: 888\n"
     ]
    }
   ],
   "source": [
    "from models_util import cost_functions as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During configuration random seed 888 has been set.\n",
      "Importing models_util.VAE1, running in cpu with seed: 888\n"
     ]
    }
   ],
   "source": [
    "from models_util import VAE1 as v1 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCBC Data scale and split for VAE\n",
    "- We will perform min-max scaling to the TMT-Ratios of the proteomic SCBC data. <br>\n",
    "- It is important to use the non-missing min and max values of dataset row-by-row <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create path and read the scbc data\n",
    "data_path = os.getcwd() + \"\\\\data\\\\processed\\\\\" \n",
    "scbc = pd.read_csv(data_path+\"protein_quant_merged.txt\",delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(104200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to numpy \n",
    "npscbc = scbc.to_numpy()\n",
    "np.isnan(npscbc).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10439, 1) (10439, 1) 0 0\n"
     ]
    }
   ],
   "source": [
    "# Get extreme values (non-missing) frome ach row. \n",
    "scbc_min = np.nanmin(npscbc, axis=1, keepdims=True)  # minimum among non-NaN\n",
    "scbc_max = np.nanmax(npscbc, axis=1,keepdims=True)  # maximum among non-NaN\n",
    "\n",
    "# check that that shapes and values are as expected \n",
    "print(scbc_max.shape,scbc_min.shape,np.isnan(scbc_max).sum(), np.isnan(scbc_min).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10439, 130)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scale data \n",
    "npscbc_scaled = (npscbc - scbc_min) /(scbc_max - scbc_min + 1e-8)\n",
    "npscbc_scaled.shape\n",
    "\n",
    "# npscbc_scaled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10439, 130)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle the rows \n",
    "np.random.shuffle(npscbc_scaled)\n",
    "npscbc_scaled.shape\n",
    "# npscbc_scaled[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7829, 130), (1044, 130), (1566, 130))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, val_data, test_data = uf.create_data_partition(\n",
    "    npscbc_scaled, test_perc=0.15, val=True, val_perc=0.1\n",
    ")\n",
    "train_data.shape, val_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test reproducibility by re-runing the function and checking the data in the first index of the matrix. We expect it to be the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass data to Custom Dataset and DataLoaders \n",
    "- check that your data is numpy matrix.\n",
    "- check if data is scaled to (0,1).\n",
    "- create three custom dataset instances.\n",
    "- the custom dataset will save all the data to memory and create a mask where NaNs are located.\n",
    "- the numpy arrays will be converted to tensors of appropriate dimensions and NaNs to zeroes.\n",
    "- then we pass the custom dataset to the dataloader object.\n",
    "- The DataLoader object contains for each row (training example) i) a tensor of 1 x 130 columns with 0-1 scaled values, ii) a 1x130 mask indicating NA positions and iii) index of the examples per batch (could be 64, 128,..., batch_size). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protein Dataset is passed to memory\n",
      "No Protein Symbols were identified\n",
      "Protein Dataset is passed to memory\n",
      "No Protein Symbols were identified\n",
      "Protein Dataset is passed to memory\n",
      "No Protein Symbols were identified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = cd.ProteinDataset(train_data)\n",
    "val_dataset = cd.ProteinDataset(val_data)\n",
    "test_dataset = cd.ProteinDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass data to the dataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.1777, 0.1589, 0.3951,  ..., 0.8203, 0.7897, 0.7998],\n",
       "         [0.0389, 0.1591, 0.4674,  ..., 0.3939, 0.5666, 0.6010],\n",
       "         [0.2107, 0.1048, 0.7604,  ..., 0.1447, 0.1145, 0.1311],\n",
       "         ...,\n",
       "         [0.7318, 0.7253, 0.5029,  ..., 0.1658, 0.1598, 0.1481],\n",
       "         [0.1812, 0.2756, 0.0876,  ..., 0.7633, 0.5545, 0.6024],\n",
       "         [0.6118, 0.6419, 0.4321,  ..., 0.0808, 0.1044, 0.0679]]),\n",
       " tensor([[False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         ...,\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False],\n",
       "         [False, False, False,  ..., False, False, False]]),\n",
       " tensor([ 625, 1896, 1534, 3794, 4551,  726, 4525, 5994, 2348,  912, 4147, 1732,\n",
       "         2168,   19, 5119, 2514, 3839, 1346,  145, 7515, 3907, 1813, 6666, 3275,\n",
       "         2968, 3535, 4981, 7389, 7084, 3903, 3380, 5775, 7344, 5068,   21, 6623,\n",
       "         7162, 4505, 4901,   49, 4047, 7059, 6684, 3800, 6624, 3108,  932, 6551,\n",
       "         6136, 7543, 5931, 4511, 3534,  987, 2596, 1678, 2502, 2359, 7289, 3056,\n",
       "         6204, 6105, 6509, 6313, 7140, 1626, 2499, 5347, 2934, 3438,  773, 2177,\n",
       "         4030, 5846, 1112, 2552, 3743, 3341, 4163,  985, 2839, 4133, 2911, 2311,\n",
       "         6085, 7714, 7385, 7072, 3786, 4628, 5524, 6964, 7488, 4445, 6480, 1408,\n",
       "          871, 5847, 3130,  611, 6901, 6163, 5824, 4577, 3098,  805, 2644, 6279,\n",
       "         2069, 6469, 5509, 6081, 5716, 4090, 6442, 5071, 7033, 1955,  931, 7043,\n",
       "         3608, 1251, 2338, 7715, 3210, 7213, 4221, 1574], dtype=torch.int32)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the train loader is not reproducible bcs it shuffles but it is not seeded yet. \n",
    "# here is one batch of training examples \n",
    "# torch.manual_seed(888)\n",
    "\n",
    "\n",
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training-Validation Loop \n",
    "It comprises the analysis of the training and validation set. VAE inherently have a tendency to overfit, so it is important to keep the test set after training loop. The whole loop is parametrized in a function: <br>\n",
    "- The function starts with a pre-training evaluation to initialize metrics at epoch = 0 <br>\n",
    "- Then training of the model begins and after each epoch, the validation set is passed through the model to get the validation - epoch metrics.<br>\n",
    "\n",
    "\n",
    "During training, these are computed:\n",
    "- KL, Gaussian Logliklihood error, and Total Error are monitored per training batch, and also averaged every n batches.\n",
    "- KL, Gaussian Logliklihood error, and Total Error are monitored per validation round (per epoch).\n",
    "- Also average KL/dimension is monitored per epoch at the validation set (how information is distributed and whether there are inactive dimensions)\n",
    "- Plot average KL per dimension and Reconstruction error per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_features = 130\n",
    "# hidden_dim = 65\n",
    "# latent_dim = 25\n",
    "\n",
    "# Instantiate the model\n",
    "model1 = v1.VAE(\n",
    "    n_features=130,\n",
    "    latent_dim=25,\n",
    "    hidden_layer=True,\n",
    "    hidden_dim=65,\n",
    "    sigmoid=True\n",
    ").to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0025)\n",
    "\n",
    "# need to set the model name automatically \n",
    "model_name = \"model0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 50\n",
    "learn_r = 0.0025\n",
    "model = model1\n",
    "loss_fun = cf.loss_fun\n",
    "freebits = 0.5\n",
    "batch_size = 128\n",
    "norm = 2.5 # for gradient clipping - optional \n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learn_r)\n",
    "\n",
    "hyperparam_str = f\"norm{norm}_bits{freebits}_bs{batch_size}_lr{optimizer.param_groups[0][\"lr\"]}\"\n",
    "\n",
    "\n",
    "# Storage\n",
    "# for each batch/iteration\n",
    "batch_dict = {\n",
    "    \"iteration\": [],\n",
    "    \"Train total Loss\": [],\n",
    "    \"Train KL Loss\": [], \n",
    "    \"Train Rec Loss\": []\n",
    "    }\n",
    "\n",
    "# for each epoch\n",
    "epoch_dict = {\n",
    "    \"epoch\": [],\n",
    "    \"Train total Loss\": [],\n",
    "    \"Train KL Loss\": [], \n",
    "    \"Train Rec Loss\": [],\n",
    "    \"Val total Loss\": [],\n",
    "    \"Val KL Loss\": [],\n",
    "    \"Val Rec Loss\": []\n",
    "    }\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(epoch+1)):\n",
    "    \n",
    "    \n",
    "    # initialize the loss metrics at epoch zero\n",
    "    if epoch == 0:\n",
    "        print(f\"Performing pre-training evaluation on the model in epoch {epoch}\")\n",
    "        val_loss, val_kl, val_rl = 0,0,0\n",
    "        model.eval()\n",
    "        with torch.inference_mode(): # it doesnt update parameters \n",
    "            lst = []\n",
    "            for val_batch, t_mask, tidx in test_loader:\n",
    "                x_mu, x_logvar, z_mu, z_logvar = model(val_batch)\n",
    "                loss = loss_fun(val_batch, x_mu, x_logvar, z_mu, z_logvar,lst,mask=t_mask,freebits=freebits)\n",
    "                val_loss += loss.detach().item()\n",
    "                val_kl += lst[-1]\n",
    "                val_rl += lst[-2]\n",
    "            \n",
    "            val_loss = val_loss/len(test_loader)\n",
    "            val_kl = val_kl/len(test_loader)\n",
    "            val_rl = val_rl/len(test_loader)\n",
    "            \n",
    "            epoch_dict[\"epoch\"].append(epoch)\n",
    "            epoch_dict[\"Train total Loss\"].append(val_loss)\n",
    "            epoch_dict[\"Train KL Loss\"].append(val_kl)\n",
    "            epoch_dict[\"Train Rec Loss\"].append(val_rl)\n",
    "            epoch_dict[\"Val total Loss\"].append(val_loss)\n",
    "            epoch_dict[\"Val KL Loss\"].append(val_kl)\n",
    "            epoch_dict[\"Val Rec Loss\"].append(val_rl)\n",
    "        \n",
    "        print(f\"\\nVal loss: {val_loss:.3f}| Val KL: {val_kl} | Val Rec: {val_rl:.3f}\\n\")\n",
    "    \n",
    "    # begin training the model from iteration 0 and after epoch 0 \n",
    "    else:\n",
    "        print(f\"Epoch {epoch}\\n--------------------\")\n",
    "        train_loss, train_kl, train_rl = 0,0,0\n",
    "        lst = [] # this list stores the averaged losses/batch that are computed from the loss\n",
    "        iter = 0\t\t\t\n",
    "        for batch, (xbatch, xmask, xidx) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            # device\n",
    "            xbatch, xmask = xbatch.to(device), xmask.to(device)\n",
    "\n",
    "            #\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_mu, x_logvar, z_mu, z_logvar = model(xbatch)\n",
    "\n",
    "            loss = loss_fun(xbatch, x_mu, x_logvar, z_mu, z_logvar,lst,mask=xmask,freebits=freebits)\n",
    "            train_loss += loss.detach().item()\n",
    "            train_kl += lst[-1]\n",
    "            train_rl += lst[-2]\n",
    "\n",
    "            batch_loss = loss.detach().item()\n",
    "            batch_kl = lst[-1]\n",
    "            batch_rl = lst[-2]\n",
    "\n",
    "            loss.backward()\n",
    "                    \n",
    "            # Optional gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            # update the batch dictionary - no val since #iterations are not the same \n",
    "            batch_dict[\"iteration\"].append(iter)\n",
    "            batch_dict[\"Train total Loss\"].append(batch_loss)\n",
    "            batch_dict[\"Train KL Loss\"].append(batch_kl)\n",
    "            batch_dict[\"Train Rec Loss\"].append(batch_rl)\n",
    "\n",
    "            iter +=1\n",
    "\n",
    "            # print every round of 10 batches the losses - smooths the results \n",
    "            if batch % 10 == 0:\n",
    "                print(f\"Iter {batch} and a total {batch*batch_size}/{len(train_loader.dataset)} proteins have passed.\")\n",
    "                print(f\"Current Loss: {train_loss/(batch+1)} | KL Loss: {train_kl/(batch+1)}| Rec Loss: {train_rl/(batch+1)}\")\n",
    "\n",
    "\n",
    "        # calculate per epoch the metrics - divide by number of batches \n",
    "        train_loss = train_loss/len(train_loader)\n",
    "        train_kl = train_kl/len(train_loader)\n",
    "        train_rl = train_rl/len(train_loader)\n",
    "        \n",
    "        # add them to the dictionary \n",
    "        epoch_dict[\"epoch\"].append(epoch)\n",
    "        epoch_dict[\"Train total Loss\"].append(train_loss)\n",
    "        epoch_dict[\"Train KL Loss\"].append(train_kl)\n",
    "        epoch_dict[\"Train Rec Loss\"].append(train_rl)\n",
    "        \n",
    "\n",
    "        # pass the validation set to the VAE \n",
    "        val_loss, val_kl, val_rl = 0,0,0\n",
    "        model.eval()\n",
    "        with torch.inference_mode(): # it doesnt update parameters based on gradients \n",
    "            lst = []\n",
    "            for val_batch, t_mask, tidx in test_loader:\n",
    "\n",
    "                x_mu, x_logvar, z_mu, z_logvar = model(val_batch)\n",
    "                loss = loss_fun(val_batch, x_mu, x_logvar, z_mu, z_logvar,lst,mask=t_mask,freebits=freebits)\n",
    "                val_loss += loss.detach().item()\n",
    "                val_kl += lst[-1]\n",
    "                val_rl += lst[-2]\n",
    "            \n",
    "            # divide by all the batches of val set to get epoch metrics \n",
    "            val_loss = val_loss/len(test_loader)\n",
    "            val_kl = val_kl/len(test_loader)\n",
    "            val_rl = val_rl/len(test_loader)\n",
    "\n",
    "            epoch_dict[\"Val total Loss\"].append(val_loss)\n",
    "            epoch_dict[\"Val KL Loss\"].append(val_kl)\n",
    "            epoch_dict[\"Val Rec Loss\"].append(val_rl)\n",
    "\n",
    "        ## Print out what's happening\n",
    "        print(f\"\\nTrain loss: {train_loss:.3f}|Train Rec: {train_rl:.3f} | Val loss: {val_loss:.3f}, Val Rec: {val_rl:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set Analysis \n",
    "It comprises the analysis of the training and validation set. VAE inherently have a tendency to overfit, so it is important to keep the test set after training loop. During training:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scbc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
